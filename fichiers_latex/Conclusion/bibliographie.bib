
@book{gotlib_rubriqueabrac_1971,
  address = {{Paris}},
  edition = {Dargaud},
  series = {{Rubrique-{\`a}-brac}},
  title = {{Rubrique-{\`a}-brac, Taume 2}},
  volume = {Taume 2},
  isbn = {978-2-205-00554-7},
  shorttitle = {{Rubrique-{\`a}-brac. T. 2}},
  language = {fre},
  publisher = {{Dargaud}},
  author = {Gotlib, Marcel},
  month = jan,
  year = {1971}
}

@inproceedings{nagorny_towards_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.01765},
  primaryClass = {cs},
  title = {Vers Un Pilotage de La Qualit{\'e} Des Pi{\`e}ces Inject{\'e}es},
  abstract = {Thermoplastics injection molding allows the production of complex parts in large series. Industrial quality requirements are increasing. The injection molding process needs to be regulate in order to maintain a working point. There is actually no method to adjust all the parameters of the process in order to optimize the final quality of the product. We can rely on the success of neural networks models to propose a robust self-adaptive adjustment method. Objective is to adjust machine parameters for each cycle, based on measured quality characteristics on the produced part. A classical industrial cycle time is often less than 30 seconds ; therefore challenges are: measuring, computing and setting up machine parameters within this short timeframe. In this presentation we establish a specific literature review, on which will base our experimental works.},
  booktitle = {{{arXiv}}:1707.01765 [Cs]},
  url = {http://arxiv.org/abs/1707.01765},
  author = {Nagorny, Pierre and Pairel, Éric and Pillet, Maurice},
  month = jul,
  year = {2017},
  keywords = {Computer Science - Systems and Control},
}

@inproceedings{nagorny_injection_2017,
  address = {{Compi{\`e}gne, France}},
  title = {Pilotage En {{Injection Plastique}} - {{{\'E}tat}} de l'{{Art}}},
  booktitle = {12{\`e}me {{Congr{\`e}s International}} de {{G{\'e}nie Industriel}} ({{CIGI}} 2017)},
  url = {https://hal.archives-ouvertes.fr/hal-01551840},
  hal = {hal-01551840},
  author = {Nagorny, Pierre and Pairel, Éric and Pillet, Maurice},
  month = may,
  year = {2017},
  keywords = {Contrôle qualité,Injection molding,Injection plastique,Pilotage,process adjustment,Process control,Process modeling,Quality control,Régulation}
}

@inproceedings{nagorny_quality_2017,
  title = {Quality Prediction in Injection Molding},
  abstract = {Injection molded part quality can be improved by precise process adjustment, which could rely on in-situ measurements of part quality. Geometrical and appearance quality (visually and sensory) requirements are increasing. However, direct measurement is often not feasible industrially. Therefore, process control must rely on a prediction of parts quality attributes. This study compares prediction performances of diverse neural networks architectures with ``classical'' regression algorithms. Dataset comes from inline industrial measurements. Regression was performed on 97 scalar statistical features extracted from multiple acquisitions sources: thermographic images and analog signals. Haralick features were extracted. Convolutional Neural Networks were trained on thermographic images and Long Short Term Memory networks were trained on raw signals. Although the dataset was small, neural networks show better predictions scores than other regression algorithms.},
  booktitle = {2017 {{IEEE International Conference}} on {{Computational Intelligence}} and {{Virtual Environments}} for {{Measurement Systems}} and {{Applications}} ({{CIVEMSA}})},
  doi = {10.1109/CIVEMSA.2017.7995316},
  author = {Nagorny, Pierre and Pillet, Maurice and Pairel, Éric and Goff, Ronan Le and Loureaux, Jérôme and Wali, M.arlène and Kiener, Patrice},
  month = jun,
  year = {2017},
  keywords = {Injection Moulding,production engineering computing,Neural Networks,Injection molding,Quality control,Process control,learning (artificial intelligence),feedforward neural nets,quality prediction,neural network architectures,Training,convolution,Thermography,feature extraction,regression analysis,decision trees,infrared imaging,LSTM,convolutional neural networks,Computer architecture,boosting,neural net architecture,Haralick feature extraction,analog signals,appearance quality,convolutional neural network training,geometrical quality,in-situ measurements,injection molded part quality improvement,long-short term memory network,multiple acquisition source,neural network geometric dimension prediction,prediction scores,quality attributes,raw signals,regression algorithms,statistical feature extraction,thermographic images},
  pages = {141-146}
}

@inproceedings{nagorny_generative_2018,
  title = {Generative Adversarial Networks for Geometric Surfaces Prediction in Injection Molding: {{Performance}} Analysis with {{Discrete Modal Decomposition}}},
  shorttitle = {Generative Adverserial Networks for Geometric Surfaces Prediction in Injection Molding},
  abstract = {Geometrical and appearance quality requirements set the limits of the current industrial performance in injection molding. To guarantee the product's quality, it is necessary to adjust the process settings in a closed loop. Those adjustments cannot rely on the final quality because a part takes days to be geometrically stable. Thus, the final part geometry must be predicted from measurements on hot parts. In this paper, we use recent success of Generative Adversarial Networks (GAN) with the pix2pix network architecture to predict the final part geometry, using only hot parts thermographic images, measured right after production. Our dataset is really small and the GAN learns to translate thermography to geometry. We firstly study prediction performances using different image similarity comparison algorithms. Moreover, we introduce the innovative use of Discrete Modal Decomposition (DMD) to analyze network predictions. The DMD is a geometrical parameterization technique using a modal space projection to geometrically describe surfaces. We study GAN performances to retrieve geometrical parameterization of surfaces.},
  booktitle = {2018 {{IEEE International Conference}} on {{Industrial Technology}} ({{ICIT}})},
  doi = {10.1109/ICIT.2018.8352405},
  author = {Nagorny, Pierre and Lacombe, Thomas and Favreli{\`e}re, Hugues and Pillet, Maurice and Pairel, Éric and Goff, Ronan Le and Wali, Marlène and Loureaux, Jérôme and Kiener, Patrice},
  month = feb,
  year = {2018},
  keywords = {adjustments,appearance quality requirements,closed loop,Discrete Modal Decomposition,final quality,Gallium nitride,GAN performances,Generative adverserial networks,Generators,geometric surfaces prediction,geometrical parameterization technique,geometry,hot parts thermographic images,image similarity comparison algorithms,industrial performance,infrared imaging,Injection molding,Injection Moulding,learning (artificial intelligence),modal space projection,Network architecture,network predictions,performance analysis,pix2pix network architecture,prediction performances,process settings,production engineering computing,Quality control,quality prediction,solid modelling,Thermography,Training},
  pages = {1514-1519}
}

@inproceedings{nagorny_polarimetric_2019,
  title = {Polarimetric Imaging for Quality Control in Injection Molding},
  volume = {11172},
  abstract = {Quality control is a key requirement of the Fourth Industrial Revolution, especially in manufacturing. For high value technical parts, geometrical and appearance quality need to be guaranteed. The quality control of the part, directly on the manufacturing line, can be achieved with non-contact sensors. With a robust quality sensor, the manufacturing process could be adjusted in real time to always achieve the optimal quality. For quality control, visible spectrum imaging is commonly used. However, more information could be obtained from broader spectrum imaging and also from the degree of linear polarization of the light from the part. We have developed a simple polarimetric imaging system to verify whether linear polarization is capable of amplifying geometric and appearance defects. We evaluate the use of this polarimetric imager given the industrial constraints of injection molding. We compare supervised classification performances on an injection molded parts dataset, using polarimetry and non-polarized images. We compare the performances of different machine learning pipelines with hand-crafted feature extraction and Deep Transfer Learning. With its industrial robustness, polarimetry could be a valuable addition to non-contact imagers for geometric and appearance quality control.},
  urldate = {2019-07-20},
  booktitle = {Fourteenth {{International Conference}} on {{Quality Control}} by {{Artificial Vision}}},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10/gf48kh},
  url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11172/1117217/Polarimetric-imaging-for-quality-control-in-injection-molding/10.1117/12.2522062.short},
  author = {Nagorny, Pierre and Lacombe, Thomas and Muller, Thomas and Favreli{\`e}re, Hugues and Pairel, Éric and Pillet, Maurice},
  month = jul,
  year = {2019},
  pages = {1117217}
}

@patent{nagorny_dispositif_2019,
  address = {{Annecy, France}},
  title = {Dispositif et Proc{\'e}d{\'e} de Contr{\^o}le de Pi{\`e}ces},
  shorttitle = {Dispositif et Proc{\'e}d{\'e} de Contr{\^o}le de Pi{\`e}ces},
  abstract = {V. R{\'e}f : The Eye
N. R{\'e}f : B18432 FR
Titre : Dispositif et proc{\'e}d{\'e} de contr{\^o}le de pi{\`e}ces
N\textdegree{} de d{\'e}p{\^o}t : 19/10706
Date de d{\'e}p{\^o}t : 27 septembre 2019},
  nationality = {France},
  language = {English},
  assignee = {Universit{\'e} Savoie Mont Blanc},
  author = {Nagorny, Pierre and Pairel, Eric and Pillet, Maurice},
  month = sep,
  year = {2019},
  pages = {24}
}

@techreport{directiongeneraledesentreprises_chiffres_2019,
  address = {{Ivry-sur-Seine, France}},
  type = {{{\'E}tude {\'e}conomique}},
  title = {{Chiffres cl{\'e}s de l'industrie manufacturi{\`e}re, {\'E}dition 2018}},
  abstract = {Chiffres cl{\'e}s de l'industrie manufacturi{\`e}re

ISSN : 2492-7090 - D{\'e}p{\^o}t l{\'e}gal : 2019
DGE - 67, rue Barb{\`e}s, 94200 Ivry-sur-Seine},
  language = {Fran{\c c}ais},
  number = {ISSN : 2492-7090},
  urldate = {2019-09-28},
  institution = {{Minist{\`e}re de l'{\'E}conomie et des Finances}},
  url = {https://www.entreprises.gouv.fr/files/files/directions_services/etudes-et-statistiques/Chiffres_cles/Industrie/2018-Chiffres-cles-industrie-manufacturiere.pdf},
  author = {{Direction G{\'e}n{\'e}rale des Entreprises}},
  year = {2019},
  pages = {8}
}

@article{vaneck_vos_2006,
  title = {{{VOS}}: {{A New Method}} for {{Visualizing Similarities}} between {{Objects}}},
  issn = {1566-5283},
  shorttitle = {{{VOS}}},
  abstract = {We present a new method for visualizing similarities between objects. The method is called VOS, which is an abbreviation for visualization of similarities. The aim of VOS is to provide a low-dimensional visualization in which objects are located in such a way that the distance between any pair of objects reflects their similarity as accurately as possible. Because the standard approach to visualizing similarities between objects is to apply multidimensional scaling, we pay special attention to the relationship between VOS and multidimensional scaling.},
  language = {en},
  urldate = {2019-09-09},
  journal = {ERIM report series research in management Erasmus Research Institute of Management},
  url = {https://repub.eur.nl/pub/7654},
  author = {{van Eck}, Nees Jan and Waltman, Ludo},
  month = apr,
  year = {2006}
}

@article{kazmer_towards_1999,
  title = {Towards {{Controllability}} of {{Injection Molding}}},
  volume = {8},
  abstract = {Process control has been recognized as an important means of improving the performance and consistency of thermoplastic parts. However, no single control strategy or system design has been universally accepted, and molding systems continue to produce defective components during production. The capability of the injection molding process is limited by the thermal and flow dynamics of the heated polymer melt. This paper discusses some of the difficulties posed by complex and distributed nature of the injection molding process. The flow and thermal dynamics of the process are analyzed with respect to transport and rheology. Then, two novel processing methods are described to enable in-cycle flow, pressure, and thermal control. Simulation and experimental results demonstrate effectiveness of these innovations to increase the consistency and flexibility in polymer processing. Such system design changes simplify the requisite control structures while improving the process robustness and productivity.},
  urldate = {2016-10-19},
  journal = {Journal of Materials Processing and Manufacturing Science},
  doi = {10.1106/DAD5-PNGG-U87B-08Y5},
  url = {http://adsabs.harvard.edu/abs/1999JMPMS...8..127K},
  author = {Kazmer, David Owen and Hatch, David},
  month = oct,
  year = {1999},
  keywords = {pressure control,flow control,thermal control,flow dynamics,thermal dynamics,system design,rheology},
  pages = {127-140},
}

@phdthesis{thyregod_modelling_2001,
  type = {Doctorat},
  title = {Modelling and Monitoring in Injection Molding},
  abstract = {This thesis is concerned with the application of statistical methods in quality improvement of injection molded parts. The methods described are illustrated with data from the manufacturing of parts for a medical device. The emphasis has been on the variation between cavities in multi-cavity molds.
From analysis of quality measurements from a longer period of manufacturing, it was found that differences in cavities was that source of variation with great- est influence on the length of the molded parts. The other large contribution to the length variation was the different machine settings. Samples taken within the same machine set-point did not cause great variation compared to the two preceding sources of variation.
A simple graphical approach is suggested for finding patterns in the cavity differences. Applying this method to data from a 16 cavity mold, a clear con- nection was found between a parts length and the producing cavitys position in the mold. In a designed experiment it was possible to isolate the machine parameters contributing to the variation between cavities. Thus, with a proper choice of levels for the machine variables, it was possible to reduce the varia- tion between cavities substantially. Also an alternative model for the shrinkage of parts from a multi-cavity mold is suggested. From applying the model to data from a shrinkage study, it seemed that the observed part differences were not only due to differences in cavity dimensions.
A model for the in-control variation for a multi-cavity molding process was suggested. Based on this model, control charting procedures have been suggested for monitoring the quality of the molded parts. Moreover, a capability index for multi-cavity molds has been suggested.
Furthermore an alternative method for in-line quality charting is suggested. The method is for continuous control by attributes, and it is an alternative to the batch oriented approach mostly used. The procedure is especially efficient for quality requirements of very low proportion non-conformities. For the pro- posed charts the ARL function is derived. It is shown that in the case where a non-conforning unit is only expected very rarely during sampling, a moving sum chart and a CUSUM chart are equivalent.
Finally, the correlation structure of 21 process variables has been studied prior to monitoring the process. It is illustrated how the process can be analysed with multivariate techniques. It was found that two principal components re- flected changes in machine set-points. Thus, there seems to be great potential in monitoring the process variables using a multivariate approach.},
  language = {eng},
  urldate = {2016-09-28},
  school = {Department of Mathecatical Modelling, Technical University of Denmark},
  url = {http://www.forskningsdatabasen.dk/en/catalog/2185769604},
  author = {Thyregod, Peter},
  month = jan,
  year = {2001},
  keywords = {Multivariate statistical analysis,Multivariate,Quality control,SPC,Shrinkage,capability,multi-cavity,control card,in-line process monitoring,cavity geometry}
}

@book{suh_principles_1990,
  title = {The {{Principles}} of {{Design}}},
  isbn = {978-0-19-504345-7},
  abstract = {Here is a basic introduction to the principles of industrial design and their application in all phases of planning and production. It is intended to offer experienced instruction, based on scientific knowledge, in place of the intuitive approach to the field often encountered in engineering practice and education. The book presents basic principles and constitutes an exposition of these fundamental axioms and their application. The emphasis is on identifying problems in a clear, scientific manner, so that the correct solution may be arrived at regardless of the mathematical treatment involved. In particular, the importance of conceptualizing design approaches--a uniquely human, intellectual skill--is highlighted, since too often educators and engineers try to limit this process to computer techniques. Case studies are extensively presented to illustrate the significance as well as the use of the axioms in solving real problems. The work is based on extensive experience at M.I.T's Laboratory for Manufacturing and Productivity, where axiomatics is a major program. The goal of the program is to bring a scientific approach to the decision-making process related to manufacturing--an approach that facilitates rational design of processes and products, as well as the optimization of manufacturing systems.},
  language = {en},
  publisher = {{Oxford University Press}},
  author = {Suh, Nam P.},
  year = {1990},
  keywords = {Technology \& Engineering / Industrial Design / General}
}

@article{agrawal_injection-molding_1987,
  title = {Injection-Molding Process Control\textemdash{{A}} Review},
  volume = {27},
  issn = {1548-2634},
  abstract = {This paper reviews control strategies employed in the injection-molding process. For clarity, the controlled variables have been categorized into all-phase control, phase-dependent control, and cycle-to-cycle control. All-phase control includes variables that must be monitored and controlled at all times; i.e., in all the phases. Control of variables that are triggered during a specific phase are discussed under phase-dependent control. In cycle-to-cycle control, previous data are used to predict future trends and take appropriate corrective actions, The cyclic, dynamic, and unsteady state nature of the injection-molding process is discussed with respect to the conventional proportional-integral (PI) and proportional- integral-derivative (PID) controllers as well as the more advanced control schemes such as self-tuning control, optimal control, and statistical process control. Suggestions involving specific advanced control schemes and recommendations for future research in injection-molding process control also are made.},
  language = {en},
  number = {18},
  urldate = {2016-10-20},
  journal = {Polymer Engineering \& Science},
  doi = {10.1002/pen.760271802},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/pen.760271802/abstract},
  author = {Agrawal, A. R. and Pandelidis, I. O. and Pecht, M.},
  month = oct,
  year = {1987},
  keywords = {PID,pressure control,optimal control,Cavity pressure,velocity control,self-tuning,SPC,viscosity},
  pages = {1345-1357}
}

@phdthesis{shankar_dynamic_1978,
  address = {{Pittsburgh}},
  type = {Doctoral {{Dissertation}}},
  title = {Dynamic {{Modeling}} and {{Control}} of {{Injection Molding Machines}}},
  abstract = {Dynamic Modeling and Control of Injection Molding Machines
Many model-based controllers have been developed, Shankar (12) was first to develop a nonlinear model-based control system in 1978 to optimize ram velocity with a discrete iterative control method.},
  language = {English},
  school = {Carnegie-Mellon University},
  author = {Shankar, A.},
  year = {1978},
  keywords = {ram velocity,Dynamic modeling,Nonlinear}
}

@article{shankar_mathematical_1982,
  title = {A {{Mathematical Model}} for the {{Evaluation}} of {{Injection Molding Machine Control}}},
  volume = {104},
  issn = {0022-0434},
  abstract = {This paper presents the development of a deterministic nonlinear lumped parameter mathematical model useful for the study and design of polymer injection molding machines and their associated controllers. The mathematical model was simulated on a digital computer and the results compared with experimental measurements taken in an existing injection molding machine. A comparison of these results showed that the developed mathematical model adequately predicts the transient behavior of averaged machine and mold variables. The resulting model is useful for control system design, machine parameter selection, and model redesign purposes.},
  number = {1},
  urldate = {2016-10-19},
  journal = {Journal of Dynamic Systems, Measurement, and Control},
  doi = {10.1115/1.3149636},
  url = {http://dx.doi.org/10.1115/1.3149636},
  author = {Shankar, A. and Paul, F. W.},
  month = mar,
  year = {1982},
  pages = {86-92}
}

@article{chiu_dynamic_1991,
  title = {Dynamic Modeling of the Mold Filling Process in an Injection Molding Machine},
  volume = {31},
  issn = {1548-2634},
  abstract = {This paper presents the development of a nonlinear mathematical model for the study of the mold filling process in an injection molding machine. The model is formulated by the Reynolds transport theorem which is applied to describe the polymer flow dynamics. The mold filling process can be approximated by the transient phenomenon of the non-Newtonian fluids flowing through a closed conduit. The comparison between the experimental results and the theoretical simulation indicate that the nonlinear model is a reasonable representation of the mold filling dynamics when the acrylonitrile-butadiene-styrene (ABS) is injected into a disk shape mold. The actuation system dynamics of an injection molding machine are also investigated. The results indicate that the nonlinear model can also adequately predict the transient behavior of the actuation system.},
  language = {en},
  number = {19},
  urldate = {2016-10-21},
  journal = {Polymer Engineering \& Science},
  doi = {10.1002/pen.760311908},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/pen.760311908/abstract},
  author = {Chiu, Cheng-Ping and Shih, Laming-Chang and Wei, Jong-Hwei},
  month = oct,
  year = {1991},
  keywords = {dynamics,simulation,Cavity pressure},
  pages = {1417-1425}
}


@article{nakamura_aspects_1972,
  title = {Some Aspects of Nonisothermal Crystallization of Polymers. {{I}}. {{Relationship}} between Crystallization Temperature, Crystallinity, and Cooling Conditions},
  volume = {16},
  copyright = {Copyright \textcopyright{} 1972 John Wiley \& Sons, Inc.},
  issn = {1097-4628},
  abstract = {The changes in temperature and crystallinity of polymer during nonisothermal crystallization were theoretically analyzed assuming a cooling condition under which heat transfer occurs at a rate proportional to the difference in temperature between polymer and the environment. When a plateau appears in the temperature change during crystallization, crystallization temperature can be predicted by a simple method. This method gives nearly the same value as that obtained by successive calculations of temperature and crystallinity throughout the whole process. In addition, a graphic method is presented to predict crystallization temperature. By using the plateau temperature observed in melt-spinning experiments, the crystallization rate under molecular orientation is evaluated. Furthermore, a method applicable to estimating the ultimate crystallinity is proposed. A rough estimation of the increase in the rate of crystallization under molecular orientation was carried out for very high-speed spinning of poly(ethylene terephthalate).},
  language = {en},
  number = {5},
  urldate = {2019-10-04},
  journal = {Journal of Applied Polymer Science},
  doi = {10/cjk29f},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/app.1972.070160503},
  author = {Nakamura, K. and Watanabe, T. and Katayama, K. and Amano, T.},
  year = {1972},
  pages = {1077-1091}
}

@article{nakamura_aspects_1973,
  title = {Some Aspects of Nonisothermal Crystallization of Polymers. {{II}}. {{Consideration}} of the Isokinetic Condition},
  volume = {17},
  copyright = {Copyright \textcopyright{} 1973 John Wiley \& Sons, Inc.},
  issn = {1097-4628},
  abstract = {In the previous paper a practical method has been applied for an analysis of non-isothermal crystallization in terms of data of isothermal crystallization. The fundamental equation was written on the assumption of the isokinetic conditions in the following form: \textbackslash{}documentclassarticle\textbackslash{}pagestyleempty\textbackslash{}begindocument\$ X\textbackslash{}left( \textbackslash{}rm t \textbackslash{}right) = 1 - \textbackslash{}exp \textbackslash{}left[ - \textbackslash{}left( \i{}nt\_0\^t K\textbackslash{}left( T \textbackslash{}right)d\textbackslash{}tau \textbackslash{}right)\^n \textbackslash{}right] \$\textbackslash{}enddocument where X(t) is the degree of phase transformation at time t, and n is the Avrami index determined in the isothermal experiments; K(T) is connected with the crystallization rate constant of the isothermal crystallization, k(T), through the relation K(T) = k(T)1/n. The equation is derived on the basis of the well-known theory of phase transformation. Experiments of nonisothermal crystallization of high-density polyethylene were carried out under various cooling conditions. The change in crystallinity during the process was followed by using the above equation in the course of the primary crystallization. A procedure of the analysis of the whole, including both the primary and secondary processes, is suggested as an eminently practical one on a more general assumption.},
  language = {en},
  number = {4},
  urldate = {2019-10-04},
  journal = {Journal of Applied Polymer Science},
  doi = {10/c2bzhv},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/app.1973.070170404},
  author = {Nakamura, K. and Katayama, K. and Amano, T.},
  year = {1973},
  pages = {1031-1041}
}

@article{bereaux_series_2004,
  title = {Series Solutions for Viscous and Viscoelastic Fluids Flow in the Helical Rectangular Channel of an Extruder Screw},
  volume = {123},
  issn = {0377-0257},
  abstract = {The flow of a viscous Newtonian and of a viscoelastic (upper-convected Maxwell) fluids in the helical rectangular channel of a screw have been solved using a series expansion on dimensionless torsion and curvature. At any given power of curvature or torsion, the velocity and stream function are solved using series of orthogonal eigenfunctions. Back pressure driven flow and drag driven flow have been both considered for Newtonian fluids, while only pressure driven flow can be studied for viscoelastic fluids. Contrary to common knowledge in polymer processing based on the Parallel Plate Model, we found that, in the case of cross-sections with large aspect ratio, torsion effects can be significant. Specific viscoelastic effects triggered by curvature and torsion have also been modelled. Our analytical model has been precisely validated by three-dimensional finite elements calculations.},
  number = {2\textendash{}3},
  urldate = {2016-10-24},
  journal = {Journal of Non-Newtonian Fluid Mechanics},
  doi = {10.1016/j.jnnfm.2004.08.011},
  url = {http://www.sciencedirect.com/science/article/pii/S0377025704002678},
  author = {B{\'e}reaux, Y. and Moguedet, M. and Raoul, X. and Charmeau, J. Y. and Balcaen, J. and Graebling, D.},
  month = nov,
  year = {2004},
  keywords = {Extruder screw,Helical rectangular channel,Toroidal channel,Lid driven cavity flow,Pressure driven flow,Upper-convected Maxwell model,Perturbation expansion},
  pages = {237-257}
}

@article{legoff_study_2005,
  title = {Study and Modeling of Heat Transfer during the Solidification of Semi-Crystalline Polymers},
  volume = {48},
  issn = {0017-9310},
  abstract = {Semi-crystalline polymers are materials whose behavior during their cooling is difficult to model because of the strong coupling between the crystallization, heat transfer, pressure and shear. Thanks to two original apparatus we study solidification of such a polymer without shear. Firstly the comparison between experimental results and a numerical model will permit to validate crystallization kinetic for cooling rate reachable by DSC. The second experiment makes it possible to analyze solidification for high cooling rate, corresponding to some manufacturing processes. It appears that crystallization has an influence on the thermal contact resistance.},
  number = {25},
  urldate = {2019-09-21},
  journal = {International Journal of Heat and Mass Transfer},
  doi = {10.1016/j.ijheatmasstransfer.2005.06.015},
  url = {http://www.sciencedirect.com/science/article/pii/S0017931005004333},
  author = {Le Goff, R. and Poutot, G. and Delaunay, D. and Fulchiron, R. and Koscher, E.},
  month = dec,
  year = {2005},
  keywords = {Semi-crystalline polymers,Crystallization,Thermal contact resistance},
  pages = {5417-5430}
}

@phdthesis{legoff_etude_2006,
  type = {Thesis},
  title = {{\'E}tude et Mod{\'e}lisation Des Transferts Thermiques Lors de La Solidification de Pi{\`e}ces Inject{\'e}es En Polym{\`e}re Semi-Cristallin Charg{\'e} de Fibres},
  abstract = {Un des enjeux majeurs de la simulation du proc{\'e}d{\'e} d'injection est la pr{\'e}diction des retraits et des d{\'e}formations dans la phase de maintien et dans la phase de refroidissement des pi{\`e}ces. Ce travail de th{\`e}se s'est inscrit dans ce contexte avec pour objectif de mieux comprendre les ph{\'e}nom{\`e}nes thermiques r{\'e}gissant la solidification de pi{\`e}ces inject{\'e}es en polym{\`e}re semi-cristallin charg{\'e} de fibres de verre. En effet, la cristallisation de ce type de mat{\'e}riaux est une r{\'e}action exothermique et ce changement d'{\'e}tat influence fortement les propri{\'e}t{\'e}s thermiques des mat{\'e}riaux ainsi que les conditions aux parois. Dans cette optique, la caract{\'e}risation et la mod{\'e}lisation des propri{\'e}t{\'e}s thermiques des mat{\'e}riaux {\'e}tudi{\'e}s ont {\'e}t{\'e} effectu{\'e}es. La conductivit{\'e} thermique a fait l'objet d'une analyse approfondie {\`a} travers la validation d'un mod{\`e}le de pr{\'e}diction prenant en compte l'anisotropie induite par l'orientation des fibres. Une {\'e}tude exp{\'e}rimentale a {\'e}galement permis de valider les mod{\`e}les thermiques de cristallisation et de transfert. Enfin, l'effet de la cristallisation sur les transferts de chaleur a {\'e}t{\'e} mis en {\'e}vidence, d'une part dans des conditions de refroidissement rapide sans cisaillement et, d'autre part dans des conditions industrielles. Les conditions d'interface ont {\'e}t{\'e} analys{\'e}es notamment gr{\^a}ce notamment {\`a} l'{\'e}valuation d'une r{\'e}sistance thermique de contact.},
  urldate = {2019-09-21},
  school = {Nantes},
  url = {http://www.theses.fr/2006NANT2069},
  author = {Le Goff, Ronan},
  month = jan,
  year = {2006},
  keywords = {Composites à fibres,Essais de comportement au feu,Moulage par injection,Polymères cristallins,Sciences pour l'ingénieur,Transfert de chaleur}
}



@article{moguedet_use_2009,
  title = {Use of {{Comsol Multiphysics}} to Understand and Optimize the Filling Phase in Injection and Micro-Injection Molding Process},
  volume = {54},
  issn = {09710078},
  abstract = {The work presented here deals with the simulation of the cavity filling stage of the injection and micro injection molding process, for thermoplastic materials. Within a research project led by the P{\^o}1e Europ{\'e}en de Plasturgie on optimizing replication of micro-features on plastic parts, a challenge is to better understand physical phenomena during the process to optimize designs and process conditions. Comsol Multiphysics\textregistered{} gives us the means to take into consideration some other aspects usually neglected in commercial 3D softwares dedicated to polymer processing. In particular, tracking of the flow front is based on a Level Set approach. Results are presented for a Newtonian and non-Newtonian polymer, and in an isothermal or thermal dependant configuration. Calculations are compared to experimental results on a polypropylene. In a second part, surface tension effects are analyzed when filling micro-geometries. Computations show no effect, unless the flow front proceeds down to less than 1 mm/s. Finally, our work shows the extended possibilities of Comsol Multiphysics\textregistered{} to deal with multi-phase flow topics for the polymer processing community.},
  number = {11},
  urldate = {2016-09-28},
  journal = {Popular Plastics \& Packaging},
  url = {http://camphrier.grenet.fr/login?url=http://search.ebscohost.com/login.aspx?direct=true\&db=buh\&AN=45567306\&lang=fr\&site=eds-live},
  author = {Moguedet, M. and Namy, P. and B{\'e}reaux, Y.},
  month = nov,
  year = {2009},
  keywords = {Thermoplastics,Microstructures,level-set,MOLDING (Chemical technology),CHEMICAL engineering,POWDER injection molding,ELASTOMERS -- Molding,GELCASTING,surface tension,Injection molding,filling},
  pages = {29-32}
}

@article{gao_adaptive_2008,
  title = {Adaptive Geometry and Process Optimization for Injection Molding Using the Kriging Surrogate Model Trained by Numerical Simulation},
  volume = {27},
  issn = {1098-2329},
  abstract = {An adaptive optimization method based on the kriging surrogate model has been developed to intelligently determine the optimal geometric dimensions and processing parameters for minimizing warpage in injection-molded components. The kriging surrogate model is a statistics-based interpolated technique that provides the approximate functional relationship between warpage and factors that influence warpage. In this study, it is used to be first trained by\textemdash{}and later replaced\textemdash{}the full-fledged, time-consuming numerical simulation in the optimization process. Based on this surrogate model, an adaptive iteration scheme that takes into account the predicted uncertainty is performed to improve the accuracy of the surrogate model while finding the optimum solution. The optimization process starts with a small number of initial training sample points and then adds additional key points during iterations by evaluating the correlations among the candidate points. As an example of validation and application, optimization of geometric dimensions and processing parameters for a box-shape part with different and stepwise wall thicknesses has been performed. The results demonstrate the feasibility and effectiveness of the proposed optimization method. \textcopyright{} 2008 Wiley Periodicals, Inc. Adv Polym Techn 27:1\textendash{}16, 2008; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/adv.20116},
  language = {en},
  number = {1},
  urldate = {2016-10-21},
  journal = {Advances in Polymer Technology},
  doi = {10.1002/adv.20116},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/adv.20116/abstract},
  author = {Gao, Yuehua and Turng, Lih-Sheng and Wang, Xicheng},
  month = mar,
  year = {2008},
  keywords = {Simulation,Kriging surrogate model,Injection molding,Warpage,Processing optimization},
  pages = {1-16}
}

@inproceedings{blyskal_applying_1994,
  address = {{Chicago, USA}},
  title = {Applying {{Design Of Experiment}} Analysis Techniques to the Injection Moulding},
  abstract = {applies DOE techniques to determine optimal settings with respect to one or more dimensional measures.},
  language = {English},
  booktitle = {{{SPE ANTEC PROCESS}}},
  publisher = {{Society of Plastics Engineers}},
  author = {Blyskal, P. J. and Meheran, P. J.},
  month = apr,
  year = {1994},
  keywords = {Design of experiment,Model,process measurements,SPC},
  pages = {729- 732}
}

@article{fei_practical_2013,
  title = {Practical {{Applications}} of {{Taguchi Method}} for {{Optimization}} of {{Processing Parameters}} for {{Plastic Injection Moulding}}: {{A Retrospective Review}}},
  volume = {2013},
  issn = {,},
  shorttitle = {Practical {{Applications}} of {{Taguchi Method}} for {{Optimization}} of {{Processing Parameters}} for {{Plastic Injection Moulding}}},
  abstract = {Determining the optimal processing parameter is routinely performed in the plastic injection moulding industry as it has a direct and dramatic influence on product quality and costs. In this volatile and fiercely competitive market, traditional trial-and-error is no longer sufficient to meet the challenges of globalization. This paper aims to review the research of the practical use of Taguchi method in the optimization of processing parameters for injection moulding. Taguchi method has been employed with great success in experimental designs for problems with multiple parameters due to its practicality and robustness. However, it is realized that there is no single technique that appears to be superior in solving different kinds of problem. Improvements are to be expected by integrating the practical use of the Taguchi method into other optimization approaches to enhance the efficiency of the optimization process. The review will shed light on the standalone Taguchi method and integration of Taguchi method with various approaches including numerical simulation, grey relational analysis (GRA), principal component analysis (PCA), artificial neural network (ANN), and genetic algorithm (GA). All the features, advantages, and connection of the Taguchi-based optimization approaches are discussed.},
  language = {en},
  urldate = {2016-10-19},
  journal = {International Scholarly Research Notices},
  doi = {10.1155/2013/462174},
  url = {http://www.hindawi.com/journals/isrn/2013/462174/abs/},
  author = {Fei, Ng Chin and Mehat, Nik Mizamzul and Kamaruddin, Shahrul},
  month = jun,
  year = {2013},
  keywords = {Taguchi optimization method},
  pages = {e462174}
}

@article{schnerr-haselbarth_automation_2000,
  title = {Automation of Online Quality Control in Injection Moulding},
  volume = {284-285},
  issn = {1439-2054},
  abstract = {Intelligent quality control for monitoring and documentation of the quality of injection-molded parts is due to replace conventional quality control. Physical process data, such as temperature and pressure, are measured by online quality control within the production cycle. An empirical mathematical model of the molding properties allows their calculation on the basis of the data acquired at the end of each production cycle. The data obtained with the new neural algorithm show better results than all algorithms applied so far.},
  language = {en},
  number = {1},
  urldate = {2016-11-07},
  journal = {Macromolecular Materials and Engineering},
  doi = {10.1002/1439-2054(20001201)284:1<81::AID-MAME81>3.0.CO;2-Q},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/1439-2054(20001201)284:1<81::AID-MAME81>3.0.CO;2-Q/abstract},
  author = {{Schnerr-H{\"a}selbarth}, O. and Michaeli, W.},
  month = dec,
  year = {2000},
  pages = {81-85}
}

@article{richard_analyse_2009,
  title = {{Analyse des strat{\'e}gies de correction de d{\'e}fauts en plasturgie {\`a} l'aide d'un mod{\`e}le de r{\'e}solution de probl{\`e}me {\`a} base de contraintes}},
  volume = {72},
  issn = {1243-1370},
  language = {fr},
  number = {3},
  urldate = {2016-09-28},
  journal = {Le travail humain},
  url = {http://www.cairn.info.camphrier-2.grenet.fr/resume.php?ID_ARTICLE=TH_723_0267},
  author = {Richard, Jean-Fran{\c c}ois and Pastr{\'e}, Pierre and Parage, Pierre and Sander, Emmanuel and Futtersack, Michel and Labat, Jean-Marc},
  month = sep,
  year = {2009},
  keywords = {Simulation,Résolution de problème,Contraintes,Analyse de protocoles individuels,Expert System},
  pages = {267-292}
}

@article{pastre_role_1994,
  title = {{Le r{\^o}le des sch{\`e}mes et concepts dans la formation des comp{\'e}tences}},
  language = {Fran{\c c}ais},
  number = {71},
  journal = {Performances humaines et techniques},
  author = {Pastr{\'e}, Pierre},
  year = {1994},
  pages = {21-28},
  note = {00039}
}

@incollection{pastre_role_2004,
  edition = {Octar{\`e}s},
  title = {Le R{\^o}le Des Concepts Pragmatiques Dans La Gestion Des Situations Probl{\`e}mes : Le Cas Des R{\'e}gleurs En Plasturgie},
  abstract = {D'ailleurs, dans une deuxi{\`e}me recherche sur les presses {\`a} injecter (Pastr{\'e}, 2004), portant, elle, sur l'activit{\'e} de r{\'e}gleurs travaillant sur des machines {\`a} commande num{\'e}rique, on a pu constater une complexification {\'e}tonnante du paysage. Mais m{\^e}me dans la situation tr{\`e}s simple qu'on a d{\'e}crit plus haut, on trouve tous les ingr{\'e}dients qui permettent de d{\'e}finir la structure conceptuelle d'une situation :
1/ des concepts organisateurs qui permettent le diagnostic, concepts pragmatiques en l'occurrence ;
2/ des indicateurs, qui sont des observables, qui permettent de donner une valeur actuelle aux concepts et dont la signification a {\'e}t{\'e} construite de telle sorte qu'elle relie observables et concepts ;
3/ des classes de situations, ici des r{\'e}gimes de fonctionnement de la machine, qu'on peut analyser {\`a} partir de la valeur donn{\'e}e aux concepts organisateurs et qui vont sp{\'e}cifier le r{\'e}pertoire de proc{\'e}dures (ou de r{\`e}gles d'action) {\`a} utiliser ;
4/ des strat{\'e}gies attendues, en fonction du niveau de conceptualisation auquel a acc{\`e}s un op{\'e}rateur : dans l'exemple cit{\'e}, il y a les op{\'e}rateurs qui ont construit le concept de bourrage et ceux qui ne l'ont pas construit. L'{\'e}nonc{\'e} de ces strat{\'e}gies attendues n'{\'e}puise pas les strat{\'e}gies effectivement mobilis{\'e}es par les acteurs, mais cela permet de mettre de l'ordre en fournissant une grille d'analyse.
Ajoutons deux remarques compl{\'e}mentaires : on a parl{\'e} plus haut soit de concepts pragmatiques, soit de concepts organisateurs. Or il est important de bien marquer la distinction. Un concept pragmatique a trois propri{\'e}t{\'e}s :
1/ du point de vue de son origine, il est construit dans l'action. Autrement dit, son origine n'est pas th{\'e}orique, mais pratique. Il ne provient pas d'un savoir, il provient de l'activit{\'e}. De ce point de vue, il fait partie de ce que Vergnaud appelle les \guillemotleft{} concepts en acte \guillemotright{} ou de ce que Vygotski appelle les \guillemotleft{} concepts quotidiens \guillemotright, si l'on veut bien admettre que le travail fait partie de la sph{\`e}re du quotidien ;
2/ du point de vue de sa fonction, un concept pragmatique est un concept organisateur de l'action, dans la mesure o{\`u} il permet d'identifier dans quelle classe de situations un acteur se trouve. Il permet de faire un diagnostic et ainsi d'orienter l'action pour qu'elle soit efficace. Tous les concepts organisateurs de l'action ne sont pas forc{\'e}ment d'origine},
  booktitle = {Recherches En Didactique Professionnelle},
  author = {Pastr{\'e}, Pierre and Samur{\c c}ay, R.},
  year = {2004},
  pages = {17-49},
}

@phdthesis{jan_expert_1992,
  address = {{Newark, NJ, USA}},
  title = {Expert {{System}} for the {{Injection Molding}} of {{Engineering Thermoplastics}}},
  abstract = {Injection molding of engineering thermoplastics is the most widely used manufacturing method in industry. It is a priority to maintain a deviation-free operating environment to ensure high quality, low cost manufacture. An expert system for the injection molding of engineering thermoplastics has been investigated. The system can be used to attenuate the deviations experienced during the injection molding of engineering thermoplastics. The system is coded in C programming language.The resolution procedures of this system include two stages such as the definition of declarative knowledge and the procedure of corrective actions. In the definition of declarative knowledge, all of the necessary information is collected for firing the inference engine. This information includes the material type, the material manufacturer, the material grade, the recommended operating conditions, the operating conditions, the deviation type, and the correlative weighting factors.The procedure of corrective action is classified by fishbone diagram into four different levels. These levels include and are ranked as method corrective actions, operating variable corrective actions, mold corrective actions, and material corrective actions.The rule values of the corrective action in each level are assigned to determine the rank for employing these corrective actions. Among those rule values, the rule values of the method corrective actions, of the mold corrective actions, and of the material corrective actions are determined by the degree of difficulty required to eliminate the deviation and the input of the molding experts. A decision algorithm is developed to calculate the priority weighting factors, rule values, of each operating variable corrective action. Furthermore, the Pareto principle is introduced to analyze the control parameters of the decision algorithm.During the interactive procedures of eliminating the deviation, the system provides an explanation function for each step. It allows the system to illustrate the reason for each action to the user. A self-learning mechanism is also developed in this study. This self-learning mechanism based on the response of the resolution results modifies the parameters which influence the sequence of the corrective actions.The system has been examined by experts in the field of injection molding. It is recognized that the system not only provides reasonable resolution sequences for eliminating the deviation, but also, accurate suggested actions for the user. Furthermore, the resolution actions have been simulated in the injection molding filling package-MOLDFLOW. This confirms that the resolution actions can actually influence the parameters which can eliminate or reduce the deviations.},
  language = {English},
  school = {New Jersey Institute of Technology},
  author = {Jan, Tzy-Cherng},
  month = oct,
  year = {1992},
  keywords = {Expert System,self-learning}
}

@article{kameoka_development_1993,
  title = {Development of an Expert System for Injection Molding Operations},
  volume = {12},
  issn = {1098-2329},
  abstract = {To provide a computer assisted system in finding optimum conditions for injection molding operations, an Expert System based on Artificial Intelligence (AI) technique and knowledge or ``know-how'' from many skilled injection molding operators has been developed. For the construction of this Expert System, we have originally applied a new method of ``Multi-dimensional Matrix'' that represents the relationship between the parameters of each molding condition and expert's knowledge. In this article we describe an outline of the system and the process of converting expert knowledge into AI rules, and demonstrate some molded samples obtained using this Expert System. \textcopyright{} 1993 by John Wiley \& Sons, Inc.},
  language = {en},
  number = {4},
  urldate = {2016-10-19},
  journal = {Advances in Polymer Technology},
  doi = {10.1002/adv.1993.060120407},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/adv.1993.060120407/abstract},
  author = {Kameoka, Seiji and Haramoto, Nobuhiro and Sakai, Tadamoto},
  month = dec,
  year = {1993},
  keywords = {Multivariate,Expert System,Multi-dimensional Matrix,Artificial Intelligence (AI),wrappage},
  pages = {403-418}
}

@article{shelesh-nezhad_intelligent_1997,
  title = {An Intelligent System for Plastic Injection Molding Process Design},
  volume = {63},
  issn = {0924-0136},
  abstract = {An AI System for obtaining the magnitude of process parameters in plastic injection molding operation has been developed. The system is user interactive and can be used at shop floor. This system applies two techniques, Rule-Based and Case-Based Reasoning. Case-Based Reasoning is used to derived the first trial setting of processing parameters, while the Rule-Based sub-system suggests a set of corrective actions to deal with possible corresponding variations in molding. The system reduces optimization time and human expert dependency.},
  number = {1},
  urldate = {2016-10-19},
  journal = {Journal of Materials Processing Technology},
  doi = {10.1016/S0924-0136(96)02664-7},
  url = {http://www.sciencedirect.com/science/article/pii/S0924013696026647},
  author = {{Shelesh-Nezhad}, K. and Siores, E.},
  month = jan,
  year = {1997},
  keywords = {Case-Based Reasoning (CBR),Rule-Based System (RBS),Injection molding,Expert System,Artificial Intelligence (AI),Hybrid Expert System (HES)},
  pages = {458-462}
}

@article{bozdana_development_2002,
  title = {Development of an Expert System for the Determination of Injection Moulding Parameters of Thermoplastic Materials: {{EX}}-{{PIMM}}},
  volume = {128},
  issn = {0924-0136},
  shorttitle = {Development of an Expert System for the Determination of Injection Moulding Parameters of Thermoplastic Materials},
  abstract = {Injection moulding is a process in which a hot polymer melt is forced to flow into an empty, cold cavity of desired shape and then allowed to solidify under a high holding pressure. There are several parameters of the injection moulding process. It is not easy to handle the relationships between these parameters in order to obtain an effective and rapid moulding. The goal of this study is to develop a frame-based, modular and interactive expert system (called ``EX-PIMM'') for the determination of the injection moulding parameters of thermoplastic materials. The selection of the most suitable plastic injection moulding machine (PIMM) and thermoplastic material for the given job are the main objectives of this study. In addition, the optimum number of cavities can be determined according to the selection strategy used in this study.},
  number = {1\textendash{}3},
  urldate = {2016-10-19},
  journal = {Journal of Materials Processing Technology},
  doi = {10.1016/S0924-0136(02)00436-3},
  url = {http://www.sciencedirect.com/science/article/pii/S0924013602004363},
  author = {Bozdana, A. Tolga and Eyercio{\u g}lu, {\"O}mer},
  month = oct,
  year = {2002},
  keywords = {Injection Moulding,Plastic injection moulding machine,Thermoplastic materials,Expert System,Expert systems,material selection},
  pages = {113-122}
}

@phdthesis{chen_study_2002,
  address = {{Hong-Kong}},
  type = {{{PhD}}},
  title = {A Study on Profile Setting of Injection Molding},
  abstract = {Injection molded part quality depends strongly on the key processing conditions such as injection velocity during filling phase, packing pressure during packing-holding phase, and melt temperature during plastication phase. Profiling methods of those key parameters are studied in this thesis.

Injection velocity is profiled to keep a constant melt-front rate throughout the mold filling to produce uniform parts. It can be implemented by controlling the average-flow-length following a ramp. To measure the melt-position and also to collect data for modeling and control, a capacitive transducer is designed and experimentally tested for different materials and molds. Potential applications of such a transducer for the detections of V/P transfer, gate freezing-off point, and over-packing, are also discussed. As such a hardware transducer may not be conveniently installed in all molds, a soft-sensor is developed via a neural network model to correlate the average-flow-length with other online measurable variables. With this soft-sensor model, a profiling method based on optimization is proposed and experimentally verified.

Two parameters defining a packing pressure profile, the gate freezing-off time, and the shape and level during packing-holding, are studied in this thesis. An online detection system for the gate freezing is developed, with results matching well with the established off-line method. Influences of the packing profiles on part weight, evenness, shrinkage, and flash are studied in details. Different types of packing profiles, including constant, ramp, and step change profiles, are compared using different mold inserts. Based on the rules concluded from this extensive experimental study, profiling methods are proposed.

The melt temperature is affected by several parameters in plastication phase. A transparent and instrumented barrel is set up to observe and to promote the understanding of the melting and transport phenomenon in the injection plastication. The melt temperature is correlated to other molding parameters via a neural network model. For a given material and a required melt temperature, proper settings of the plastication conditions are obtained through optimization.},
  language = {English},
  urldate = {2016-10-31},
  school = {Hong Kong University of Science and Technology},
  url = {http://lbezone.ust.hk/bib/b774130},
  author = {Chen, Xi},
  month = aug,
  year = {2002},
  keywords = {capacitive transducer,dynamic neural network,injection speed,Injection velocity,Levenberg-Marquardt,neural network,nozzle pressure,nozzle temperature,packing pressure,part weight,plastification,pressure control,quality,velocity control,velocity profiles,weight,windows barrel}
}

@patent{laczko_controller_1975,
  address = {{Westwood, Massachusets}},
  title = {Controller for Injection Molding Machine},
  abstract = {In an injection molding machine there is provided control circuitry for monitoring a number of predetermined process conditions for controlling the quality and consistency of injection molded articles. The system comprises a hydraulic pressure transducer and a cavity pressure transducer employed in concert with the control circuitry which monitors the viscosity of the article presently being formed, provides a corrective adjustment during the present cycle when the measured viscosity deviates from a reference viscosity, compares the applied hydraulic pressure with a hydraulic reference pressure, and measuring the peak cavity pressure during one cycle in order to control the applied pressure during the subsequent cycle as a function of the previously measured peak cavity pressure.},
  nationality = {USA},
  language = {English},
  assignee = {Bbf Group Inc},
  number = {US 3893792 A},
  urldate = {2016-10-21},
  url = {http://www.google.com/patents/US3893792},
  author = {Laczko, Franck},
  month = jul,
  year = {1975}
}

@article{kamal_dynamics_1987,
  title = {Dynamics and Control of Pressure in the Injection Molding of Thermoplastics},
  volume = {27},
  issn = {1548-2634},
  abstract = {A detailed study was carried out to understand the dynamics of pressure variations at different points in the injection-molding system. Thus, hydraulic, nozzle, and cavity pressures were evaluated, in addition to the pressure gradient in the cavity. Both steps and pseudorandom binary sequences (PRBS) were employed to obtain and compare dynamic models describing these variables. Subsequently, these models were employed to evaluate and select optimal controllers for the different variables.},
  language = {en},
  number = {18},
  urldate = {2016-10-21},
  journal = {Polymer Engineering \& Science},
  doi = {10.1002/pen.760271809},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/pen.760271809/abstract},
  author = {Kamal, M. R. and Patterson, W. I. and Conley, N. and Abu Fara, Deeb and Lohfink, G.},
  month = oct,
  year = {1987},
  keywords = {PID,nozzle pressure,closed loop,stochastic,dynamics,Dahlin,pressure control,PRBS,optimal control},
  pages = {1403-1410},
}

@article{fara_evaluation_1985,
  title = {Evaluation of Simple Dynamic Models and Controllers for Hydraulic and Nozzle Pressure in Injection Molding},
  volume = {25},
  issn = {1548-2634},
  abstract = {Simple pseudo-steady state relations between the hydraulic and nozzle pressures of an injection molding machine were presented and verified experimentally. A simulation study was performed to evaluate the performance of simple controllers using dynamic models developed for the hydraulic and nozzle pressures. The controllers chosen were the discrete proportional, proportional-integral (PI), and proportional-integral-derivative (PID) types, tuned according to the ITAE criterion. The control of hydraulic pressure simulation showed that the PI controller had the best overall performance, whereas the result of nozzle pressure control loop simulation showed that the PID controller performance was better than that of the PI controller. All the controllers, in both loops, gave responses that were about an order of magnitude more rapid than the open loop response.},
  language = {en},
  number = {11},
  urldate = {2016-10-21},
  journal = {Polymer Engineering \& Science},
  doi = {10.1002/pen.760251109},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/pen.760251109/abstract},
  author = {Abu Fara, Deeb and Kamal, M. R. and Patterson, W. I.},
  month = aug,
  year = {1985},
  keywords = {PID,PI,hydraulic pressure,nozzle pressure,closed loop,stochastic},
  pages = {714-723}
}

@phdthesis{fara_control_1988,
  title = {Control of Nozzle and Cavity Pressure during Filling and Packing in Thermoplastics Injection Molding},
  copyright = {All items in eScholarship@McGill are protected by copyright with all rights reserved unless otherwise indicated.},
  lccn = {75857},
  abstract = {Thermoplastics injection molding involves plastication followed by the injection of the melt into a cold cavity. Packing is employed to compensate for shrinkage due to cooling. Ultimately, the solidified part is ejected from the mold without damage. The successful operation of an injection molding machine requires control of the process variables during each of the consecutive stages in addition to correctly identifying the points of transition from one stage to the next. Pressure and its variation during the injection molding cycle play an important role with regard to productivity, product quality, and product reproducibility. From the practical point of view, it is necessary to consider simultaneously hydraulic pressure, nozzle pressure, and the distribution of pressure in the cavity. Control of each phase of the injection molding process is best achieved by controlling one or a combination of the above pressure parameters. The present work describes a comprehensive study of the dynamics and control of pressure during each stage of the injection molding cycle. Deterministic models were obtained for cavity gate pressure during the filling and packing stages. Dynamic model predictions were in good agreement with experimental data. The response of cavity gate pressure exhibited nonlinear behavior which was investigated and rectified by a gain scheduling control strategy. Stochastic models were obtained for cavity gate pressure response in the filling stage for the purpose of comparison and future design of more advanced control algorithms The dynamic models were employed to design and evaluate control schemes for the injection molding cycle. Nozzle and cavity pressures were used in conjunction with PI, PID and Dahlin controllers. The hydraulic system of the injection molding machine was redesigned to incorporate two servovalves in order to achieve control over the cavity pressuretime profile during the packing stage as well as over peak cavity and hold pressures. The control loops were designed through a simulation study which also gave good indications of system limitations. On the basis of this study, very good and reliable integrated control over the filling, packing, and holding stages was achieved by a general control scheme which allows the transfer of control from one variable to another during the various stages of the process.},
  language = {English},
  urldate = {2016-10-21},
  url = {http://digitool.library.mcgill.ca/R/?func=dbin-jump-full\&object_id=75857\&local_base=GEN01-MCG02},
  author = {Abu Fara, Deeb},
  month = aug,
  year = {1988},
  keywords = {PI,hydraulic pressure,nozzle pressure,dynamics,cavity pressure control,Cavity pressure}
}

@inproceedings{fara_comprehensive_1990,
  title = {Comprehensive {{Strategies}} for {{Sequential Closed Loop Pressure Control Throughout}} the {{Injection Molding Cycle}}},
  volume = {36},
  language = {English},
  booktitle = {{{SPE ANTEC Papers}}},
  publisher = {{Society of Plastics Engineers}},
  author = {Abu Fara, Deeb and Kamal, M.R. and Patterson, W.I.},
  year = {1990},
  keywords = {closed loop,pressure control},
  pages = {239}
}

@phdthesis{kazmer_dynamic_1995,
  title = {Dynamic Feed Control: A New Method for Injection Molding of High Quality Plastic Parts},
  abstract = {invention utilizes multiple valves in the feed system of a mold to selectively regulate the flow to each area of the cavity in response to real-time feedback from nearby cavity pressure transducers
Dynamic feed control: a new method for injection molding of high quality plastic parts
Analysis of the experimental results showed an increase in the process capability, Cp, from 0.56 for the conventional molding process to 1.67 for Dynamic Feed Control.},
  school = {Citeseer},
  author = {Kazmer, David Owen and Barkan, P},
  year = {1995},
  keywords = {cavity pressure control,Process control,Design methodology,process capability,control strategy,dynamic fedd control,matlab,Closed-loop,state-space,switchover point control,gain-schelduling,multiple valves}
}

@article{reilly_assessment_2001,
  title = {An Assessment of {{Dynamic Feed}} Control in Modular Tooling},
  volume = {5},
  issn = {1533-905X},
  language = {eng},
  number = {1},
  urldate = {2016-10-19},
  journal = {The Journal of injection molding technology},
  url = {http://cat.inist.fr/?aModele=afficheN\&cpsidt=1013233},
  author = {Reilly, Jim F. and Doyle, Mark and Kazmer, David Owen},
  year = {2001},
  keywords = {Design of experiment,closed loop,capability,multi-cavity,dynamic feed control,modular tooling,Taguchi methods},
  pages = {49-60}
}

@article{gomes_injection_1986,
  title = {An Injection Molding Study. {{Part II}}: {{Evaluation}} of Alternative Control Strategies for Melt Temperature},
  volume = {26},
  issn = {1548-2634},
  shorttitle = {An Injection Molding Study. {{Part II}}},
  abstract = {Melt temperature is a key variable of the injection molding process. The evaluation of different control strategies for the melt temperature was based on the process dynamics presented in Part I of this work. The controllers tested were: PID (proportional-integral-derivative) control of barrel temperatures, direct PID control of melt temperature, Dahlin control, and Smith Predictor control of melt temperature. The controllers were tuned by simulation studies and were evaluated by a comparison of performance indices. The controller responses obtained through experiments were evaluated by comparing the rise time, peak overshoot, damping, settling time, and control effort. Analyses were performed to ascertain the sensitivity characteristics of the controllers.},
  language = {en},
  number = {12},
  urldate = {2016-10-21},
  journal = {Polymer Engineering \& Science},
  doi = {10.1002/pen.760261206},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/pen.760261206/abstract},
  author = {Gomes, V. G. and Patterson, W. I. and Kamal, M. K.},
  month = jul,
  year = {1986},
  keywords = {PID,closed loop,temperature,melt temperature,Dahlin,barrel temperatures},
  pages = {867-876}
}

@article{kamal_injection_1986,
  title = {An Injection Molding Study. {{Part I}}: {{Melt}} and Barrel Temperature Dynamics},
  volume = {26},
  issn = {1548-2634},
  shorttitle = {An Injection Molding Study. {{Part I}}},
  abstract = {The dynamics of the key variables in the plastication phase of injection molding were studied using a microcomputer controlled laboratory scale injection molding machine. Interfaces for the measurement and manipulation of suitable variables were developed. An approximate theoretical analysis provided a preliminary understanding of the effect of a step input in heating power on melt and barrel temperatures. Deterministic and stochastic models were derived from experimental data for the melt and the front and rear zone barrel temperatures with heater power manipulation. Experiments were performed for evaluating the effects of back pressure manipulation, heating zone interaction, and process disturbances on the melt and barrel temperatures. Experiments showed that it is desirable to directly control the melt temperature rather than the barrel temperatures.},
  language = {en},
  number = {12},
  urldate = {2016-10-26},
  journal = {Polymer Engineering \& Science},
  doi = {10.1002/pen.760261205},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/pen.760261205/abstract},
  author = {Kamal, M. R. and Patterson, W. I. and Gomes, V. G.},
  month = jul,
  year = {1986},
  keywords = {stochastic,temperature,dynamics,melt temperature},
  pages = {854-866}
}

@inproceedings{gustafson_model_1987,
  title = {Model Predictive Control ({{MPC}}) of Injection Molding Machines},
  abstract = {The problem of on-line control of plastic injection molding machines is considered. The specific problem studied is accurate regulation of melt temperature which has a significant effect on quality of the molded parts. A robust system identification and control mothodology is developed which uses canonical variates analysis for identification and model predictive control for regulation. Data collected from an operating machine are used for identification of control system models in two operating states. Temperature regulation performance is analyzed using simulations in which significant disturbances in machine variables and noise are introduced. The results indicate that the proposed methodology is superior to current PID controllers and can be easily implemented in current digital process control computers.},
  urldate = {2016-10-21},
  booktitle = {26th {{IEEE Conference}} on {{Decision}} and {{Control}}},
  publisher = {{IEEE}},
  doi = {10.1109/CDC.1987.272888},
  url = {http://ieeexplore.ieee.org/document/4049654/},
  author = {Gustafson, Donald and Lebow, William},
  month = dec,
  year = {1987},
  keywords = {Injection molding,PID,temperature,melt temperature,Predictive models,Predictive control,Control system synthesis,Plastics,Robust control,System identification,Performance analysis,Computational modeling},
  pages = {2017-2026}
}

@article{pandelidis_optimal_1988,
  title = {Optimal Anticipatory Control of Ram Velocity in Injection Molding},
  volume = {28},
  issn = {1548-2634},
  abstract = {This paper discusses a computer control system for ram velocity of an injection molding machine using optimal state feedback based on the linear quadratic control theory. A new approach for the selection of appropriate weighting matrices is presented in this context. The simulation results reveal that the optimal controller has improved performance over the conventional PID controller presently used, having faster speed of response, significantly better tracking performance, and better noise filtering properties. The execution speed and the core storage requirements would allow implementation even on a small online computer.},
  language = {en},
  number = {3},
  urldate = {2016-10-21},
  journal = {Polymer Engineering \& Science},
  doi = {10.1002/pen.760280305},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/pen.760280305/abstract},
  author = {Pandelidis, I. O. and Agrawal, A. R.},
  month = feb,
  year = {1988},
  keywords = {PID,optimal control,optimal anticipatory,ram velocity,disturbance,LQR},
  pages = {147-156}
}

@article{demirci_numerical_1997,
  title = {A {{Numerical}} and {{Experimental Investigation}} of {{Neural Network}}-{{Based Intelligent Control}} of {{Molding Processes}}},
  volume = {119},
  issn = {1087-1357},
  abstract = {The current investigation focused on the development of intelligent injection molding processes by utilizing a neural network based control unit. In this study, the emphasis was on the control of flow front progression during injection molding processes. The progression of a flow front into a mold, cavity is crucial since it dictates the locations of possible air voids and weld lines. It is desired that the flow front progresses towards the vent locations and that weld lines coincide with locations where their quality decreasing influence has a minimum impact on the overall part performance. The intelligent control scheme developed is based on a neural network that was trained with data obtained from a first-principles based process model rather than actual molding experimentation. The control strategy was developed such that one can specify a desired flow progression scheme and the controller will take corrective actions during the molding process to realize this scheme. This is done by controlling the inlet flow rate at various inlet gate locations. Experiments were conducted with a 2-D, complex shaped, mold cavity to test the performance of the control unit during actual injection molding processes. The mold had two inlet gates and three different desired flow progression schemes were considered. In all cases, the first principles model/neural network based control unit was able to steer the flow front along the corresponding desired flow progression path.},
  number = {1},
  urldate = {2016-10-31},
  journal = {Journal of Manufacturing Science and Engineering},
  doi = {10.1115/1.2836559},
  url = {http://dx.doi.org/10.1115/1.2836559},
  author = {Demirci, H. H. and Coulter, John P. and G{\"u}{\c c}eri, S. I.},
  month = feb,
  year = {1997},
  keywords = {back-propagation,neural networks,BPN},
  pages = {88-94}
}

@article{woll_pattern-based_1997,
  title = {Pattern-Based Closed-Loop Quality Control for the Injection Molding Process},
  volume = {37},
  issn = {1548-2634},
  abstract = {The basis for a novel pattern-based closed-loop control strategy for the injection molding process is presented. The strategy uses artificial neural networks (ANNs) embedded within a cascade design to analyze sensor patterns, identify process character and control part quality. The platform for this work, the injection molding process, is an industrially significant, cyclic manufacturing operation. Final part quality of this process is a nonlinear function of many machine and polymer variables. Part quality control of this process is currently attained via single input\textendash{}single output machine controls supervised by human operators. Presented here is a method that employs ANN technology to improve upon this approach and provide the basis for closed-loop part quality control. In the cascade design, machine controller set-points of an inner loop are updated based on ANN analysis of mold cavity pressure patterns. The controller action maintains the desired pressure pattern set-point of the outer loop associated with desired part quality. Control strategy details are provided along with set-point tracking demonstrations that support feasibility of this pattern-based approach.},
  language = {en},
  number = {5},
  urldate = {2016-09-28},
  journal = {Polymer Engineering \& Science},
  doi = {10.1002/pen.11723},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/pen.11723/abstract},
  author = {Woll, Suzanne L. B. and Cooper, Douglas J.},
  month = may,
  year = {1997},
  keywords = {closed loop,Quality control,cavity pressure control,velocity control,neural network,Pattern recognition,Backpropagation,Length measurement},
  pages = {801-812}
}

@article{michaeli_online_2009,
    title = {Online Control of the Injection Molding Process Based on Process Variables},
    volume = {28},
    issn = {1098-2329},
    abstract = {The conventional control of the injection molding process is based on machine variables, which cannot sufficiently characterize the course of the process. Hence, a system that controls the injection molding process based on process variables has been developed at the Institute of Plastics Processing at RWTH Aachen University during the last years. It controls the quality determining process variable cavity pressure directly and realizes a desired course of cavity pressure in the injection and holding pressure phases. The cavity pressure course in the holding pressure phase is controlled online on the basis of pvT behavior of the processed plastic material. Thus, an optimal course of the process in the pvT diagram can be guaranteed and the quality constancy of the molded parts can be clearly increased. Using the pvT-based process control, the effect of varying mold and melt temperatures on the molded part weight can be decreased by about 90\% compared with the conventional process control. \textcopyright{} 2009 Wiley Periodicals, Inc. Adv Polym Techn 28:65\textendash{}76, 2009; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/adv.20153},
    language = {en},
    number = {2},
    urldate = {2016-09-19},
    journal = {Advances in Polymer Technology},
    doi = {10.1002/adv.20153},
    url = {http://onlinelibrary.wiley.com/doi/10.1002/adv.20153/abstract},
    author = {Michaeli, Walter and Schreiber, Andreas},
    month = jun,
    year = {2009},
    keywords = {Thermoplastics,Computer modeling,Injection molding,pressure control,Cavity pressure,neural network,Artificial neural network,pvT behavior},
    pages = {65-76}
}

@book{landau_adaptive_1979,
  address = {{New York, NY, USA}},
  title = {Adaptive {{Control}}: {{The Model Reference Approach}}},
  isbn = {978-0-8247-6548-4},
  shorttitle = {Adaptive {{Control}}},
  publisher = {{Marcel Dekker, Inc.}},
  author = {Landau, Ioan Doré},
  year = {1979}
}

@book{egardt_stability_1979,
  address = {{Berlin, Heidelberg}},
  title = {Stability of {{Adaptive Controllers}}},
  isbn = {978-0-387-09646-9},
  publisher = {{Springer-Verlag}},
  author = {Egardt, Bo},
  editor = {Thoma, M. and Balakrishnan, A. V.},
  year = {1979}
}

@article{sanschagrin_process_1983,
  title = {Process Control of Injection Molding},
  volume = {23},
  issn = {1548-2634},
  abstract = {This paper discusses the most important input parameters affecting the conventional injection-molding process and describes a closed-loop control system for determining the interaction between ten process inputs and three output parameters. The input parameters are: back pressure, holding pressure, injection time, open mold time, shot size, clamping pressure, injection pressure, screw speed, and boost cut-off. The output parameters studied are: part weight, maximum cavity pressure, and maximum mold deflection.},
  language = {en},
  number = {8},
  urldate = {2016-10-19},
  journal = {Polymer Engineering \& Science},
  doi = {10.1002/pen.760230804},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/pen.760230804/abstract},
  author = {Sanschagrin, Bernard},
  month = jun,
  year = {1983},
  pages = {431-438}
}

@phdthesis{bethoux_approche_1976,
  address = {{Grenoble, France}},
  type = {{Th{\`e}se 3{\`e}me cycle}},
  title = {{Approche unitaire des m{\'e}thodes d'identification et de commande adaptative des proc{\'e}d{\'e}s dynamiques}},
  abstract = {The identification method originally used by Bethoux (13)for study of papermaking and distillation processes was adapted for this purpose. The molding process is defined by the equation:
As such, Theorem 4.1 is not applicable despite that the noise term is white. However, for the case C(q-1) and D(q-1) known, (5.52) reduces to:
{$\epsilon$} ( t + 1 , \texttheta\textasciicircum{} ) = D ( q - 1 ) [ \texttheta{} 1 - \texttheta\textasciicircum{} 1 ] T {$\varphi$} 1 ( t , \texttheta\textasciicircum{} ) (5.53) C(q-1)
and applying Theorem 4.1, one has the convergence condition:
H {${'}$}(z-1) = D(z-1) - {$\lambda$}2 (5.54)
C(z-1) 2
is a strictly positive real transfer function.
As such, this algorithm is an extension of the GLS algorithm of Bethoux (1976)
and Landau (1990a) which corresponds to the case C(q-1) = 1.
This algorithm is more appropriate to be used than the ELS, for the case where
the disturbance term in (5.37) has a narrow frequency spectrum which can usually be modeled as 1 e(t) where D(q-1) is a low-damped second-order polyno- D(q-1)
mial (while using a model of the form C(q-1)e(t) will require a large number of parameters to be identified).},
  language = {Fran{\c c}ais},
  school = {Institut National Polytechnique de Grenoble},
  author = {Bethoux, Guy},
  year = {1976}
}

@phdthesis{devos_contribution_1990,
  address = {{Universit{\'e} des Sciences et Techniques de Lille - Flandres - Artois}},
  type = {{Doctorat}},
  title = {{Contribution {\`a} l'optimisation de la conduite d'une presse {\`a} injecter les polym{\`e}res thermoplastiques}},
  abstract = {Pierre Devos (Ing{\'e}nieur de l'Industrie et des Mines {\`a} l'Ecole des Mines de Douai). Confidentiel. Directeur : Pierre Borne. Rapporteurs : G. Dauphin-Tanguy, M. Staroswiecki. Examinateurs : S. Degallaix, J.P. Hautier, J. Lienard, J. Pabiot.
Ce travail a {\'e}t{\'e} r{\'e}alis{\'e} dans les laboratoires et les ateliers du D{\'e}partement
Technologie des Polym{\`e}res et Composites de 1'Ecole des Mines de Douai, en collaboration avec la Soci{\'e}t{\'e} X. Il l s'agit d'une analyse param{\'e}trique du processus d'injection sous haute pression des polym{\`e}res thermoplas- tiques.
Ce travail est bas{\'e} d'une part sur l'instrumenta- tion d'une presse d'injection et de son moule, d'autre part sur la mise en place d'un auxiliaire de commande particulier.
L'objectif est une am{\'e}lioration des performances des machines qui doit permettre de diminuer les taux de rebuts qui affectent la productivit{\'e} des presses, et d'assurer un meilleur contr{\^o}le des dimensions des pi{\`e}ces.},
  language = {Fran{\c c}ais},
  school = {Universit{\'e} des Sciences et Techniques de Lille - Flandres - Artois},
  author = {Devos, Pierre},
  month = jul,
  year = {1990},
  keywords = {Process monitoring,PID,pressure control,PVT,Process control,process parameters,physical properties,process measurements,Adaptive control,masse,géométrie}
}

@phdthesis{tsoi_fuzzy_1997,
  type = {Thesis},
  title = {A Fuzzy Logic Controller ({{FLC}}) for Ram Velocity in Injection Molding},
  abstract = {Injection ram velocity to a large degree determines the melt injection rate during the injection phase in an injection molding process, and has strong influences on the molded part quality, such as shrinkage, warpage, and impact strength. An injection molding machine operates under strongly different operating conditions, such as different set-point profiles, barrel temperatures, molds and materials. This causes the ram velocity dynamics to vary significantly, and consequently results in poor control performance for a typical PID controller.

This thesis presents a computer control system for the injection ram velocity using a real-time fuzzy logic controller (FLC), together with a fuzzy feedforward controller (FFC). The rule base of the FLC is optimized by analyzing the phase plane characteristics and, the optimal membership functions of FLC are based on the 2k factors design technique. The experimental results reveal that the controller has improved performance over the conventional PID controller, in the response speed, set-point tracking ability, noise rejection, and robustness.

In this study, the non-linearity and time-varying characteristics of the injection ram velocity have been investigated by an experimental model as well as a simplified physically-based model.

Finally, the possibility of applying an adaptive fuzzy controller (AFC) to the ram velocity control is explored in this study. An AFC is designed and tested for the control of ram velocity during filling. The experimental results show that the controller worked in the cup mold, but failed in the modified flat mold. A possible reason for this may be the strong effect of different molds on the dynamics of ram velocity and the use of cup mold data for the adaptive rate y and the input and output variables fuzzy sets. The application of AFC to injection molding may give a direction for further study.},
  language = {English},
  url = {http://lbezone.ust.hk/bib/b566080\#},
  author = {Tsoi, Hoi-Pang},
  year = {1997},
  keywords = {fuzzy controller,fuzzy logic,ram velocity control}
}

@inproceedings{tsoi_real-time_1997,
  address = {{Hong-Kong}},
  title = {A {{Real}}-Time {{Fuzzy Logic Controller}} for {{Ram Velocity}} in {{Injection Molding}}},
  abstract = {4th International Conference on Manufacturing Technology in Hong Kong, CD-ROM , November30 - 3 December 1997},
  language = {English},
  booktitle = {4th {{International Conference}} on {{Manufacturing Technology}}},
  url = {http://hdl.handle.net/1783.1/33977},
  author = {Tsoi, Hoi-Pang and Gao, Furong},
  month = dec,
  year = {1997}
}

@article{huang_fuzzy_2000,
  title = {Fuzzy Logic Controller for a Retrofitted Closed-Loop Injection Moulding Machine},
  volume = {214},
  issn = {0959-6518, 2041-3041},
  abstract = {For the purposes of producing precise products and providing the flexibility for products variation, an open-loop control commercial injection moulding machine is retrofitted into a closed-loop control system for monitoring the filling and post-filling phases of the injection processes. The hydraulic control unit, control and interface circuits, safety limit switches and personal-computer-based controller were redesigned and constructed. Since the injection moulding processes have complicated non-linear dynamics and model uncertainty, the application of a classical proportional-integral-derivative controller has adaptivity and robustness problems, especially with regards to the pressure control during compression phase. Here, an intelligent fuzzy controller is employed to adjust the injection speed of the filling phase and to control the nozzle pressure of the post-filling phase. The experimental results show that this controller has good performance in actual injection moulding processes.},
  language = {en},
  number = {1},
  urldate = {2016-10-21},
  journal = {Proceedings of the Institution of Mechanical Engineers, Part I: Journal of Systems and Control Engineering},
  doi = {10.1243/0959651001540483},
  url = {http://pii.sagepub.com.camphrier-2.grenet.fr/content/214/1/9},
  author = {Huang, S.-J. and Lee, T.-H.},
  month = feb,
  year = {2000},
  keywords = {injection moudling machine,nozzle pressure,closed loop,fuzzy controller,injection speed,filling and post-filling phases},
  pages = {9-22}
}

@inproceedings{sherbelis_methods_1997,
  title = {The Methods and Benefits of Establishing a Process Window},
  abstract = {Setting and control of the injection molding process is unique in that quality is multi-dimensional and discrete (not continuous), the process is stochastic (not deterministic), and that quality data is both non-uniform and sparse. The proposed approach automatically searches the process space and efficiently locates a defect free process zone suitable for production, i.e. a Process Window. The performance of the method is then compared to a multi-objective optimization which uses a Simplex search method. Finally, the paper presents some molding results using the system and quantifies the impact of process improvements on quality and productivity.},
  booktitle = {Submitted for the 1997 {{Annual Technical Conference}} of the {{Society}} of {{Plastics Engineers}}},
  author = {Sherbelis, Gal and Garvey, Emlyn and Road, Colchester and Kazmer, David Owen},
  month = apr,
  year = {1997},
  keywords = {Design of experiment,Process Window,quality model,process quality model,machine capability,capability}
}

@book{berins_spi_1991,
  title = {{{SPI Plastics Engineering Handbook}} of the {{Society}} of the {{Plastics Industry}}},
  isbn = {978-1-4615-7606-8},
  abstract = {I am pleased to present the Fifth Edition of the Plastics Engineering Handbook. Last published in 1976, this version of the standard industry reference on...},
  urldate = {2016-11-09},
  publisher = {{Kluwer Academic}},
  url = {http://www.springer.com/us/book/9781461576068},
  author = {Berins, Michael L.},
  year = {1991}
}

@article{kazmer_comparison_2010,
  title = {A Comparison of Seven Filling to Packing Switchover Methods for Injection Molding},
  volume = {50},
  issn = {1548-2634},
  abstract = {The effectiveness of seven methods for controlling switchover from the filling to packing stage were investigated, including: (1) screw position, (2) injection time, (3) machine pressure, (4) nozzle pressure, (5) runner pressure near the sprue, (6) cavity pressure near the gate, and (7) cavity temperature at the end of flow. The activation threshold for each of the seven switchover methods was iteratively determined so as to produce similar part weights relative to a standard process. A design of experiments was implemented for each of the seven switchover methods that perturbs the process settings by an amount equal to six standard deviations of the standard process so as to replicate the expected long-term process variation. The results suggest that conventional switchover methods (e.g., screw position) had lower short-term variation, but other methods were more robust with respect to rejecting long-term process variation. The merits of different dimensional measurements for quality control are also discussed relative to the society of the plastics industry (SPI) standard toleranc
es. POLYM. ENG. SCI., 50:2031\textendash{}2043, 2010. \textcopyright{} 2010 Society of Plastics Engineers},
  language = {en},
  number = {10},
  urldate = {2016-10-19},
  journal = {Polymer Engineering \& Science},
  doi = {10.1002/pen.21731},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/pen.21731/abstract},
  author = {Kazmer, David Owen and Velusamy, Suganya and Westerdale, Sarah and Johnston, Stephen and Gao, Robert X.},
  month = oct,
  year = {2010},
  keywords = {switchover point control,switchover},
  pages = {2031-2043}
}

@techreport{ISO_20457_2018,
  address = {{Geneva, Switzerland}},
  type = {Standard},
  title = {{{ISO}} 20457:2018: {{Plastics moulded parts}} \textendash{} {{Tolerances and acceptance conditions}}},
  shorttitle = {{{ISO}} 20457:2018({{E}})},
  abstract = {ISO 20457:2018 specifies possible manufacturing tolerances for plastic moulded parts.

This document specifies all integral features with general tolerances with surface profile tolerance within a specified datum system. It allows for additional specifications in case of functional needs and requirements using the ISO-GPS-tools for dimensional and geometrical tolerating.

This document addresses injection moulding, injection compression moulding, transfer moulding, compression moulding and rotational moulding of non-porous moulded parts made from thermoplastics, thermoplastic elastomers and thermosets of thermoplastics. This document is applicable to other plastic processes if agreed to by the contractual parties.

Moulded part surface imperfections such as sink marks, undesired flow structures and roughness, as well as joint lines are not addressed in this document.

This document is not intended to supplant, replace or in any way interfere with requirements for tolerances found in product standards.},
  language = {English},
  number = {ISO 20457:2018(E)},
  urldate = {2019-09-04},
  institution = {{International Organization for Standardization / International Organization for Standardization}},
  url = {https://www.iso.org/standard/68097.html},
  author = {{{ISO/TC} 261 }},
  month = sep,
  year = {2018},
  pages = {28},
  price = {CHF 138}
}

@techreport{afnor_nf_1987,
  type = {Standard},
  title = {{{NF T58}}-000},
  abstract = {Tol{\'e}rances applicables aux pi{\`e}ces moul{\'e}es en plastiques (thermodurcissables et thermoplastiques)},
  language = {English},
  urldate = {2016-11-22},
  institution = {{AFNOR}},
  url = {http://www.norme-standard.com/tag/nf-t58-000/},
  author = {Association Française de Normalisation (AFNOR)},
  month = oct,
  year = {1987}
}

@article{lu_stagebased_2004,
  series = {7th {{International Symposium}} on {{Advanced Control}} of {{Chemical Processes}} ({{ADCHEM}} 2003), {{Hong}}-{{Kong}}, 11-14 {{January}} 2004},
  title = {Stage-{{Based Multivariate Statistical Analysis}} for {{Injection Molding}}},
  volume = {37},
  issn = {1474-6670},
  abstract = {A multi stage based PCA modelling and monitoring approach is demonstrated in this paper to injection molding process, a typical multi stage batch process. Analysing the changes of process correlation can lead to effective division of a batch process into several "operation" stages, in good agreement with process knowledge. This shows that multistage based sub-PCA model can be employed not only for effectively process monitoring and fault diagnosis, but also to enhance process understanding.},
  number = {1},
  urldate = {2016-10-24},
  journal = {IFAC Proceedings Volumes},
  doi = {10/gf7jxc},
  url = {http://www.sciencedirect.com/science/article/pii/S1474667017387712},
  author = {Lu, Ningyun and Yang, Yi and Gao, Furong and Wang, Fuli},
  month = jan,
  year = {2004},
  keywords = {Injection molding,Multistage batch process,Multivariate statistical analysis,Principal component analysis,Process monitoring},
  pages = {439-444}
}

@article{kazmer_comparison_2008,
  title = {A {{Comparison}} of {{Statistical Process Control}} ({{SPC}}) and {{On}}-{{Line Multivariate Analyses}} ({{MVA}}) for {{Injection Molding}}},
  volume = {23},
  issn = {0930-777X},
  abstract = {Manufacturing process automation is often impeded by limitations related to automatic quality assurance. Many plastics manufacturers use univariate statistical process control (SPC) for quality control by charting the critical process states relative to defined control limits. Alternatively, principal component analysis (PCA) and projection to latent stuctures (PLS) are multivariate methods that measure the process variance by the distance to the model (DModX) and the Hotelling t-squared (T2) values. A methodology for robust model development is described to perturb the manufacturing process for process characterization based on a design of experiments; best subset analysis is used to provide an optimal set of regressors for univariate SPC. Four different statistical models were derived from the same data set for a highly instrumented injection molding process. The performance of these models was then assessed with respect to fault diagnosis and defect identification when the molding process was subjected to twelve common process faults. Across two hundred molding cycles, the univariate SPC models correctly diagnosed five of the twelve process faults with one false positive, detecting only eighteen of twenty four defective products while indicating two false positives. With the same molding cycles, PCA and PLS provided nearly identical performance by correctly diagnosing ten of the twelve process faults and detecting twenty three of the twenty four defective products; PCA indicated two false positives while PLS indicated only one false positive.},
  number = {5},
  urldate = {2016-09-19},
  journal = {International Polymer Processing},
  doi = {10.3139/217.2192},
  url = {http://www.hanser-elibrary.com/doi/abs/10.3139/217.2192},
  author = {Kazmer, David Owen and Westerdale, S. and Hazen, D.},
  month = nov,
  year = {2008},
  keywords = {PCA,SPC,PLS,Multivariable,MVA,multivariable analysis},
  pages = {447-458}
}

@article{shewhart_economic_1930,
  title = {Economic {{Quality Control}} of {{Manufactured Product}}},
  volume = {9},
  issn = {1538-7305},
  abstract = {That we cannot nuke all pieces of a given kind of product identically alike is accepted as a general truth. It follows that the qualities of pieces of the same kind of product differ among themselves, or, in other words, the quality of product must be expected to vary. The causes of this variability are, in general, unknown. The present paper presents a scientific basis for determining when we have gone as far as it is economically feasible to go in eliminating these unknown or chance causes of variability in the quality of a product, When this state has been reached, the product is said to be controlled because it is then possible to set up limits within which the quality may be expected to remain in the future. By securing control, we attain the five economic advantages discussed in Part III.},
  language = {en},
  number = {2},
  urldate = {2016-10-28},
  journal = {Bell System Technical Journal},
  doi = {10.1002/j.1538-7305.1930.tb00373.x},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/j.1538-7305.1930.tb00373.x/abstract},
  author = {Shewhart, Walter Andrew},
  month = apr,
  year = {1930},
  pages = {364-389}
}

@book{shewhart_economic_1931,
  address = {{New York, NY, USA}},
  title = {Economic {{Control}} of {{Quality}} of {{Manufactured Product}}},
  isbn = {978-0-87389-076-2},
  abstract = {When Walter A. Shewhart (the father of modern quality control) described his book as...an indication of the direction in which future developments may be expected to take place, could he have forseen its enormous impact? This monumental work laid the foundation for this modern quality control discipline, and it remains as current today as ever. it began as an attempt to develop a scientific basis for attaining economic control of quality through the establishment of control limits to indicate when the quality of product is varying more than is economically desirable. In his search for better knowledge of economy in manufacture, Shewhart touches upon all aspects of statistical quality control. the book includes a presentation of the fundamental concepts and advantages of statistical control; ways of expressing quality of product (a section containing a discourse that has been described as a masterpiece on the meaning of quality); the basis for specification of quality control; sampling fluctuations in quality; allowable variability in quality (which contains the first fully developed use of control charts); and quality control in practice. This is required reading for anyone seriously interested in the study of quality control. About the Author: the father of modern quality control, Walter A. Shewhart brought together the disciplines of statistics, engineering, and economics in a simple but highly effective tool: the control chart. This technique, and the principles behind it, have played key roles in economic developments from the 1940\&\#39;s through to the present day. Most of Shewhart\&\#39;s professional career was spent at Western Electric as an engineer from 1918 to 1924 and at Bell Telephone Laboratories from 1925 until his retirement in 1956. In addition, he served for more than 20 years as the first editor of the Mathematical Statistics Series published by John Wiley \&amp; Sons.},
  language = {en},
  publisher = {{Van Nostrand}},
  url = {http://archive.org/details/in.ernet.dli.2015.150272},
  author = {Shewhart, Walter Andrew},
  month = jan,
  year = {1931},
  keywords = {Business \& Economics / Economics / General}
}

@article{johnston_-line_2015,
  title = {On-Line Multivariate Optimization of Injection Molding},
  volume = {55},
  issn = {00323888},
  language = {en},
  number = {12},
  urldate = {2016-09-19},
  journal = {Polymer Engineering \& Science},
  doi = {10.1002/pen.24163},
  url = {http://doi.wiley.com/10.1002/pen.24163},
  author = {Johnston, Stephen and McCready, Christopher and Hazen, Daniel and VanDerwalker, Darin and Kazmer, David Owen},
  month = dec,
  year = {2015},
  keywords = {Principal component analysis,PCA,cycle time,design of experiments,Transfert function,design of,Feedback control,linear time invariant},
  pages = {2743-2750}
}

@inproceedings{pillet_maitrise_2003,
  title = {Ma{\^i}trise {{Statistique}} Des {{Proc{\'e}d{\'e}s}} ({{MSP}}) - {{Cas}} Particulier de l'injection Plastique},
  abstract = {Th{\`e}me propos{\'e} : Contr{\^o}le et Mesure R{\'E}SUM{\'E}. L'objet de cette communication porte sur le cas particulier de la mise en place de la MSP dans le cas de l'injection plastique (syst{\`e}me multi-g{\'e}n{\'e}rateurs). Les r{\'e}sultats expos{\'e}s proviennent d'une collaboration entre l'Universit{\'e} de Savoie et plusieurs entreprises sp{\'e}cialistes de l'injection. Nous montrerons les difficult{\'e}s de mise en place de la MSP traditionnelle dans ce cas particulier et pr{\'e}senterons les outils que nous avons propos{\'e}s pour r{\'e}pondre aux besoins des industriels. Une carte de contr{\^o}le sp{\'e}cialement adapt{\'e}e sera pr{\'e}sent{\'e}e, ainsi qu'une proc{\'e}dure de d{\'e}termination des r{\`e}gles de pilotage {\`a} partir de plan d'exp{\'e}riences. Nous aborderons {\'e}galement le probl{\`e}me du calcul et de l'interpr{\'e}tation des capabilit{\'e}s qui diff{\`e}re tr{\`e}s sensiblement du cas standard. ABSTRACT. This communication relates the particular case of the Statistical Process control in the case of the injection plastic (system multi-generators). The exposed results come from collaboration between the University of Savoy and several injection companies. We will show the difficulties that appear with the implementation of the SPC in this particular case and will present the tools proposed to meet the need for the professionals. A control chart especially adapted will be presented. A procedure of determination of the pilot rules starting from an experimental design will be presented. We take into account the problem of the calculation and interpretation of capabilities, which differs very appreciably from the standard case. MOTS-CL{\'E}S : Ma{\^i}trise Statistique des Proc{\'e}d{\'e}s, carte de contr{\^o}le, aptitude, capabilit{\'e}, injection plastique, multi-g{\'e}n{\'e}rateurs.},
  author = {Pillet, Maurice and Maire, Jean Luc and Bronet, Vincent},
  month = oct,
  year = {2003}
}

@article{liu_window-based_2016,
  title = {Window-{{Based Stepwise Sequential Phase Partition}} for {{Nonlinear Batch Process Monitoring}}},
  volume = {55},
  issn = {0888-5885},
  abstract = {In this paper, a window-based stepwise sequential phase partition method is proposed to improve monitoring performance for nonlinear batch processes with multiphase operations. The three-dimensional information matrix of batch operation is unfolded and normalized in the batch-wise direction to facilitate establishing the kernel principal component analysis (KPCA) models for phase partition. A moving window is introduced to improve the partition performance, with respect to the process dynamics and time sequence for operation. Consequently, phase partition algorithms are developed for even- and uneven-length batch processes, respectively. Moreover, a traversal algorithm is given to determine the optimal choice of the KPCA parameters and the window size for phase partition. A numerical case and two industrial multiphase processes of injection molding and penicillin fermentation are used to demonstrate the effectiveness and merit of the proposed phase partition method.},
  number = {34},
  urldate = {2016-10-24},
  journal = {Industrial \& Engineering Chemistry Research},
  doi = {10.1021/acs.iecr.6b01257},
  url = {http://dx.doi.org/10.1021/acs.iecr.6b01257},
  author = {Liu, Jingxiang and Liu, Tao and Zhang, Jie},
  month = aug,
  year = {2016},
  keywords = {Process monitoring,Process Window,Phase partition,nonlinear systems},
  pages = {9229-9243}
}

@article{zhang_statistical_2016,
  title = {A Statistical Quality Monitoring Method for Plastic Injection Molding Using Machine Built-in Sensors},
  volume = {85},
  issn = {0268-3768, 1433-3015},
  language = {en},
  number = {9-12},
  urldate = {2016-09-19},
  journal = {The International Journal of Advanced Manufacturing Technology},
  doi = {10.1007/s00170-015-8013-2},
  url = {http://link.springer.com/10.1007/s00170-015-8013-2},
  author = {Zhang, Yun and Mao, Ting and Huang, Zhigao and Gao, Huang and Li, Dequn},
  month = aug,
  year = {2016},
  keywords = {Design of experiment,Principal component analysis,PCA,SPC,Statistical Process Control,MPCA},
  pages = {2483-2494}
}

@article{ma_design_1974,
  title = {A Design Approach to a Computer-Controlled Injection-Molding Machine},
  volume = {14},
  issn = {1548-2634},
  abstract = {Plastics injection molding is a complex, multi-variable process which has not yet been clarified. The hardware controller is difficult to build without a realistic mathematical model. One powerful technique for solving such a difficulty is to employ digital control technology which can utilize sophisticated control strategies in order to optimize the system performance. Machine operation and resultant production can be tightly controlled by using computer monitoring, data storage and data logging. This paper presents elements of the various problems that process engineers may encounter in designing a computer-controlled system for plastics injection-molding machinery. The variables of the process and a functional analysis of process control are discussed. A hardware system design; including specification of computer, selection of peripheral devices, system design, and input/output interfacing; is presented.},
  language = {en},
  number = {11},
  urldate = {2016-10-19},
  journal = {Polymer Engineering \& Science},
  doi = {10.1002/pen.760141106},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/pen.760141106/abstract},
  author = {Ma, Carl Y. W.},
  month = nov,
  year = {1974},
  pages = {768-772}
}

@incollection{nwokah_control_2001,
  title = {Control of {{Polymer Processing}}},
  volume = {24},
  isbn = {978-1-4200-3674-9},
  abstract = {9 Control of Polymer Processing
9.1 Introduction
9.2 Process Description
9.3 Process Variability
9.4 Modeling
9.5 Process Control
9.6 Conclusions
References},
  language = {en},
  urldate = {2016-09-28},
  booktitle = {The {{Mechanical Systems Design Handbook}}},
  publisher = {{CRC Press}},
  url = {http://www.crcnetbase.com/doi/abs/10.1201/9781420036749.ch9},
  author = {Kazmer, David Owen and Danai, Kourosh},
  editor = {Nwokah, Osita and Hurmuzlu, Yildirim},
  month = dec,
  year = {2001},
  keywords = {closed loop,neural network,Parameter optimization,machine control,set-point regulation,state-variable control,process variability,VSM,Parameter-based control},
}

@phdthesis{giroud_mesure_2001,
  title = {{Mesure et calcul des contraintes r{\'e}siduelles dans les pi{\`e}ces inject{\'e}es en thermoplastiques avec et sans fibres de renfort}},
  abstract = {Dans un premier temps, les ph{\'e}nom{\`e}nes {\`a} l'origine des contraintes r{\'e}siduelles en injection de thermoplastique sont d{\'e}crits, avec les sp{\'e}cificit{\'e}s li{\'e}es aux polym{\`e}res renforc{\'e}s de fibres. Le refroidissement h{\'e}t{\'e}rog{\`e}ne associ{\'e} {\`a} une variation de comportement m{\'e}canique conduit aux contraintes d'origine thermique, auxquelles il faut ajouter les contraintes dues {\`a} la pression dans la phase fluide fig{\'e}e par la solidification. Les fibres de renfort introduisent une h{\'e}t{\'e}rog{\'e}n{\'e}it{\'e} et une anisotropie de comportement m{\'e}canique et de retrait. Dans un deuxi{\`e}me temps, une m{\'e}thode de mesure des contraintes r{\'e}siduelles bas{\'e}e sur l'enl{\`e}vement de couches et la mesure des moments de flexion est pr{\'e}sent{\'e}e. Elle permet la mesure des contraintes r{\'e}siduelles pour des mat{\'e}riaux anisotropes comme les thermoplastiques renforc{\'e}s de fibres. En g{\'e}n{\'e}ral, on obtient des contraintes de traction en c{\oe}ur et de compression en peau. L'effet du fraisage pour enlever les couches est {\'e}valu{\'e}. La pression et le temps de maintien, la temp{\'e}rature de r{\'e}gulation du moule, le taux de fibres influencent les profils de contraintes.},
  language = {fr},
  urldate = {2016-11-01},
  school = {{\'E}cole Nationale Sup{\'e}rieure des Mines de Paris},
  url = {https://pastel.archives-ouvertes.fr/tel-00392610/document},
  author = {Giroud, Thomas},
  month = dec,
  year = {2001},
  keywords = {physical model,Shrinkage,Shrinkage measurements,Constraint Propagation}
}

@article{del_castillo_statistical_2006,
  title = {Statistical Process Adjustment: A Brief Retrospective, Current Status, and Some Opportunities for Further Work},
  volume = {60},
  issn = {1467-9574},
  shorttitle = {Statistical Process Adjustment},
  abstract = {Industrial statisticians frequently face problems in their practice where adjustment of a manufacturing process is necessary. In this paper, a view of the origins and recent work in the area of statistical process adjustment (SPA) is provided. A discussion of some topics open for further research is also given including new problems in semiconductor manufacturing process control. The goal of the paper is to help display the SPA field as a research area with its own identity and content, and promote further interest in its development and application in the industry.},
  language = {en},
  number = {3},
  urldate = {2016-10-27},
  journal = {Statistica Neerlandica},
  doi = {10.1111/j.1467-9574.2006.00328.x},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9574.2006.00328.x/abstract},
  author = {Del Castillo, E.},
  month = aug,
  year = {2006},
  keywords = {engineering process control,time-series control,EWMA controllers,Bayesian control,deadband adjustment,set-up adjustment},
  pages = {309-326}
}

@inproceedings{haeussler_quality_1993,
  address = {{NEW ORLEANS}},
  title = {Quality {{Assurance}} in {{Injection Molding}} with {{Neural Networks}}},
  volume = {1},
  abstract = {University of Paderborn. Then Haeussler and Wortberg (16) constructed and trained an ANN to predict part weight and lenght from pattern discrete process measurements with non linear model. = improved part quality but not implemented closed-loop on part weight on length.},
  language = {English},
  booktitle = {Conference {{Proceedings}}},
  publisher = {{Society of Plastics Engineers}},
  author = {Haeussler, J. and Wortberg, J.},
  month = may,
  year = {1993},
  keywords = {neural network,Pattern recognition,parts weight prediction},
  pages = {123-129}
}

@article{woll_online_1996,
  title = {Online Pattern-Based Part Quality Monitoring of the Injection Molding Process},
  volume = {36},
  issn = {1548-2634},
  abstract = {The quality of injection molded parts is currently monitored in the plant using techniques that focus on the statistical analysis of discrete data and, in particular, peak values. This paper presents an alternative online technique for part quality monitoring that focuses on the analysis of complete data patterns. Specifically, this paper discusses the application of artificial neural networks (ANNs) as part quality monitoring tools. The method of approach is to train a back propagation network (BPN) to associate part quality with the corresponding data pattern produced during injection. In Part I of this work, the data pattern consists of a series of discrete values and the part quality measure is defined as part weight. In Part II, the data pattern is the measurement profile observed from a pressure sensor placed in the mold cavity and the part quality measure is defined as part length. Results show that ANNs are successful in predicting part quality based on data patterns when an entire sensor profile is analyzed. Furthermore, demonstrations show that the approach is superior in predicting part quality when compared to statistical techniques now widely practiced by the injection molding process industry.},
  language = {en},
  number = {11},
  urldate = {2016-09-27},
  journal = {Polymer Engineering \& Science},
  doi = {10.1002/pen.10542},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/pen.10542/abstract},
  author = {Woll, Suzanne L. B. and Cooper, Douglas J. and Souder, Blair V.},
  month = may,
  year = {1996},
  keywords = {Cavity pressure,neural network,Pattern recognition,parts weight prediction,Artificial neural network,back-propagation,box-behnken,Dimensional metrology,SPC},
  pages = {1477-1488}
}

@phdthesis{yang_injection_2004,
  type = {{{PhD Thesis}}},
  title = {Injection Molding Control: From Process to Quality},
  lccn = {Thesis CENG 2004 Yang},
  shorttitle = {Injection Molding Control},
  abstract = {Thesis (Ph.D.)--Hong Kong University of Science and Technology, 2004, Also available in electronic version, Additional copy available in the Dept. of Chemical Engineering},
  language = {eng},
  school = {Hong Kong University of Science and Technology},
  author = {Yang, Yi},
  month = jan,
  year = {2004},
  keywords = {cycle-to-cycle,Feedforward and Feedback Control,fuzzy controller,fuzzy logic,INJECTION molding of plastics,learning control,neural network,part weight prediction,PLS,Predictive control,predictive controller,Quality control}
}

@article{fournier_conduite_2006,
  title = {Conduite Adaptative Du Proc{\'e}d{\'e} d'injection Des Thermoplastiques},
  volume = {base documentaire : TIB151DUO.},
  abstract = {Un proc{\'e}d{\'e} d'injection thermoplastique industriel doit permettre d'{\'e}laborer des pi{\`e}ces plastiques complexes pr{\'e}sentant des performances m{\'e}caniques et dimensionnelles, mais sous une reproductibilit{\'e} optimale et {\`a} un co{\^u}t raisonnable. Or, les syst{\`e}mes classiques de commande des presses {\`a} injecter ne permettent pas de g{\'e}rer ces deux types de contraintes. Cet article pr{\'e}sente un concept novateur de pilotage qui r{\'e}duit la dispersion de la masse des pi{\`e}ces produites. Deux syst{\`e}mes sont compar{\'e}s pour diverses configurations mat{\'e}riau/moule/machine {\`a} celles d'une conduite de presse classique r{\'e}gulant, dans la phase de maintien, la pression hydraulique.},
  number = {ref. article : in53},
  journal = {Techniques de l'ing{\'e}nieur Proc{\'e}d{\'e}s d'injection des thermoplastiques},
  url = {http://www.techniques-ingenieur.fr/base-documentaire/materiaux-th11/procedes-d-injection-des-thermoplastiques-42151210/conduite-adaptative-du-procede-d-injection-des-thermoplastiques-in53/},
  author = {Fournier, Jean-{\'E}tienne and Havard, Nicolas and Lacrampe, Marie-France and Ryckebusch, Marc and Krawczak, Patricia},
  month = apr,
  year = {2006},
  keywords = {closed loop,Adaptive control,weight control}
}

@article{ivester_automatic_1998,
  title = {Automatic Tuning of Injection Molding by the {{Virtual Search Method}}},
  volume = {2},
  issn = {1533-905X},
  language = {eng},
  number = {3},
  urldate = {2016-09-27},
  journal = {The Journal of injection molding technology},
  url = {http://cat.inist.fr/?aModele=afficheN\&cpsidt=1811595},
  author = {Ivester, R. and Danai, K. and Kazmer, David Owen},
  year = {1998},
  keywords = {simulation,Dimensional metrology,cycle time,Virtual Search Method,simplex},
  pages = {103-108}
}

@article{yang_knowledgebased_2000,
  title = {A {{Knowledge}}-{{Based Tuning Method}} for {{Injection Molding Machines}}},
  volume = {123},
  issn = {1087-1357},
  abstract = {Complexity of manufacturing processes has hindered methodical specification of machine setpoints for improving productivity. Traditionally in injection molding, the machine setpoints are assigned either by trial and error, based on heuristic knowledge of an experienced operator, or according to an empirical model between the inputs and part quality attributes, which is obtained from statistical design of experiments (DOE). In this paper, a Knowledge-Based Tuning (KBT) Method is presented which takes advantage of the a priori knowledge of the process, in the form of a qualitative model, to reduce the demand for experimentation. The KBT Method provides an estimate of the process feasible region (process window) as the basis of finding the suitable setpoints, and updates its knowledge-base using the data that become available during tuning. As such, the KBT Method has several advantages over conventional tuning methods: (1) the qualitative model provides a generic form of representation for linear and nonlinear processes alike, therefore, there is no need for selecting the form of the empirical model through trial and error, (2) the use of a priori knowledge eliminates the need for initial trials to construct an empirical model, so an initial feasible region can be identified as the basis of search for the suitable setpoints, and (3) the search within the feasible region leads to a higher fidelity model of this region when the input/output data from consecutive process iterations are used for learning. The KBT Method's utility is demonstrated in production of digital video disks (DVDs).},
  number = {4},
  urldate = {2016-09-27},
  journal = {Journal of Manufacturing Science and Engineering},
  doi = {10.1115/1.1382596},
  url = {http://dx.doi.org/10.1115/1.1382596},
  author = {Yang, Dongzhe and Danai, Kourosh and Kazmer, David Owen},
  month = dec,
  year = {2000},
  keywords = {Expert System,knowledge-based systems,tuning,learning systems,qualitative model},
  pages = {682-691}
}

@article{lau_neural_2001,
  title = {Neural Networks for the Dimensional Control of Molded Parts Based on a Reverse Process Model},
  volume = {117},
  issn = {0924-0136},
  abstract = {This paper presents the application of neural networks in suggesting the change of molding parameters for improving the dimensional quality of molded parts based on the concept of reverse process modeling. Instead of using the molding condition parameters as input values and dimensional outcomes as output values, the reverse process model configures the dimensional outcomes as inputs and the molding condition parameters as outputs. With the mapping on input and output layers of neural networks based on this configuration, the trained neural networks learn the correlation between the dimensional outcome values and the corresponding molding parameters. This model, which serves to learn from sample data and induce the values for change of the operating molding conditions, has been implemented for the dimensional improvement of injection molding parts, the dimensions of which are primarily determined by the process parameters such as injection time and cooling temperature.},
  number = {1\textendash{}2},
  urldate = {2016-10-28},
  journal = {Journal of Materials Processing Technology},
  doi = {10.1016/S0924-0136(01)01086-X},
  url = {http://www.sciencedirect.com/science/article/pii/S092401360101086X},
  author = {Lau, H. C. W. and Ning, A. and Pun, K. F. and Chin, K. S.},
  month = nov,
  year = {2001},
  keywords = {Neural Networks,back-propagation,Dimensional metrology,Reverse process model,Plastic molding,Molding conditions,cooling time,MIMO,Statistical Process Adjustment,gradient descent,feed,Parameter-based control},
  pages = {89-96}
}

@article{bruno_albert_formalisation_2016,
  title = {Formalisation Du {{Contr{\^o}le Qualit{\'e} Haptique}} : {{Structuration S{\'e}mantique}} Des {{Sensations Haptiques}}},
  shorttitle = {Formalisation Du {{Contr{\^o}le Qualit{\'e} Haptique}}},
  abstract = {La qualit{\'e} per{\c c}ue a une place primordiale dans le processus de s{\'e}lection d'un produit par des consommateurs. Diff{\'e}rents sens sont impliqu{\'e}s et le toucher en fait partie. A l'inverse de la vision, peu d'{\'e}tudes ont propos{\'e} des m{\'e}thodes permettant le contr{\^o}le qualit{\'e} haptique, impliquant le tactile et la kinesth{\'e}sie. De plus, celles-ci sont g{\'e}n{\'e}ralement sp{\'e}cifiques {\`a} certains produits ou mat{\'e}riaux.
La premi{\`e}re {\'e}tape de cette {\'e}tude propose une description g{\'e}n{\'e}rique et structur{\'e}e des sensations tactiles, qui servira de base pour le d{\'e}veloppement d'une m{\'e}thodologie de contr{\^o}le qualit{\'e} haptique, ainsi que d'un futur syst{\`e}me automatis{\'e}.},
  urldate = {2016-09-28},
  doi = {10.13140/RG.2.2.15316.60806},
  url = {https://doi.org/10.13140/RG.2.2.15316.60806},
  author = {{Albert, Bruno} and {Knecht, Christophe} and {de Bertrand de Beuvron, Fran{\c c}ois} and {Charrier Julien} and {Pillet Maurice} and {Maire Jean-Luc} and {Zanni-Merk Cecilia}},
  year = {2016}
}

@inproceedings{albert_generic_2016,
  title = {Generic and Structured Description of Tactile Sensory Perceptions},
  booktitle = {6th {{Kansei Engineering}} and {{Emotion Research}}},
  url = {http://icube-publis.unistra.fr/4-AMPZ16},
  author = {Albert, Bruno and Maire, Jean-Luc. and Pillet, Maurice and {Zanni-Merk}, Cecilia and {de Bertrand de Beuvron}, Fran{\c c}ois and Knecht, Christophe and Charrier, J.},
  month = aug,
  year = {2016},
  keywords = {perceived quality,tactile control sensory measurement,tactile descriptors,sensations},
  x-international-audience = {Yes},
  x-language = {EN}
}

@incollection{albert_smart_2019,
  title = {A {{Smart System}} for {{Haptic Quality Control}}: {{A Knowledge}}-{{Based Approach}} to {{Formalize}} the {{Sense}} of {{Touch In}}},
  shorttitle = {A {{Smart System}} for {{Haptic Quality Control}}},
  urldate = {2019-04-24},
  booktitle = {Knowledge {{Discovery}}, {{Knowledge Engineering}} and {{Knowledge Management}}. {{IC3K}} 2016.{{Communications}} in {{Computer}} and {{Information Science}}, Vol 914. {{Springer}}, {{Cham}}},
  publisher = {{Fred A., Dietz J., Aveiro D., Liu K., Bernardino J., Filipe J. (eds)}},
  url = {https://hal.archives-ouvertes.fr/hal-01952452},
  hal = {hal-01952452},
  author = {Albert, Bruno and de Bertrand de Beuvron, Fran{\c c}ois and {Zanni-Merk}, Cecilia and Maire, Jean-Luc and Pillet, Maurice and CHARRIER, Julien and KNECHT, Christophe},
  year = {2019},
  pages = {173-190},
  doi = {10.1007/978-3-319-99701-8_8}
}

@phdthesis{albert_maitrise_2019,
  title = {{Ma{\^i}trise de la qualit{\'e} haptique des produits}},
  abstract = {Les travaux r{\'e}alis{\'e}s au cours de cette th{\`e}se ont men{\'e} {\`a} la proposition d'une solution m{\'e}thodologique et logicielle accompagnant les industriels vers une meilleure ma{\^i}trise du contr{\^o}le sensoriel haptique de leurs produits. Ce type de contr{\^o}le, qui fait appel au sens du toucher de mani{\`e}re dynamique, pose en effet de nombreux probl{\`e}mes dans l'industrie. Un cas d'{\'e}tude, r{\'e}alis{\'e} sur des bracelets d'une montre de luxe, illustre la difficult{\'e} {\`a} juger de mani{\`e}re objective et r{\'e}p{\'e}table la qualit{\'e} haptique des produits. Notre analyse de la litt{\'e}rature confirme {\'e}galement que la mise en place de ce contr{\^o}le requiert une grande expertise {\`a} acqu{\'e}rir et {\`a} capitaliser. En particulier, la mani{\`e}re de d{\'e}crire une sensation haptique et la mani{\`e}re de contr{\^o}ler le produit ont g{\'e}n{\'e}ralement un impact significatif sur le r{\'e}sultat du contr{\^o}le. Dans le but de r{\'e}duire ces difficult{\'e}s, trois contributions principales sont apport{\'e}es. Une m{\'e}thode est tout d'abord propos{\'e}e pour formaliser les sensations haptiques {\'e}l{\'e}mentaires {\`a} l'origine de la perception sensorielle. Il s'agit d'un processus de mod{\'e}lisation s{\'e}mantique qui m{\`e}ne {\`a} l'identification d'un nombre r{\'e}duit de sensations haptiques {\'e}l{\'e}mentaires et {\`a} leur mise en relation avec les mots utilis{\'e}s dans ce domaine. Une repr{\'e}sentation ontologique est ensuite construite, sur la base de mod{\`e}les existants, pour structurer de mani{\`e}re robuste l'ensemble des connaissances n{\'e}cessaires au contr{\^o}le haptique (descripteurs, sensations, anomalies, stimuli, r{\'e}cepteurs sensoriels, effecteurs, modes d'exploration, etc.). Cette mod{\'e}lisation rend possible un acc{\`e}s contextualis{\'e} {\`a} ces connaissances et leur enrichissement au fil du temps. Enfin, nous proposons une m{\'e}thodologie aidant {\`a} la mise en {\oe}uvre syst{\'e}matique d'un contr{\^o}le haptique des produits. Elle s'appuie sur les connaissances mod{\'e}lis{\'e}es pour g{\'e}n{\'e}rer un standard de contr{\^o}le adapt{\'e} {\`a} la probl{\'e}matique sp{\'e}cifique rencontr{\'e}e. Ce standard inclut les sp{\'e}cifications formalis{\'e}es des anomalies haptiques {\`a} contr{\^o}ler, des protocoles ainsi qu'un r{\'e}f{\'e}rentiel de contr{\^o}le {\`a} utiliser. L'outil logiciel INSENSO, d{\'e}velopp{\'e} au sein de la soci{\'e}t{\'e} INEVA, int{\`e}gre cette m{\'e}thodologie de mise en {\oe}uvre du contr{\^o}le ainsi que le syst{\`e}me de gestion des connaissances associ{\'e}es. L'application de cette m{\'e}thodologie sur le cas des bracelets de montre a men{\'e} au d{\'e}veloppement dans l'entreprise manufacturi{\`e}re d'un standard de contr{\^o}le haptique. L'utilisation de ce standard a permis de r{\'e}duire de mani{\`e}re significative la variabilit{\'e} des r{\'e}sultats de contr{\^o}le. Les solutions d{\'e}velopp{\'e}es sur le contr{\^o}le haptique ouvrent de nombreuses perspectives dans le domaine plus g{\'e}n{\'e}ral du contr{\^o}le sensoriel, notamment en ce qui concerne son instrumentation.},
  language = {fr},
  urldate = {2019-07-03},
  school = {Universit{\'e} Grenoble Alpes},
  url = {https://tel.archives-ouvertes.fr/tel-02137398},
  HAL = {tel-02137398},
  author = {Albert, Bruno},
  month = mar,
  year = {2019}
}

@inproceedings{desage_syntactic_2015,
  address = {{Le Creusot, France}},
  title = {Syntactic Texture and Perception for a New Generic Visual Anomalies Classifcation.},
  volume = {9534},
  abstract = {The research purpose is to improve aesthetic anomalies detection and evaluation based on what is perceived by
human eye and on the 2006 CIE report. It is therefore important to define parameters able to discriminate
surfaces, in accordance with the perception of human eye. Our starting point in assessing aesthetic anomalies
is geometric description such as defined by ISO standard,2 i.e. traduce anomalies description with perception
words about texture divergence impact. However, human controllers observe (detect) the aesthetic anomaly
by its visual effect and interpreter for its geometric description. The research question is how define generic
parameters for discriminating aesthetic anomalies, from enhanced information of visual texture such as recent
surface visual rendering approach. We propose to use an approach from visual texture processing that quantify
spatial variations of pixel for translating changes in color, material and relief. From a set of images from different
angles of light which gives us access to the surface appearance, we propose an approach from visual effect to
geometrical specifications as the current standards have identifed the aesthetic anomalies.},
  urldate = {2016-09-09},
  booktitle = {The {{International Conference}} on {{Quality Control}} by {{Artificial Vision}} 2015},
  url = {https://hal.archives-ouvertes.fr/hal-01246517},
  hal = {hal-01246517},
  author = {Desage, Simon-Fr{\'e}d{\'e}ric and Pitard, Gilles and Pillet, Maurice and Favreli{\`e}re, Hugues and Maire, Jean Luc and Frelin, Fabrice and Samper, Serge and Le Go{\"i}c, Ga{\"e}tan},
  month = jun,
  year = {2015},
  keywords = {classification,geometrical description,Surface imperfection,visual effect,visual inspection,visual texture},
}

@phdthesis{desage_constraints_2015,
  type = {PhD Thesis},
  title = {Constraints and Opportunities for Automation of Visual Inspection with Regard to the Human Process},
  urldate = {2016-10-13},
  school = {Universit{\'e} Grenoble Alpes},
  url = {https://tel.archives-ouvertes.fr/tel-01254349},
  HAL = {tel-01254349},
  author = {Desage, Simon-Frédéric},
  month = nov,
  year = {2015},
  keywords = {Apprentissage machine,Aspects de surface,Automation,Automatisation,Computer vision,Image processing,Inspection visuelle,Machine learning,Pattern recognition,Reconnaissance de formes,Surface appearance,Texture visuelle,Traitement d'images,Vision par ordinateur,visual inspection,visual texture}
}

@phdthesis{lacombe_exploitation_2018a,
  address = {{Annecy, France}},
  type = {{PhD Thesis}},
  title = {{Exploitation d'une information multi{\'e}clairages pour une approche g{\'e}n{\'e}rique de l'inspection automatique de la qualit{\'e} visuelle des produits en industrie.}},
  abstract = {La maitrise de la qualit{\'e} visuelle des produits constitue un enjeu toujours plus important pour les entreprises {\`a} l'heure actuelle, dans de tr{\`e}s nombreux secteurs d'application. Afin de garantir cette ma{\^i}trise, les industriels se dotent de m{\'e}thodes et processus ax{\'e}s sur le contr{\^o}le visuel des produits. N{\'e}anmoins, le contr{\^o}le visuel humain en industrie est confront{\'e} {\`a} des probl{\`e}mes de variabilit{\'e} des r{\'e}sultats, dus en partie {\`a} l'aspect subjectif de ce type d'inspection. A partir de ce constat, diff{\'e}rentes strat{\'e}gies ont {\'e}t{\'e} d{\'e}velopp{\'e}es pour r{\'e}duire cette variabilit{\'e}. La premi{\`e}re consiste en la mod{\'e}lisation et la formalisation du contr{\^o}le humain pour aider les op{\'e}rateurs {\`a} r{\'e}aliser le contr{\^o}le de mani{\`e}re plus r{\'e}p{\'e}table. La seconde r{\'e}side dans le d{\'e}veloppement de syst{\`e}mes de contr{\^o}le automatis{\'e}s, tr{\`e}s r{\'e}p{\'e}tables par nature, mais cependant encore {\`a} l'heure actuelle peu flexibles et adaptables en terme d'applications. Le d{\'e}veloppement de syst{\`e}mes plus g{\'e}n{\'e}riques en terme de vari{\'e}t{\'e} de d{\'e}fauts {\`a} d{\'e}tecter et de types de pi{\`e}ces {\`a} observer constitue donc un challenge pour le domaine de l'inspection automatis{\'e}e. Ce travail de th{\`e}se porte sur le rapprochement de ces deux approches. En s'inspirant des pratiques du contr{\^o}le visuel humain pour d{\'e}velopper un syst{\`e}me de contr{\^o}le automatis{\'e}, et plus particuli{\`e}rement de l'exploitation de divers angles d'{\'e}clairages pour observer les produits, nous souhaitons allier la forte r{\'e}p{\'e}tabilit{\'e} des syst{\`e}mes automatiques avec la flexibilit{\'e} des pratiques humaines. Dans cet objectif, une {\'e}tude d{\'e}taill{\'e}e est propos{\'e}e, partant de la d{\'e}finition du cahier de charges d'un syst{\`e}me de contr{\^o}le automatis{\'e} inspir{\'e} du contr{\^o}le humain dans un contexte industriel, puis s'int{\'e}ressant ensuite {\`a} ses diff{\'e}rentes composantes : l'acquisition de l'information multi-{\'e}clairages, l'extraction d'attributs des images et leur r{\'e}duction, et enfin la classification menant {\`a} une d{\'e}cision quant {\`a} la qualit{\'e} du produit inspect{\'e}. Pour chaque {\'e}tape une {\'e}tude bibliographique des m{\'e}thodes existantes est men{\'e}e et mise en regard des contraintes de l'application industrielle. De plus, des contributions m{\'e}thodologiques innovantes pour l'extraction et la r{\'e}duction d'attributs sont {\'e}galement propos{\'e}es. La premi{\`e}re consiste en l'utilisation de param{\`e}tres issus de la D{\'e}composition Modale Discr{\`e}te des images comme descripteurs pertinents dans un objectif de classification. La seconde r{\'e}side dans la proposition d'une nouvelle m{\'e}thode de s{\'e}lection d'attributs mettant en oeuvre un crit{\`e}re d'{\'e}valuation fond{\'e} sur la statistique du T{$^2$} et les cartes de contr{\^o}le. L'union des investigations bibliographiques et des contributions m{\'e}thodologiques issues de ces travaux de th{\`e}se m{\`e}ne enfin {\`a} la proposition d'une approche innovante exploitant une information multi-{\'e}clairages pour le contr{\^o}le visuel automatis{\'e}. La mise en oeuvre de cette approche sur des {\'e}chantillons issus de l'industrie plastique permet de mettre en avant l'int{\'e}r{\^e}t de cette derni{\`e}re pour la d{\'e}tection de d{\'e}faut d'aspect par rapport {\`a} une m{\'e}thode plus classique n'exploitant qu'un seul angle d'{\'e}clairage.},
  language = {fr},
  urldate = {2019-07-03},
  school = {UGA - Universit{\'e} Grenoble Alpes},
  url = {https://hal.archives-ouvertes.fr/tel-01959270},
  hal = {tel-01959270},
  author = {Lacombe, Thomas},
  month = sep,
  year = {2018}
}

@article{schwenke_optical_2002,
  title = {Optical {{Methods}} for {{Dimensional Metrology}} in {{Production Engineering}}},
  volume = {51},
  issn = {0007-8506},
  abstract = {Metrology in production engineering must be fast, accurate, robust and automated, and ideally integrated into the production line. In many respects, optical methods seem to fulfil these requirements. Although optical methods have a long tradition in dimensional metrology, the rapid progress in the development of optoelectronic components and availability of increased computational power makes many new technical approaches possible. This paper provides a technical overview of the optical methods available for dimensional metrology. Methods for the measurement of length, angle, surface form and spatial co-ordinates are described. The paper summarises both the metrological characteristics and the technical limitations of the methods. Furthermore, it presents some new and promising approaches that, in the future, may play an important role in dimensional metrology for production.},
  number = {2},
  urldate = {2016-10-19},
  journal = {CIRP Annals - Manufacturing Technology},
  doi = {10.1016/S0007-8506(07)61707-7},
  url = {http://www.sciencedirect.com/science/article/pii/S0007850607617077},
  author = {Schwenke, Heinrich and {Neuschaefer-Rube}, Ulrich and Pfeifer, Tilo and Kunzmann, Horst},
  month = jan,
  year = {2002},
  keywords = {Optical Methods,Production Engineering,Dimensional metrology,optical metrology},
  pages = {685-699}
}

@article{gao_multivariate_2012,
  title = {Multivariate Sensing and Wireless Data Communication for Process Monitoring in {{RF}}-Shielded Environment},
  volume = {61},
  issn = {0007-8506},
  abstract = {Online process metrology is critical to ensuring manufacturing quality and productivity. This paper presents the design and modelling of a multivariate sensor that enables the simultaneous measurement of multiple parameters from within an RF shielded environment, e.g. an injection mold. A coded wave modulation scheme is developed for wirelessly transmitting the parameters through the mold. The design of the modulator is optimized through a coupled field analysis for noise reduction. The effectiveness of the sensing method is demonstrated in the online measurement of melt pressure, temperature, viscosity, and velocity. This sensing method is applicable to various process monitoring scenarios.},
  number = {1},
  urldate = {2016-09-19},
  journal = {CIRP Annals - Manufacturing Technology},
  doi = {10.1016/j.cirp.2012.03.014},
  url = {http://www.sciencedirect.com/science/article/pii/S0007850612000169},
  author = {Gao, Robert X. and Kazmer, David Owen},
  year = {2012},
  keywords = {Metrology,Sensor,Quality control,wireless sensor networks,wireless transmission},
  pages = {523-526}
}

@article{kazmer_feasibility_2011,
  title = {Feasibility {{Analysis}} of an {{In}}-Mold {{Multivariate Sensor}}},
  volume = {26},
  issn = {0930-777X},
  abstract = {The initial design of a novel multivariate sensor is described for the measurement of melt temperature, melt pressure, melt velocity, melt viscosity, and mold temperature. Melt pressure and temperature are respectively obtained through the incorporation of a piezoceramic element and infrared photodetector within the sensor head. Melt velocity is derived from the initial response of the melt temperature as the polymer melt flows across the sensor's lens. The apparent melt viscosity is then derived from the melt velocity and the time derivative of the increasing melt pressure given the cavity thickness. The feasibility of the envisioned sensor is then analyzed using a production-grade mold that is instrumented with commercial piezoelectric pressure sensors, infrared pyrometer, and thermocouples. Several predictive models of part weight are developed using multiple regression of data obtained from a design of experiments to evaluate the capability of the envisioned multivariate sensor. The results indicate a correlation coefficient, R2, of 0.79 for a model based on the machine settings, 0.80 for a model based on a cavity pressure sensor, 0.90 for a model based on the multivariate sensor, and 0.98 for a non-linear model based on the multivariate sensor. The implication is that multiple orthogonal sensing streams with high fidelity models are necessary to provide automatic quality assurance sufficient for fully automated plastics manufacturing.},
  number = {1},
  urldate = {2016-10-19},
  journal = {International Polymer Processing},
  doi = {10.3139/217.2397},
  url = {http://www.hanser-elibrary.com/doi/abs/10.3139/217.2397},
  author = {Kazmer, David Owen and Johnston, S. P. and Gao, R. X. and Fan, Z.},
  month = mar,
  year = {2011},
  keywords = {Online measurement,in-mold,multivariate sensors,in-mold sensor,feasibility},
  pages = {63-72}
}

@article{kurt_experimental_2009,
  title = {Experimental Investigation of Plastic Injection Molding: {{Assessment}} of the Effects of Cavity Pressure and Mold Temperature on the Quality of the Final Products},
  volume = {30},
  issn = {0261-3069},
  shorttitle = {Experimental Investigation of Plastic Injection Molding},
  abstract = {Today more than ever, the molding industry demands better product quality. The quality of molded parts is crucial with regard to their functional capacity, and hence a great deal of attention should be directed towards maintaining consistent tolerances and overall dimensions. As the demand for high quality final parts continues to increase, the controlling parameters of cavity pressure and mold temperature become ever more significant in the plastic injection molding process. Therefore, in this research, cavity pressure and mold surface temperature have been measured and recorded by pressure and temperature\textendash{}pressure sensors using a Kistler CoMo 2869A injection-type apparatus. The influences of the measured factors on the quality of the final parts have been investigated experimentally. The results of this experimental study indicate that cavity pressure and mold temperature are the dominant factors determining the quality of the final product in plastic injection molding.},
  number = {8},
  urldate = {2016-10-26},
  journal = {Materials \& Design},
  doi = {10.1016/j.matdes.2009.01.004},
  url = {http://www.sciencedirect.com/science/article/pii/S0261306909000089},
  author = {Kurt, Mustafa and Saban Kamber, O. and Kaynak, Yusuf and Atakok, Gurcan and Girit, Oguz},
  month = sep,
  year = {2009},
  keywords = {Cavity pressure,temperature measurement,pressure measurement,quality model,SPC,mold temperature},
  pages = {3217-3224}
}

@techreport{ISO_9000_2015,
  address = {{Geneva, Switzerland}},
  type = {Standard},
  title = {{{ISO}} 9000:2015: {{Quality}} Management Systems -- {{Fundamentals}} and Vocabulary},
  shorttitle = {{{ISO}} 9000:2015},
  abstract = {ISO 9000:2015 describes the fundamental concepts and principles of quality management which are universally applicable to the following:

organizations seeking sustained success through the implementation of a quality management system;
customers seeking confidence in an organization's ability to consistently provide products and services conforming to their requirements;
organizations seeking confidence in their supply chain that their product and service requirements will be met;
organizations and interested parties seeking to improve communication through a common understanding of the vocabulary used in quality management;
organizations performing conformity assessments against the requirements of ISO 9001;
providers of training, assessment or advice in quality management;
developers of related standards.
ISO 9000:2015 specifies the terms and definitions that apply to all quality management and quality management system standards developed by ISO/TC 176.},
  language = {English},
  number = {ISO 9000:2015},
  urldate = {2019-09-11},
  institution = {{International Organization for Standardization / International Organization for Standardization}},
  url = {https://www.iso.org/standard/45481.html},
  author = {{{ISO/TC} 176 }},
  month = sep,
  year = {2015},
  pages = {51},
  price = {CHF 178},
}

@techreport{ISO_8015_2011,
  address = {{Geneva, Switzerland}},
  type = {Standard},
  title = {{{ISO}} 8015:2011: {{Geometrical}} Product Specifications ({{GPS}}) -- {{Fundamentals}} -- {{Concepts}}, Principles and Rules},
  shorttitle = {{{ISO}} 9000:2015},
  abstract = {ISO 8015:2011 specifies fundamental concepts, principles and rules valid for the creation, interpretation and application of all other International Standards, Technical Specifications and Technical Reports concerning geometrical product specifications (GPS) and verification.

It applies to the interpretation of GPS indications on all types of drawings. For the purposes of ISO 8015:2011, the term "drawing" is to be interpreted in the broadest possible sense, encompassing the total package of documentation specifying the workpiece.},
  language = {English},
  number = {ISO 9000:2015},
  urldate = {2019-09-11},
  institution = {{International Organization for Standardization / International Organization for Standardization}},
  url = {https://www.iso.org/standard/55979.html},
  author = {{{ISO/TC} 213 }},
  month = jun,
  year = {2011},
  pages = {10},
  price = {CHF 58}
}

@book{deming_quality_1982,
  title = {Quality, Productivity, and Competitive Position},
  abstract = {Includes bibliographical references and index},
  language = {English},
  urldate = {2019-09-12},
  publisher = {{Cambridge, MA : Massachusetts Institute of Technology, Center for Advanced Engineering Study}},
  url = {http://archive.org/details/qualityproductiv00demi},
  author = {Deming, William Edwards},
  year = {1982},
  keywords = {Quality control},
  note = {04180}
}

@article{agazzi_optimal_2013,
  title = {Optimal Cooling Design in Injection Moulding Process \textendash{} {{A}} New Approach Based on~Morphological Surfaces},
  volume = {52},
  issn = {1359-4311},
  abstract = {The design of the cooling channels in the thermoplastic injection process is one of the most important steps during the mould design. An inappropriate cooling will lead to defects in the part and a low production rate. In this paper, a new approach with no a priori for the design of the cooling channels is presented for the determination of the cooling system of a 3D industrial part. Based on morphological concepts, the idea of regulation of the temperatures of the polymer and the mould by a cooling surface is introduced. The methodology is decomposed in two steps: the first step leads to the determination of the optimal fluid temperature distribution along the cooling surface in order to minimize an objective function composed of two terms linked to the quality of the part and the productivity of the process. The conjugate gradient algorithm coupled with a Lagrangian technique is implemented for the determination of the distribution of the fluid temperature. The second step illustrates the transition from cooling surface to discrete cooling channels, based on the analysis of the isothermal surfaces in the mould.},
  number = {1},
  urldate = {2016-09-28},
  journal = {Applied Thermal Engineering},
  doi = {10.1016/j.applthermaleng.2012.11.019},
  url = {http://www.sciencedirect.com/science/article/pii/S1359431112007326},
  author = {Agazzi, Alban and Sobotka, Vincent and LeGoff, Ronan and Jarny, Yvon},
  month = apr,
  year = {2013},
  keywords = {Conformal cooling,Inverse method,Transient heat transfer,Morphological analysis,Injection moulding process,Optimization,Cooling channels},
  pages = {170-178}
}

@article{ageyeva_inmold_2019,
  title = {In-{{Mold Sensors}} for {{Injection Molding}}: {{On}} the {{Way}} to {{Industry}} 4.0},
  volume = {19},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  shorttitle = {In-{{Mold Sensors}} for {{Injection Molding}}},
  abstract = {The recent trend in plastic production dictated by Industry 4.0 demands is to acquire a great deal of data for manufacturing process control. The most relevant data about the technological process itself come from the mold cavity where the plastic part is formed. Manufacturing process data in the mold cavity can be obtained with the help of sensors. Although many sensors are available nowadays, those appropriate for in-mold measurements have certain peculiarities. This study presents a comprehensive overview of in-mold process monitoring tools and methods for injection molding process control. It aims to survey the recent development of standard sensors used in the industry for the measurement of in-mold process parameters, as well as research attempts to develop unique solutions for solving certain research and industrial problems of injection molding process monitoring. This review covers the established process monitoring techniques\&mdash;direct temperature and pressure measurement with standard sensors and with the newly developed sensors, as well as techniques for the measurement of indirect process parameters, such as viscosity, warpage or shrinkage.},
  language = {en},
  number = {16},
  urldate = {2019-09-15},
  journal = {Sensors},
  doi = {10/gf78qn},
  url = {https://www.mdpi.com/1424-8220/19/16/3551},
  author = {Ageyeva, Tatyana and Horv{\'a}th, Szabolcs and Kov{\'a}cs, J{\'o}zsef G{\'a}bor},
  month = jan,
  year = {2019},
  keywords = {in-mold sensors,Industry 4.0,injection molding,process control},
  pages = {3551}
}

@phdthesis{malhab_moulage_2012,
  address = {{Paris, France}},
  type = {{PhD Thesis}},
  title = {{Moulage par microinjection des polym{\`e}res semi-cristallins}},
  abstract = {La miniaturisation des pi{\`e}ces est une {\'e}tape importante pour la progression de la microtechnologie dans plusieurs domaines (connectique, m{\'e}dical, optique, microsyst{\`e}mes m{\'e}caniques). Pour cela, le moulage par microinjection, semble {\^e}tre la solution cl{\'e} pour la production {\`a} grande {\'e}chelle de micro-composants de polym{\`e}res. Pour les polym{\`e}res semi-cristallins, la cristallisation, sous fort taux de cisaillement et sous des vitesses de refroidissement {\'e}lev{\'e}es (about 100 K/s), induit des morphologies et des propri{\'e}t{\'e}s sp{\'e}cifiques. Elle prend donc une importance consid{\'e}rable dans le processus de microinjection par rapport au moulage par injection classique o{\`u} les {\'e}paisseurs inject{\'e}es sont g{\'e}n{\'e}ralement sup{\'e}rieures {\`a} 1 mm. Ces microstructures ont une grande influence sur les propri{\'e}t{\'e}s m{\'e}caniques du produit final. La pr{\'e}diction de ces propri{\'e}t{\'e}s {\`a} partir de la description de la microstructure est un d{\'e}fi technique et scientifique. Durant cette th{\`e}se, deux polym{\`e}res semi-cristallins ont {\'e}t{\'e} microinject{\'e}s, le poly{\'e}thyl{\`e}ne haute densit{\'e} et le polyamide 12. Les analyses obtenues par la microscopie otiques montrent que les morphologies cristallines varient entre les micro- et les macro-pi{\`e}ces. Tandis que la morphologie de `peau-c{\oe}ur' est pr{\'e}sente dans les macropi{\`e}ces, les micropi{\`e}ces pr{\'e}sentent une morphologie plut{\^o}t particuli{\`e}re. Les analyses combin{\'e}es de diffusion et de diffraction des rayons X (SAXS et WAXS) avec un microfaisceau synchrotron, nous ont permis de d{\'e}terminer la microstructure induite par le processus de microinjection dans toute l'{\'e}paisseur des pi{\`e}ces. Nous avons constat{\'e} que la morphologie et les orientations cristallines induites sont tr{\`e}s d{\'e}pendantes des conditions d'injection ou de microinjection. Une diminution de l'{\'e}paisseur, de la vitesse et de la temp{\'e}rature du moule, augmente l'orientation cristalline en limitant la relaxation des cha{\^i}nes de polym{\`e}res.},
  language = {fr},
  urldate = {2019-09-21},
  school = {{\'E}cole Nationale Sup{\'e}rieure d'Arts et M{\'e}tiers},
  url = {https://pastel.archives-ouvertes.fr/pastel-00831028},
  author = {Malhab, Nada Bou},
  month = dec,
  year = {2012}
}

@article{regnier_local_1993,
  title = {Local Orthotropic Shrinkage Determination in Injected Moulded Polymer Plates},
  volume = {12},
  issn = {0142-9418},
  abstract = {An apparatus to precisely measure local orthotropic shrinkage in polymer plates has been designed. The principle of local shrinkage determination is based on the comparison of the distance between two small engravings on the plate mould and the distance between the reproduced engravings on the polymer plate. Under a microscope, the distances are measured in two directions on micrometric translation stages. The performance of the entire device, which includes on-line experimental data acquisition, is discussed; the precision of shrinkage measurement remains below 4\%. Shrinkage prediction models, which are based on statistical analysis and built with the first experimental results, have proved reliable and have been integrated as an experimental database in a general deformation prediction process of polymer parts.},
  number = {5},
  urldate = {2019-09-21},
  journal = {Polymer Testing},
  doi = {10/c4mr3b},
  url = {http://www.sciencedirect.com/science/article/pii/014294189390010M},
  author = {R{\'e}gnier, G. and Trotignon, J. P.},
  month = jan,
  year = {1993},
  pages = {383-392}
}

@incollection{regnier_experimental_1994,
  series = {{{NATO Science Series E}}},
  title = {Experimental {{Designs}} for an {{Experimental Modelling}} of {{Material}} or {{Structural Tests}}},
  volume = {269},
  abstract = {The experimental study of material or structural behaviours often consists of a multiparametric analysis. To describe the physical phenomenon in a limited region, it is valuable to identify the coefficients of a multilinear model with a regression. But the precision on these coefficients depends on the arrangement of the test matrix which represents the parameter values taken for each test. The optimal matrix form is reached by an orthogonal test matrix, but these orthogonal designs are restricting and sometimes impossible to realise because of physical constraints. Nevertheless, according to a criterion, optimal designs can be defined by respecting certain constraints: D-optimal designs are presented.},
  urldate = {2019-09-21},
  booktitle = {Probabilities and {{Materials}}: {{Tests}}, {{Models}} and {{Applications}}},
  publisher = {{Springer Netherlands}},
  url = {https://hal.archives-ouvertes.fr/hal-01634460},
  author = {R{\'e}gnier, Gilles and Soulier, Bruno},
  year = {1994},
  pages = {197-208}
}

@article{delaunay_nature_2000a,
  title = {Nature of Contact between Polymer and Mold in Injection Molding. {{Part I}}: {{Influence}} of a Non-Perfect Thermal Contact},
  volume = {40},
  copyright = {Copyright \textcopyright{} 2000 Society of Plastics Engineers},
  issn = {1548-2634},
  shorttitle = {Nature of Contact between Polymer and Mold in Injection Molding. {{Part I}}},
  abstract = {In injection molding, the pressure in the cavity usually reaches the atmospheric pressure before the ejection, therefore the thermal contact between polymer and mold is modified. This paper aims to evaluate the nature of the thermal contact between the polymer and the mold during the holding and cooling phase. An experimental plate mold has been designed to study this phenomenon. Thermal sensors facing each other and pressure sensors have been set in the mold. An inverse method is used to determine the heat flux density crossing the polymer mold interface, and the mold surface temperature. Then, a second inverse algorithm allows to determine the temperature profile at the end of the filling and the time evolution of the thermal contact resistance (TCR). Finally, the polymer temperature distribution in the thickness is determined between the thermal sensors. The results of this study show that the TCR between the polymer and the mold is not negligible and not constant with time. The polymer temperature at the surface can be 20\textdegree{}C higher than the mold surface temperature. Moreover, asymmetric air gaps have been observed when cavity pressure becomes equal to atmospheric pressure, therefore asymmetric temperature profile in the thickness are generated.},
  language = {en},
  number = {7},
  urldate = {2019-10-07},
  journal = {Polymer Engineering \& Science},
  doi = {10/dshfg7},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pen.11300},
  author = {Delaunay, D. and Bot, P. Le and Fulchiron, R. and Luye, J. F. and Regnier, G.},
  month = jul,
  year = {2000},
  pages = {1682-1691}
}

@article{delaunay_nature_2000,
  title = {Nature of Contact between Polymer and Mold in Injection Molding. {{Part II}}: {{Influence}} of Mold Deflection on Pressure History and Shrinkage},
  volume = {40},
  issn = {1548-2634},
  shorttitle = {Nature of Contact between Polymer and Mold in Injection Molding. {{Part II}}},
  abstract = {In injection molding of thermoplastic parts, high hold pressures are set during the packing phase to generate a post-filling, which compensates the shrinkage of polymer due to its cooling. The polymer pressure in mold cavity leads to a cavity deformation due to mold and machine compliance. Then, the increase in cavity thickness can modify the post-filling and consequently the pressure history, the volumetric shrinkage and the part mass. The first goal of this paper is to present a simple method to locally determine mold rigidities: over-packed slabs are injected and local deflections are determined from measurements of the local residual pressure, the local in-plane shrinkages and the plate thickness. In the studied plate mold, which can be considered as stiff compared to some industrial molds, a rigidity of more than 1 {$\mu$}m/MPa has been measured close to the center of the plate. The second goal of this paper is to show the influence of mold deflection on dimensional properties. If the cavity thickness is small as for our 1-mm-thick plate mold, considering an infinitely rigid mold cannot do realistic predictions of polymer pressure history, volumetric shrinkages and part mass. Nevertheless, in-plane shrinkage seems to be less affected by mold deflection. It means that the additional polymer mass due to mold deflection is mainly distributed in the part thickness.},
  language = {en},
  number = {7},
  urldate = {2016-11-01},
  journal = {Polymer Engineering \& Science},
  doi = {10.1002/pen.11301},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/pen.11301/abstract},
  author = {Delaunay, D. and Le Bot, P. and Fulchiron, R. and Luye, J. F. and Regnier, G.},
  month = jul,
  year = {2000},
  keywords = {Shrinkage,mold deflection,contact,interface},
  pages = {1692-1700}
}

@article{mendoza_spatial_2003,
  title = {Spatial Distribution of Molecular Orientation in Injection Molded {{iPP}}: Influence of Processing Conditions},
  volume = {44},
  issn = {0032-3861},
  shorttitle = {Spatial Distribution of Molecular Orientation in Injection Molded {{iPP}}},
  abstract = {In this paper, the influence of processing conditions on the spatial distribution of the molecular orientation was determined within the depth of the thickness of injection molded isotactic polypropylene (iPP) plates. Small 35{$\mu$}m-thick slices were microtomed from the surface to the core of 1 and 3mm-thick plates. The orientation functions along the three crystallographic axes were determined on the slices from IR dichroism measurements and WAXS pole figures. It was found that the orientation of the amorphous phase was low and the crystalline orientation had a maximum in the shearing layer, which was solidified during the filling stage. The plate thickness seemed to govern the global level of orientation, while the injection speed determined the thickness of the shearing layer without changing the maximum of orientation. Changing the mold temperature from 20 to 40 \textdegree{}C did not modify the molecular orientation. A specific bimodal crystalline orientation was found in the shearing layer. This crystalline structure continued in the post-filling layer, but the local symmetry axes tilted towards the core.},
  number = {11},
  urldate = {2019-09-21},
  journal = {Polymer},
  doi = {10/bdg6zh},
  url = {http://www.sciencedirect.com/science/article/pii/S0032386103002532},
  author = {Mendoza, R. and R{\'e}gnier, G. and Seiler, W. and Lebrun, J. L.},
  month = may,
  year = {2003},
  keywords = {Injection molding,Isotactic polypropylene,Molecular orientation},
  pages = {3363-3373}
}

@incollection{galeski_nano_2009,
  title = {Nano- and {{Micromechanics}} of {{Crystalline Polymers}}},
  isbn = {978-3-446-41323-8},
  urldate = {2019-09-21},
  booktitle = {Nano- and {{Micromechanics}} of {{Polymer Blends}} and {{Composites}}},
  publisher = {{Carl Hanser Verlag GmbH \& Co. KG}},
  url = {https://www.hanser-elibrary.com/doi/abs/10.3139/9783446430129.001},
  author = {Galeski, A. and Regnier, G.},
  month = may,
  year = {2009},
  pages = {1-58}
}

@phdthesis{passaro_du_2014,
  title = {{Du couple mat{\'e}riau-proc{\'e}d{\'e} {\`a} la qualit{\'e} per{\c c}ue : {\'e}laboration d'un outil pr{\'e}dictif de la perception visuelle des pi{\`e}ces en injection de polypropyl{\`e}ne d'int{\'e}rieur du v{\'e}hicule.}},
  shorttitle = {{Du couple mat{\'e}riau-proc{\'e}d{\'e} {\`a} la qualit{\'e} per{\c c}ue}},
  abstract = {Le but du travail est d'analyser l'impact du couple grain/mat{\'e}riau des pi{\`e}ces grain{\'e}es en injection de polypropyl{\`e}ne sur les caract{\'e}ristiques de la surface et leur Qualit{\'e} Per{\c c}ue. En l'absence d'outils pr{\'e}existants, un syst{\`e}me de caract{\'e}risation et de pr{\'e}diction des propri{\'e}t{\'e}s d'aspect des \guillemotleft{} grains \guillemotright{} a {\'e}t{\'e} d{\'e}velopp{\'e} int{\'e}gralement. Il comprend un r{\'e}f{\'e}rentiel de caract{\'e}risation ({\`a} 12 descripteurs) et deux instruments : un panel visuel entra{\^i}n{\'e} sp{\'e}cifiquement et un panel instrumental. Le panel instrumental utilise des mesures issues de 3 instruments. Il a permis d'{\'e}laborer 12 mod{\`e}les pr{\'e}dictifs. En appliquant ce dispositif {\`a} un ensemble d'{\'e}chantillons en polypropyl{\`e}ne inject{\'e}, il a {\'e}t{\'e} possible de dissocier l'influence de l'esth{\'e}tique du grain (descripteurs de \guillemotleft{} fantaisie \guillemotright{}) de ce qui permet de diff{\'e}rencier mati{\`e}res et qualit{\'e} de r{\'e}alisation (descripteurs de \guillemotleft{} brillant \guillemotright{}). Une exploration des impacts de la composition des mati{\`e}res, de la topographie des grains et de la r{\'e}ponse de la mati{\`e}re {\`a} cette topographie lors de l'injection a {\'e}t{\'e} r{\'e}alis{\'e}e sur un groupe de 4 grains et 3 mati{\`e}res, pour 3 couleurs. Enfin, une large enqu{\^e}te client a {\'e}t{\'e} men{\'e}e, dont les r{\'e}sultats ont {\'e}t{\'e} exploit{\'e}s pour {\'e}tablir comment les caract{\'e}ristiques {\'e}valu{\'e}es par le panel entrent en relation avec la perception de la qualit{\'e}. Il appara{\^i}t que l'esth{\'e}tique du grain et son association {\`a} la couleur, puis le brillant per{\c c}u, impactent majoritairement les jugements de qualit{\'e}. Quoique prospectif, ce travail permet de d{\'e}velopper des \guillemotleft{} g{\^e}nes \guillemotright{} de grains adapt{\'e}s {\`a} diff{\'e}rents types de client{\`e}le en consid{\'e}rant d{\`e}s le d{\'e}part l'influence du couple \guillemotleft{} g{\'e}om{\'e}trie du grain - potentiel visuel de la mati{\`e}re \guillemotright.},
  language = {fr},
  urldate = {2018-03-19},
  school = {Ecole Nationale Sup{\'e}rieure des Mines de Saint-Etienne},
  url = {https://tel.archives-ouvertes.fr/tel-01150587/document},
  author = {Passaro, Caterina},
  month = dec,
  year = {2014}
}

@phdthesis{pairel_maitrise_2016,
  type = {Habilitation {\`a} Diriger Des Recherches},
  title = {Ma{\^i}trise de La Qualit{\'e} G{\'e}om{\'e}trique Des Produits},
  abstract = {L'habilitation {\`a} diriger les recherches (HDR) est le dipl{\^o}me universitaire n{\'e}cessaire {\`a}
la direction de th{\`e}se de doctorat et {\`a} l'acc{\`e}s au corps des Professeurs d'Universit{\'e}. C'est une
reconnaissance, d'une part, de la capacit{\'e} {\`a} {\'e}laborer des projets de recherche, {\`a} les animer et {\`a}
diriger de jeunes chercheurs ; et d'autre part, de la qualit{\'e} et de l'originalit{\'e} scientifique des
recherches d{\'e}velopp{\'e}es.
Aussi,  dans  la  premi{\`e}re  partie,  les  projets  de  recherche  que  j'ai  {\'e}labor{\'e}s  et  des
codirections de th{\`e}se que j'ai  assum{\'e}es sont pr{\'e}sent{\'e}s chronologiquement.  Pour chacun,  le
bilan de la production scientifique est dress{\'e} et un r{\'e}capitulatif final classe cette production
suivant diff{\'e}rents crit{\`e}res. 
Dans la deuxi{\`e}me partie, les r{\'e}sultats scientifiques obtenus depuis mon doctorat sont
pr{\'e}sent{\'e}s en d{\'e}tail, organis{\'e}s par les quatre {\'e}tapes de la qualit{\'e} g{\'e}om{\'e}trique des produits,
th{\`e}me g{\'e}n{\'e}ral de mes travaux : 
\textbullet{} en Pr{\'e}paration, il s'agit essentiellement d'{\'e}tablir les caract{\'e}ristiques {\`a} mesurer
sur les produits, d'identifier les param{\`e}tres du proc{\'e}d{\'e} qui les fabrique, et d'{\'e}tablir un mod{\`e}le
variationnel  direct  des  caract{\'e}ristiques  en  fonction  des  param{\`e}tres.  Trois  m{\'e}thodes  sont
pr{\'e}sent{\'e}es : COPILOT-PRO\textregistered{} permettant de corriger des cotes \guillemotleft{}1D\guillemotright{} ; une correction des profils
\guillemotleft{} 2D \guillemotright{} obtenus par emboutissage ; et une m{\'e}thode \guillemotleft{} 3D \guillemotright{} de correction par points. En outre la
m{\'e}thode COPILOT-PRO\textregistered{} permet d'{\'e}tablir les {\'e}tapes de mesurage {\`a} introduire dans le processus
de fabrication.
\textbullet{} en Fabrication, il s'agit essentiellement d'utiliser les mod{\`e}les variationnels directs
coupl{\'e}s  {\`a}  une  surveillance  statistique  des  caract{\'e}ristiques  mesur{\'e}es  sur  les  exemplaires
successifs du produit. La m{\'e}thode COSELECT permet ce couplage ainsi qu'une s{\'e}lection des
param{\`e}tres {\`a} ajuster.
\textbullet{} en V{\'e}rification, il s'agit essentiellement d'{\^e}tre capable de v{\'e}rifier le respect, par
les produits, des tol{\'e}rances g{\'e}om{\'e}triques par zones ou par fronti{\`e}res. La th{\'e}orie des CALIBRES
AJUSTANTS, {\'e}labor{\'e}e pour cela, est pr{\'e}sent{\'e}e ainsi que son usage en m{\'e}trologie.
\textbullet{} enfin en Sp{\'e}cification, il s'agit d'{\^e}tre capable de sp{\'e}cifier la g{\'e}om{\'e}trie optimale
du produit et les tol{\'e}rances d'{\'e}cart autour d'elle pour les produits fabriqu{\'e}s. Un mod{\`e}le, dit en
\guillemotleft{} zones li{\'e}es \guillemotright, et son langage graphique sont pr{\'e}sent{\'e}s avant d'{\^e}tre critiqu{\'e}s au regard des
besoins de la Fabrication. La critique concerne aussi les outils de tol{\'e}rancement normalis{\'e}s par
l'ISO qui utilisent {\'e}galement les zones de tol{\'e}rance.
Du  besoin  et  de  l'analyse  des  outils  de  sp{\'e}cification  actuels,  l'axe  des  recherches
futures, sur le sujet de la sp{\'e}cification, est pr{\'e}sent{\'e}.
Enfin  la  troisi{\`e}me  partie  dresse  le  panel  de  mon  implication  dans  des  groupes
scientifiques, dans les structures de l'Universit{\'e} et dans la p{\'e}dagogie de mes enseignements.},
  school = {ComUE Grenoble-Alpes},
  url = {https://hal.archives-ouvertes.fr/tel-01470017},
  author = {Pairel, Eric},
  month = dec,
  year = {2016},
  keywords = {Coselect,tolérancement,GD\&T,COPILOT-PRO,CALIBRES AJUSTANTS,Metrologie}
}

@inproceedings{baath_new_2012,
  title = {Towards {{New Interferometer Technology}} for {{Surface Metrology}}},
  volume = {1},
  abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 49 universities and research institutions.},
  language = {eng},
  urldate = {2019-09-22},
  booktitle = {12th {{International Conference}} of the {{European Society}} for {{Precision Engineering}} and {{Nanotechnology}} ({{EUSPEN}} 2012), {{June}} 4th-7th, 2012, {{Stockholm}}, {{Sweden}}},
  publisher = {{EUSPEN}},
  url = {http://urn.kb.se/resolve?urn=urn:nbn:se:hh:diva-19612},
  author = {B{\aa}{\aa}th, Lars and Ros{\'e}n, Bengt-G{\"o}ran},
  year = {2012},
  pages = {158-161}
}

@article{hunter_matplotlib_2007,
  title = {Matplotlib: {{A 2D Graphics Environment}}},
  volume = {9},
  shorttitle = {Matplotlib},
  abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems},
  number = {3},
  journal = {Computing in Science Engineering},
  doi = {10/drbjhg},
  author = {Hunter, J. D.},
  month = may,
  year = {2007},
  keywords = {2D graphics package,application development,computer graphics,Computer languages,Equations,Graphical user interfaces,Graphics,Image generation,interactive scripting,Interpolation,mathematics computing,Matplotlib,object-oriented programming,operating system,Operating systems,Packaging,Programming profession,publication-quality image generation,Python,scientific programming,scripting languages,software packages,user interface,User interfaces},
  pages = {90-95}
}

@misc{caswell_matplotlib_2019,
  title = {Matplotlib/Matplotlib v3.1.0},
  copyright = {Open Access},
  abstract = {matplotlib: plotting with Python},
  urldate = {2019-09-23},
  howpublished = {Zenodo},
  url = {https://zenodo.org/record/2893252},
  author = {Caswell, Thomas A and Droettboom, Michael and Hunter, John and Firing, Eric and Lee, Antony and Klymak, Jody and Stansby, David and Andrade, Elliott Sales De and Nielsen, Jens Hedegaard and Varoquaux, Nelle and Root, Benjamin and Hoffmann, Tim and Elson, Phil and May, Ryan and Dale, Darren and {Jae-Joon Lee} and Sepp{\"a}nen, Jouni K. and McDougall, Damon and Straw, Andrew and Hobson, Paul and Gohlke, Christoph and Yu, Tony S and Ma, Eric and Vincent, Adrien F. and Silvester, Steven and Moad, Charlie and Katins, Jan and Kniazev, Nikita and Ariza, Federico and Ernest, Elan},
  month = may,
  year = {2019},
  doi = {10.5281/zenodo.2893252}
}

@techreport{iso_tc108_iso_2008,
  address = {{Geneva, Switzerland}},
  type = {Standard},
  title = {{{ISO}} 18434-1:2008 {{Condition}} Monitoring and Diagnostics of Machines \textemdash{} {{Thermography}} \textemdash{} {{Part}} 1: {{General}} Procedures},
  shorttitle = {{{ISO}} 18434-1:2008},
  abstract = {ISO 18434-1:2008 provides an introduction to the application of infrared thermography (IRT) to machinery condition monitoring and diagnostics, where "machinery" includes machine auxiliaries such as valves, fluid and electrically powered machines, and machinery-related heat exchanger equipment. In addition, IR applications pertaining to machinery performance assessment are addressed.

ISO 18434-1:2008: introduces the terminology of IRT as it pertains to condition monitoring and diagnostics of machines; describes the types of IRT procedures and their merits; provides guidance on establishing severity assessment criteria for anomalies identified by IRT; outlines methods and requirements for carrying out IRT of machines, including safety recommendations; provides information on data interpretation, and assessment criteria and reporting requirements; provides procedures for determining and compensating for reflected apparent temperature, emissivity, and attenuating media.

ISO 18434-1:2008 also encompasses testing procedures for determining and compensating for reflected apparent temperature, emissivity, and attenuating media when measuring the surface temperature of a target with a quantitative IRT camera.},
  language = {English},
  number = {ISO 18434-1:2008},
  urldate = {2019-09-23},
  institution = {{International Organization for Standardization / International Organization for Standardization}},
  url = {https://www.iso.org/standard/41648.html},
  author = {{ISO/TC 108}},
  month = mar,
  year = {2008},
  pages = {24},
  price = {CHF 118}
}

@phdthesis{legrand_thermographie_2002,
  type = {Thesis},
  title = {Thermographie Multispectrale Haute et Basse Temp{\'e}rature : Application Au Contr{\^o}le Non Destructif},
  shorttitle = {Thermographie Multispectrale Haute et Basse Temp{\'e}rature},
  abstract = {,},
  urldate = {2019-09-23},
  school = {Dijon},
  url = {http://www.theses.fr/2002DIJOS014},
  author = {Legrand, Anne-Claire},
  month = jan,
  year = {2002},
  keywords = {Contrôle non destructif par thermographie infrarouge,Informatique et instrumentation de l'image,Mesure,Qualité -- Contrôle,Thermographie,Traitement d'images}
}

@inproceedings{herrmann_cracks_2019,
  title = {Cracks Detection on Glass Object Based on Active Thermography Approach},
  volume = {11172},
  abstract = {Thermal imaging is nowadays widely used in many industrial areas for non destructive analysis. This paper focuses on the detection by active thermography of cracks in a transparent and specular piece of glass. A continuous laser scans the target by creating a hot spot that is acquired by a thermal camera. A thermal map is computed by averaging of thermal measurements for all the positions of the hot spot on the target. Then edge filters are applied to highlight the cracks. Experimental results demonstrate the validity of the material damage estimation in a short processing time.},
  urldate = {2019-09-23},
  booktitle = {Fourteenth {{International Conference}} on {{Quality Control}} by {{Artificial Vision}}},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10/gf8qfn},
  url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11172/111721E/Cracks-detection-on-glass-object-based-on-active-thermography-approach/10.1117/12.2520920.short},
  author = {Herrmann, Thomas and Migniot, Cyrille and Aubreton, Olivier},
  month = jul,
  year = {2019},
  pages = {111721E}
}

@article{eren_scanning_2009,
  title = {Scanning from Heating: {{3D}} Shape Estimation of Transparent Objects from Local Surface Heating},
  volume = {17},
  copyright = {\&\#169; 2009 Optical Society of America},
  issn = {1094-4087},
  shorttitle = {Scanning from Heating},
  abstract = {Today, with quality becoming increasingly important, each product requires three-dimensional in-line quality control. On the other hand, the 3D reconstruction of transparent objects is a very difficult problem in computer vision due to transparency and specularity of the surface. This paper proposes a new method, called Scanning From Heating (SFH), to determine the surface shape of transparent objects using laser surface heating and thermal imaging. Furthermore, the application to transparent glass is discussed and results on different surface shapes are presented.},
  language = {EN},
  number = {14},
  urldate = {2019-09-23},
  journal = {Optics Express},
  doi = {10.1364/OE.17.011457},
  url = {https://www.osapublishing.org/oe/abstract.cfm?uri=oe-17-14-11457},
  author = {Eren, Gonen and Aubreton, Olivier and Meriaudeau, Fabrice and Secades, L. A. Sanchez and Fofi, David and Naskali, A. Teoman and Truchetet, Frederic and Ercil, Aytul},
  month = jul,
  year = {2009},
  keywords = {Absorption coefficient,Carbon dioxide lasers,Laser beams,Laser energy,Laser irradiation,Laser sources},
  pages = {11457-11468}
}

@article{zhu_unpaired_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.10593},
  primaryClass = {cs},
  title = {Unpaired {{Image}}-to-{{Image Translation}} Using {{Cycle}}-{{Consistent Adversarial Networks}}},
  abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X \textbackslash{}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y \textbackslash{}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) \textbackslash{}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  journal = {arXiv:1703.10593 [cs]},
  url = {http://arxiv.org/abs/1703.10593},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  month = mar,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{ronneberger_unet_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.04597},
  primaryClass = {cs},
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  volume = {Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  shorttitle = {U-{{Net}}},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  journal = {arXiv:1505.04597 [cs]},
  url = {http://arxiv.org/abs/1505.04597},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  month = may,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{wang_image_2004,
  title = {Image Quality Assessment: From Error Visibility to Structural Similarity},
  volume = {13},
  issn = {1057-7149},
  shorttitle = {Image Quality Assessment},
  abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu/{$\sim$}lcv/ssim/.},
  number = {4},
  journal = {IEEE Transactions on Image Processing},
  doi = {10.1109/TIP.2003.819861},
  author = {Wang, Zhou and Bovik, A. C. and Sheikh, H. R. and Simoncelli, E. P.},
  month = apr,
  year = {2004},
  keywords = {Quality control,data mining,Humans,Algorithms,Pattern Recognition; Automated,Layout,Indexes,data compression,image coding,visual perception,JPEG,JPEG2000,distorted image,error sensitivity,error visibility,human visual perception,human visual system,image compression,image database,perceptual image quality assessment,reference image,structural information,structural similarity index,Degradation,Image quality,Quality assessment,Transform coding,Visual system,Data Interpretation; Statistical,Hypermedia,Image Enhancement,Image Interpretation; Computer-Assisted,Information Storage and Retrieval,Models; Statistical,Reproducibility of Results,Sensitivity and Specificity,Signal Processing; Computer-Assisted,Subtraction Technique},
  pages = {600-612}
}

@article{wu_learning_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.07584},
  primaryClass = {cs},
  title = {Learning a {{Probabilistic Latent Space}} of {{Object Shapes}} via {{3D Generative}}-{{Adversarial Modeling}}},
  abstract = {We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.},
  urldate = {2016-12-19},
  journal = {arXiv:1610.07584 [cs]},
  url = {http://arxiv.org/abs/1610.07584},
  author = {Wu, Jiajun and Zhang, Chengkai and Xue, Tianfan and Freeman, William T. and Tenenbaum, Joshua B.},
  month = oct,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning}
}

@phdthesis{adragna_tolerancement_2007,
  title = {{Tol{\'e}rancement des Syst{\`e}mes Assembl{\'e}s, une approche par le Tol{\'e}rancement Inertiel et Modal}},
  abstract = {L'objectif du tol{\'e}rancement des syst{\`e}mes assembl{\'e}s est de d{\'e}finir les tol{\'e}rances des composants permettant la satisfaction du client : l'assemblage et le bon fonctionnement des syst{\`e}mes. On peut identifier des cas limites du tol{\'e}rancement pour lesquelles ces objectifs sont mal respect{\'e}s. Diff{\'e}rents mod{\`e}les de complexit{\'e} croissante sont identifi{\'e}s : 1D, 3D et 3D avec prise en compte des d{\'e}fauts de forme. On peut aussi distinguer diff{\'e}rentes hypoth{\`e}ses de comportement des composants du syst{\`e}me : rigide non d{\'e}formable, flexible {\'e}lastique et {\'e}lasto-plastique. Ce projet de recherche se propose de traiter les probl{\'e}matiques de tol{\'e}rancement sous l'hypoth{\`e}se de comportement rigide des composants, pour les diff{\'e}rentes complexit{\'e}s de mod{\'e}lisation existante : 1D, 3D et 3D avec d{\'e}fauts de forme. Notre approche se fonde sur le crit{\`e}re inertie I de quantification des {\'e}carts d'une caract{\'e}ristique par rapport {\`a} sa cible. Ce crit{\`e}re, bas{\'e} sur la fonction de perte de Taguchi, est propos{\'e} par Pillet dans une m{\'e}thode de tol{\'e}rancement 1D. Pour {\'e}tendre cette approche de tol{\'e}rancement {\`a} la qualification de plusieurs caract{\'e}ristiques, dans le cas des surfaces, nous choisissons d'utiliser la m{\'e}thode modale de description des d{\'e}fauts de forme de toutes g{\'e}om{\'e}tries propos{\'e}e par Samper. Ces deux approches, de quantification (inertiel) et de qualification (modal), {\'e}voluent pour enfin {\^e}tre fusionn{\'e}es et proposer une m{\'e}thode d'acceptation multi-caract{\'e}ristique, le tol{\'e}rancement modal inertiel. La mod{\'e}lisation 1D du tol{\'e}rancement est bien cern{\'e}e. Le graphe (d,s2) permet l'analyse des tol{\'e}rances des composants en vue de v{\'e}rifier la conformit{\'e} de la r{\'e}sultante pour toutes les configurations. On met ainsi {\`a} disposition un outil permettant de v{\'e}rifier un tol{\'e}rancement quelle que soit l'expression de la tol{\'e}rance, intervalle de tol{\'e}rance ou inertie, quels que soient les indices de capabilit{\'e} et sous l'hypoth{\`e}se statistique d'ind{\'e}pendance des variables ou non (non illustr{\'e}e ici).},
  language = {fr},
  urldate = {2019-03-20},
  school = {Universit{\'e} de Savoie},
  url = {https://tel.archives-ouvertes.fr/tel-00403876/document},
  author = {Adragna, Pierre-Antoine},
  month = dec,
  year = {2007}
}

@article{adragna_proposition_2010,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1002.0253},
  title = {A Proposition of {{3D}} Inertial Tolerancing to Consider the Statistical Combination of the Location and Orientation Deviations},
  volume = {10},
  issn = {1477-9056, 1741-8178},
  abstract = {Tolerancing of assembly mechanisms is a major interest in the product life cycle. One can distinguish several models with growing complexity, from 1-dimensional (1D) to 3-dimensional (3D) (including form deviations), and two main tolerancing assumptions, the worst case and the statistical hypothesis. This paper presents an approach to 3D statistical tolerancing using a new acceptance criterion. Our approach is based on the 1D inertial acceptance criterion that is extended to 3D and form acceptance. The modal characterisation is used to describe the form deviation of a geometry as the combination of elementary deviations (location, orientation and form). The proposed 3D statistical tolerancing is applied on a simple mechanism with lever arm. It is also compared to the traditional worst-case tolerancing using a tolerance zone.},
  number = {1/2/3},
  urldate = {2016-10-26},
  journal = {International Journal of Product Development},
  doi = {10.1504/IJPD.2010.029985},
  url = {http://arxiv.org/abs/1002.0253},
  author = {Adragna, Pierre-Antoine and Samper, Serge and Pillet, Maurice},
  year = {2010},
  keywords = {Computer Science - Other Computer Science},
  pages = {26}
}

@article{goic_multi_2011,
  title = {Multi Scale Modal Decomposition of Primary Form, Waviness and Roughness of Surfaces},
  volume = {33},
  issn = {1932-8745},
  abstract = {This article introduces an innovative method for the multi-scale analysis of high value-added surfaces, which consists of applying a method based on a new parameterization. This kind of surface parameterization refers to natural modes of vibration, and is therefore named modal parameterization. It allows us to characterize the form, waviness and roughness defects of a surface. This parameterization opens up new fields of analysis, such as the appearance quality of surfaces. It is thereby possible to decompose a measured surface in a vector basis, of which vectors are represented by plane natural eigenmodes sorted by frequency and complexity. Different filtering operations can then be produced, such as extracting the primary form of the surface. To analyze the perceived quality of surfaces, these investigations focus on two approaches: that appearance defects have small periodicity, and that there is a link between curvatures and the visual impact of an anomaly. This methodology is applied to two prestige lighters, whose surfaces were measured by extended field confocal microscopy. Moreover, a prospect of this work is to develop an augmented-reality-type monitoring tool for sensory experts.},
  language = {eng},
  number = {5},
  journal = {Scanning},
  doi = {10.1002/sca.20253},
  author = {Go{\"i}c, Ga{\"e}tan Le and Favreli{\`e}re, Hugues and Samper, Serge and Formosa, Fabien},
  month = sep,
  year = {2011},
  pages = {332-341},
  pmid = {21695704}
}

@article{goic_multiscale_2016,
  title = {Multiscale Roughness Analysis of Engineering Surfaces: {{A}} Comparison of Methods for the Investigation of Functional Correlations},
  volume = {66-67},
  issn = {0888-3270},
  shorttitle = {Multiscale Roughness Analysis of Engineering Surfaces},
  abstract = {This study investigates the correlations between the topography of different damaged rough surfaces and process conditions. Several surfaces are measured and compared to determine if they can be discriminated. The analysis is performed by using Gaussian Filtering, Wavelet Transform and a more recent approach named Discrete Modal Decomposition. Standardized 3D roughness parameters are computed for each multiscale method, filter (e.g., high-pass, low-pass and band-pass) and available scale. The relevance (i.e., the ability to discriminate surface topographies corresponding to different process conditions) is then investigated using a statistical analysis based on the MesRugTM expert system. The results indicate clear differences between the multiscale methods and show that the Wavelet approach is useful when characterizing localized surface defects while Gaussian Filtering is more appropriate for highly periodic morphological structures. For more complex topographies, this study also clearly shows that the Discrete Modal Decomposition exhibits compelling abilities that fall between those of the Gaussian and Wavelet approaches; this method is clearly more relevant than the Gaussian method in the case of localized defects and less relevant in the case of highly periodical structures and fractal surfaces (1/f{$\alpha$} spectrum). This can be explained by the modulated frequency/amplitude descriptors generated via the modal basis.},
  number = {Supplement C},
  journal = {Mechanical Systems and Signal Processing},
  doi = {10.1016/j.ymssp.2015.05.029},
  url = {http://www.sciencedirect.com/science/article/pii/S0888327015002824},
  author = {Go{\"i}c, Ga{\"e}tan Le and Bigerelle, Maxence and Samper, Serge and Favreli{\`e}re, Hugues and Pillet, Maurice},
  month = jan,
  year = {2016},
  keywords = {discrete wavelet transform,Roughness analysis,3D topography,Discrete Modal Decomposition,Gaussian Filtering},
  pages = {437-457},
  file = {/Users/Pierre/Zotero/storage/7ZSSGQ2S/Goïc et al. - 2016 - Multiscale roughness analysis of engineering surfa.pdf;/Users/Pierre/Zotero/storage/SH9M52IX/S0888327015002824.html},
  note = {00009}
}

@article{favreliere_modal_2009,
  title = {Modal {{Tolerancing}} : {{From}} Metrology to Specifications},
  shorttitle = {Modal {{Tolerancing}}},
  abstract = {The product definition uses an established language between every actors of the product life cycle : designer, producer and metrologist. It can be a common language, such as the ISO standard, or specific one. It allows to define dimensions, tolerances and any kind of geometrical errors : form, position and orientation. The production of parts is never to be perfect, therefore it is necessary to reach a compromise between the designer requirements and the manufacturing processes. Morevoer, controlling the acceptable variations of the real geometry around the target remains a major issue of geometrical tolerancing. Many existing models aiming at defining those variations but appear to be insufficient to characterize the form error. The objective of the presented work is to define a new form parameterization from which a model of form tolerancing is built. Resulting from vibratory mechanics and namely called "modal parameterization", it models a geometrical element in a geometrical form space. First, the modal parameterization is used to visualize the geometrical errors of surfaces and to characterize those surfaces using tailor-made metrology, this latter approach is called Modal Metrology of Surfaces (MemoSurf\textregistered{}). Beyond visualization, the metrology is also a checking stage of the geometrical specifications. As a consequence, in the second approach, an assembly method allowing to assess the accuracy of an assembly exhibiting form errors. The applicability of the presented tolerancing model is related to a proper statement of the functional requirements. Finally, the last part of this document proposes to define a specification language throughout the modal parameterization framework. In order to assess the acceptable variations of the modal parameters, two industrial exemples are presented : the first is concerning a process control and the second, a requirement specification.},
  author = {Favreliere, Hugues},
  month = oct,
  year = {2009}
}

@article{marschner_imagebased_2000,
  title = {Image-Based Bidirectional Reflectance Distribution Function Measurement},
  volume = {39},
  copyright = {\textcopyright{} 2000 Optical Society of America},
  issn = {1539-4522},
  abstract = {We present a new image-based process for measuring a surface's bidirectional reflectance rapidly, completely, and accurately. Requiring only two cameras, a light source, and a test sample of known shape, our method generates densely spaced samples covering a large domain of illumination and reflection directions. We verified our measurements both by tests of internal consistency and by comparison against measurements made with a gonioreflectometer. The resulting data show accuracy rivaling that of custom-built dedicated instruments.},
  language = {EN},
  number = {16},
  urldate = {2017-05-22},
  journal = {Applied Optics},
  doi = {10.1364/AO.39.002592},
  url = {https://www.osapublishing.org/abstract.cfm?uri=ao-39-16-2592},
  author = {Marschner, Stephen R. and Westin, Stephen H. and Lafortune, Eric P. F. and Torrance, Kenneth E.},
  month = jun,
  year = {2000},
  keywords = {Image analysis,scattering,Scattering measurements,Optical properties,rough surfaces},
  pages = {2592-2600}
}

@inproceedings{ghosh_brdf_2007,
  title = {{{BRDF Acquisition}} with {{Basis Illumination}}},
  abstract = {Realistic descriptions of surface reflectance have long been a topic of interest in both computer vision and computer graphics research. In this paper, we describe a novel and fast approach for the acquisition of bidirectional reflectance distribution functions (BRDFs). We develop a novel theory for directly measuring BRDFs in a basis representation by projecting incident light as a sequence of basis functions from a spherical zone of directions. We derive an orthonormal basis over spherical zones that is ideally suited for this task. BRDF values outside the zonal directions are extrapolated by re-projecting the zonal measurements into a spherical harmonics basis, or by fitting analytical reflection models to the data. We verify this approach with a compact optical setup that requires no moving parts and only a small number of image measurements. Using this approach, a BRDF can be measured in just a few minutes.},
  booktitle = {2007 {{IEEE}} 11th {{International Conference}} on {{Computer Vision}}},
  doi = {10.1109/ICCV.2007.4408935},
  author = {Ghosh, Abhijeet and Achutha, Shruthi and Heidrich, Wolfgang and O'Toole, Matthew},
  month = oct,
  year = {2007},
  keywords = {Analytical models,analytical reflection models,basis illumination,Bidirectional control,bidirectional reflectance distribution functions,BRDF acquisition,computer graphics,Computer vision,Distribution functions,Extrapolation,Light sources,Lighting,Optical filters,Optical noise,optical setup,orthonormal basis,reflectivity,spherical harmonics basis,spherical zones,surface reflectance},
  pages = {1-8}
}

@article{desage_visual_2014,
  title = {Visual {{Quality Inspection}} and {{Fine Anomalies}}: {{Methods}} and {{Application}}},
  volume = {435},
  shorttitle = {Visual {{Quality Inspection}} and {{Fine Anomalies}}},
  abstract = {This study develops a surface inspection methodology used to detect complex geometry products and metallic reflective surfaces imperfections. This work is based on combination of three complementary methods: an optical one (structured light information), an algorithmic one (data processing) and a statistical one (parameters processing). A usual industrial application illustrates this processing. \textcopyright{} IFIP International Federation for Information Processing 2014.},
  journal = {IFIP Advances in Information and Communication Technology},
  url = {https://hal.inria.fr/hal-01260893},
  author = {D{\'e}sage, Simon-Fr{\'e}d{\'e}ric and Pitard, Gilles and Pillet, Maurice and Favreli{\`e}re, Hugues and Frelin, Fabrice and Samper, Serge and Le Go{\"i}c, Ga{\"e}tan},
  month = nov,
  year = {2014},
  pages = {94-106}
}

@article{zhang_friction_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.07998},
  primaryClass = {cs},
  title = {Friction from {{Reflectance}}: {{Deep Reflectance Codes}} for {{Predicting Physical Surface Properties}} from {{One}}-{{Shot In}}-{{Field Reflectance}}},
  shorttitle = {Friction from {{Reflectance}}},
  abstract = {Images are the standard input for vision algorithms, but one-shot infield reflectance measurements are creating new opportunities for recognition and scene understanding. In this work, we address the question of what reflectance can reveal about materials in an efficient manner. We go beyond the question of recognition and labeling and ask the question: What intrinsic physical properties of the surface can be estimated using reflectance? We introduce a framework that enables prediction of actual friction values for surfaces using one-shot reflectance measurements. This work is a first of its kind vision-based friction estimation. We develop a novel representation for reflectance disks that capture partial BRDF measurements instantaneously. Our method of deep reflectance codes combines CNN features and fisher vector pooling with optimal binary embedding to create codes that have sufficient discriminatory power and have important properties of illumination and spatial invariance. The experimental results demonstrate that reflectance can play a new role in deciphering the underlying physical properties of real-world scenes.},
  urldate = {2019-09-27},
  journal = {arXiv:1603.07998 [cs]},
  url = {http://arxiv.org/abs/1603.07998},
  author = {Zhang, Hang and Dana, Kristin and Nishino, Ko},
  month = mar,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{lacombe_see_2017,
  address = {{Bourges, France}},
  title = {Voir pour Toucher? Caractérisation Optique de la Sensation d’Adhérence},
  shorttitle = {See to Touch ?},
  abstract = {Le domaine de la qualit{\'e} en industrie a connu au cours des derni{\`e}res d{\'e}cennies une s{\'e}ries d'{\'e}tudes et d'innovations ayant eu pour but d'am{\'e}liorer les perceptions rendues au travers des produits fabriqu{\'e}s. L' {\'e}tape de validation de la qualit{\'e} ne rel{\`e}ve donc plus seulement de v{\'e}rifications techniques, mais d'une diversit{\'e} de contr{\^o}les qui correspondent aux sens impliqu{\'e}s lors de l'interaction avec un produit. Bien que le contr{\^o}le de la qualit{\'e} visuelle ait {\'e}t{\'e} relativement bien {\'e}tudi{\'e} et instrument{\'e}, la qualit{\'e} associ{\'e}e au sens du toucher pose encore de nombreux probl{\`e}mes en industrie. De plus, dans certains cas d'application, le contact avec les produits {\`a} des fins de validation n'est pas possible, ou entra{\^i}nerait une d{\'e}gradation de celui-ci. Ce projet a ainsi vis{\'e} {\`a} {\'e}tudier les possibilit{\'e}s de substitution du contr{\^o}le au toucher par un contr{\^o}le sans contact, plus pr{\'e}cis{\'e}ment en ce qui concerne la perception de la sensation d'adh{\'e}rence. Tout d'abord la performance de la perception visuelle humaine a des fins de contr{\^o}le haptique a {\'e}t{\'e} {\'e}valu{\'e}e. Les r{\'e}sultats sugg{\`e}rent qu'il n'est a priori pas possible de contr{\^o}ler {\`a} l'{\oe}il nu la sensation d'adh{\'e}rence. En revanche, cette etude montre qu'une instrumentation optique permet effectivement d'obtenir une mesure fiable de l'adh{\'e}rence. En effet, l'acquisition d'images gr{\^a}ce a un syst{\`e}me multi-{\'e}clairages et une analyse sp{\'e}cifique a permis d'{\'e}tablir des relations entre certains param{\`e}tres particuliers des images et une mesure d'adh{\'e}rence avec contact sur divers {\'e}chantillons.},
  booktitle = {{{QUALITA}} 2017},
  url = {https://hal.archives-ouvertes.fr/hal-01587088},
  author = {Lacombe, Thomas and Albert, Bruno and Dereli, Halise and Tomczyk, M{\'e}lanie and {Nait-Youcef}, Youva and Philppa, Jocelyn and Favreli{\`e}re, Hughes},
  month = aug,
  year = {2017},
  keywords = {Contrôle qualité,Haptique,Sensation and Perception,Vision par ordinateur,Visuel}
}

@article{riviere_multispectral_2012,
  title = {Multispectral Polarized {{BRDF}}: Design of a Highly Resolved Reflectometer and Development of a Data Inversion Method},
  issn = {0078-5466},
  shorttitle = {Multispectral Polarized {{BRDF}}},
  abstract = {Multispectral and polarized light reflectance measurements are very useful to characterize materials such as paint coatings. This article presents an overview of an automated high-angular resolved, in-plane multispectral polarized reflectometer and its calibration process. A comprehensive study based on multispectral BRDF and DOLP measurements is conducted on different colour and glossy aspects of paint coatings. An original inverse method from in-plane measurements is used to model the out-of-plane BRDF and to investigate the role of the surface and subsurface scattering phenomena in its components.},
  language = {English},
  number = {Vol. 42, nr 1},
  urldate = {2016-12-07},
  journal = {Optica Applicata},
  url = {https://www.infona.pl/resource/bwmeta1.element.baztech-article-BPW7-0019-0081},
  author = {Riviere, N. and Ceolato, R. and Hespel, L.},
  year = {2012},
  pages = {7-22}
}

@article{pezzaniti_mueller_1995,
  title = {{Mueller matrix imaging polarimetry}},
  volume = {34},
  issn = {0091-3286},
  language = {English (US)},
  number = {6},
  urldate = {2018-11-01},
  journal = {Optical Engineering},
  url = {https://arizona.pure.elsevier.com/en/publications/mueller-matrix-imaging-polarimetry},
  author = {Pezzaniti, Larry, Joseph and Chipman, Russell A},
  month = jun,
  year = {1995},
  pages = {1558-1568}
}

@article{queau_design_2018b,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.02683},
  title = {Design and Simplified Calibration of a {{Mueller}} Imaging Polarimeter for Material Classification},
  volume = {43},
  issn = {0146-9592, 1539-4794},
  abstract = {This study is concerned with the design of a Mueller imaging polarimeter for the visualization of spatially-varying Mueller matrix fields. A simplified calibration procedure is advocated, where all the optical elements are calibrated simultaneously rather than independently as in the state-of-the-art. This is shown to significantly reduce the bias inherent to sequential calibration methods. In addition, this procedure requires no reference sample, it allows calibration both in transmission or in reflection modes, and it relies on ready-to-use cameras. Put together, these novelties should help non-specialists in optics designing and calibrating a Mueller imaging polarimeter for applications such as material classification.},
  language = {EN},
  number = {20},
  urldate = {2019-09-26},
  journal = {Optics Letters},
  doi = {10/gf8gdj},
  url = {http://arxiv.org/abs/1811.02683},
  author = {Qu{\'e}au, Yvain and Leporcq, Florian and Alfalou, Ayman},
  month = oct,
  year = {2018},
  keywords = {Physics - Instrumentation and Detectors,Physics - Optics},
  pages = {4941-4944}
}

@inproceedings{queau_learning_2019,
  address = {{Mulhouse, France}},
  title = {Learning to Classify Materials Using {{Mueller}} Imaging Polarimetry},
  abstract = {This study investigates the combination of Mueller imaging polarimetry with machine learning for the automated optical classication of raw materials. It shows that standard image classication techniques based on support vector machines or deep neural networks can readily be applied to polarimetric data extracted from Mueller matrix measurements. The feasability of such an approach is empirically demonstrated through the classication of multispectral depolarization images of real-world materials (banana, wood and foam samples).},
  urldate = {2019-09-26},
  booktitle = {Fourteenth {{International Conference}} on {{Quality Control}} by {{Artificial Vision}} ({{QCAV}})},
  doi = {10/gf8twj},
  url = {https://hal.archives-ouvertes.fr/hal-02088951},
  author = {Qu{\'e}au, Yvain and Leporcq, Florian and Lechervy, Alexis and Alfalou, Ayman},
  month = may,
  year = {2019},
  keywords = {deep neural networks,machine learning,Material classication,Mueller imaging,polarimetry}
}

@article{samper_form_2007,
  title = {Form {{Defects Tolerancing}} by {{Natural Modes Analysis}}},
  volume = {7},
  issn = {1530-9827},
  number = {1},
  urldate = {2019-03-20},
  journal = {Journal of Computing and Information Science in Engineering},
  doi = {10.1115/1.2424247},
  url = {https://computingengineering.asmedigitalcollection.asme.org/article.aspx?articleid=1400699},
  author = {Samper, Serge and Formosa, Fabien},
  month = mar,
  year = {2007},
  pages = {44-51}
}

@book{timoshenko_vibration_1937,
  address = {{Stanford University Palo Alto, CA, USA}},
  edition = {2nd},
  title = {Vibration {{Problems In Engineering}}},
  lccn = {31611},
  language = {eng},
  urldate = {2019-03-21},
  publisher = {{D. Van Nostrand Company Inc., New-York}},
  url = {http://archive.org/details/vibrationproblem031611mbp},
  author = {Timoshenko, S.},
  collaborator = {{Osmania University} and {Digital Library Of India}},
  year = {1937},
  keywords = {NATURAL SCIENCES}
}

@article{haralick_textural_1973,
  title = {Textural {{Features}} for {{Image Classification}}},
  volume = {SMC-3},
  issn = {0018-9472},
  abstract = {Texture is one of the important characteristics used in identifying objects or regions of interest in an image, whether the image be a photomicrograph, an aerial photograph, or a satellite image. This paper describes some easily computable textural features based on gray-tone spatial dependancies, and illustrates their application in category-identification tasks of three different kinds of image data: photomicrographs of five kinds of sandstones, 1:20 000 panchromatic aerial photographs of eight land-use categories, and Earth Resources Technology Satellite (ERTS) multispecial imagery containing seven land-use categories. We use two kinds of decision rules: one for which the decision regions are convex polyhedra (a piecewise linear decision rule), and one for which the decision regions are rectangular parallelpipeds (a min-max decision rule). In each experiment the data set was divided into two parts, a training set and a test set. Test set identification accuracy is 89 percent for the photomicrographs, 82 percent for the aerial photographic imagery, and 83 percent for the satellite imagery. These results indicate that the easily computable textural features probably have a general applicability for a wide variety of image-classification applications.},
  number = {6},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  doi = {10.1109/TSMC.1973.4309314},
  author = {Haralick, R. M. and Shanmugam, K. and Dinstein, I.},
  month = nov,
  year = {1973},
  keywords = {spatial resolution,Humans,Application software,Crops,Earth,Image classification,Image resolution,Piecewise linear techniques,Satellites,Testing},
  pages = {610-621}
}

@article{yamazaki_fourdirectional_2016,
  title = {Four-Directional Pixel-Wise Polarization {{CMOS}} Image Sensor Using Air-Gap Wire Grid on 2.5-{$M$}m Back-Illuminated Pixels},
  abstract = {Polarization information is useful in highly functional imaging. This paper presents a four-directional pixel-wise polarization CMOS image sensor using an air-gap wire grid on 2.5-{$\mu$}m back-illuminated pixels. The fabricated air-gap wire grid polarizer achieved a transmittance of 63.3 \% and an extinction ratio of 85 at 550 nm, outperforming conventional polarization sensors. The pixel-wise polarizers fabricated with the wafer process on back-illuminated image sensors exhibit good oblique-incidence characteristics, even with small polarization pixels of 2.5 {$\mu$}m. The proposed image sensor realizes mega-pixel various fusion-imaging applications, such as surface reflection reduction, highly accurate depth mapping, and condition-robust surveillance.},
  journal = {2016 IEEE International Electron Devices Meeting (IEDM)},
  doi = {10/gf8zsc},
  author = {Yamazaki, Tomohiro and Maruyama, Yasushi and Uesaka, Yusuke and Nakamura, Motoaki and Matoba, Yoshihisa and Terada, Takashi and Komori, Kenta and Ohba, Yoshiyuki and Arakawa, Shinichi and Hirasawa, Yasutaka and Kondo, Yuhi and Murayama, Jun and Akiyama, Kentaro and Oike, Yusuke and Sato, Shuzo and Ezaki, Takayuki},
  year = {2016},
  keywords = {Active pixel sensor,CMOS,Functional imaging,Image sensor,Incidence matrix,Oblique projection,Polarization (waves),Polarizer Device Component,sensor (device)},
  pages = {8-8}
}

@article{denizart_thermal_1995,
  title = {Thermal Stresses and Strains in Injection Moulding: Experiments and Computations},
  volume = {30},
  issn = {1573-4803},
  shorttitle = {Thermal Stresses and Strains in Injection Moulding},
  abstract = {The effect of thermal stresses in moulded components has been demonstrated by measuring the warpage of centre-gated discs which were injected in a mould, the two sides of which were regulated at different temperatures. Polypropylene discs exhibited larger warpage than polystyrene ones. The measured curvature increased with the difference in mould temperature, and decreased when the packing pressure increased. Thermal stresses and strains have been computed assuming an orthotropic thermo-elastic behaviour for the polymer. A three-dimensional finite element method was used. The different steps of the moulding process, and associated changes of boundary conditions, have been taken into account. The results are qualitatively in agreement with the measurements. Finally the calculation was applied to a box mould, leading to a characteristic box deformation in ``diamond ace'' form which is consistent with the experiments.},
  language = {en},
  number = {2},
  journal = {Journal of Materials Science},
  doi = {10.1007/BF00354424},
  url = {https://doi.org/10.1007/BF00354424},
  author = {Denizart, O. and Vincent, M. and Agassant, J. F.},
  month = jan,
  year = {1995},
  keywords = {Polymer,Polypropylene,Finite element method,Boundary Condition,Polystyrene},
  pages = {552-560}
}

@article{atkinson_highsensitivity_2018,
  title = {High-Sensitivity Analysis of Polarization by Surface Reflection},
  volume = {29},
  issn = {1432-1769},
  abstract = {The exploitation of polarization information in the field of computer vision has become progressively more popular during the last few decades. This is primarily due to (1) the fact that polarization is a source of mostly untapped information for machine vision; (2) the relative computational ease by which geometrical information about a scene (e.g. surface normals) may be extracted from polarization data; and (3) the recent introduction of camera hardware able to capture polarization data in real time. The motivation for this paper is that a detailed quantitative study into the precision of polarization measurements with respect to expectation has yet to be performed. The paper therefore presents a detailed analysis and optimization of the key aspects of data capture necessary to acquire the most precise (as opposed to fast) results for the benefit of future research into the field of ``polarization vision.'' The paper mainly focuses on a rotating polarizer method as this is shown to be the most accurate for high-sensitivity measurements. Commercial polarization cameras by contrast generally sacrifice precision for the benefit of much shorter capture times. That said, the paper reviews the state of the art in polarization camera technology and quantitatively evaluates the performance of one such camera: the Fraunhofer ``POLKA.''},
  language = {en},
  number = {7},
  urldate = {2019-09-29},
  journal = {Machine Vision and Applications},
  doi = {10/gf8ztt},
  url = {https://doi.org/10.1007/s00138-018-0962-7},
  author = {Atkinson, Gary A. and Ernst, J{\"u}rgen D.},
  month = oct,
  year = {2018},
  keywords = {Physics-based reflectance modeling,Polarization,Shape analysis},
  pages = {1171-1189}
}

@article{brewster_experiments_1833,
  title = {Experiments on the Depolarization of Light as Exhibited by Various Mineral, Animal, and Vegetable Bodies, with a Reference of the Phenomena to the General Principles of Polarization. {{By David Brewster}}, {{LL}}. {{D}}. {{F}}. {{R}}. {{S}}. {{Edin}}. and {{F}}. {{S}}. {{A}}. {{In}} a Letter Addressed to the {{Right Hon}}. {{Sir Joseph Banks}}, {{Bart}}. {{K}}. {{B}}. {{P}}. {{R}}. {{S}}},
  volume = {2},
  abstract = {When a ray of light has been so modified by reflection or refraction that in certain planes it is not divided into two parts by a prism of Iceland spar, that ray is said to be polarized; but it may again, by several means, be rendered divisible, and is then said to be depolarized. The object of the author, in this letter, is to comprise experiments on the depolarizing properties of a great variety of substances at the same time, and thence to deduce the general principles on which the various degrees or modes in which they exhibit this property depend. Dr. Brewster has already, in a former communication, described the general phenomena of depolarization by mica, calcareous spar, topaz, and other regularly crystallized bodies, which have two neutral axes at right angles to each other, and two depolarizing axes also at right angles to each other, but making angles of 45\textdegree{} with each of the neutral axes.},
  urldate = {2019-09-29},
  journal = {Abstracts of the Papers Printed in the Philosophical Transactions of the Royal Society of London},
  doi = {10.1098/rspl.1815.0003},
  url = {https://royalsocietypublishing.org/doi/abs/10.1098/rspl.1815.0003},
  author = {Brewster, David},
  month = jan,
  year = {1833},
  pages = {4-6}
}

@article{tyo_review_2006,
  title = {Review of Passive Imaging Polarimetry for Remote Sensing Applications},
  volume = {45},
  copyright = {\&\#169; 2006 Optical Society of America},
  issn = {2155-3165},
  abstract = {Imaging polarimetry has emerged over the past three decades as a powerful tool to enhance the information available in a variety of remote sensing applications. We discuss the foundations of passive imaging polarimetry, the phenomenological reasons for designing a polarimetric sensor, and the primary architectures that have been exploited for developing imaging polarimeters. Considerations on imaging polarimeters such as calibration, optimization, and error performance are also discussed. We review many important sources and examples from the scientific literature.},
  language = {EN},
  number = {22},
  urldate = {2019-09-29},
  journal = {Applied Optics},
  doi = {10/bx8tpw},
  url = {https://www.osapublishing.org/ao/abstract.cfm?uri=ao-45-22-5453},
  author = {Tyo, J. Scott and Goldstein, Dennis L. and Chenault, David B. and Shaw, Joseph A.},
  month = aug,
  year = {2006},
  keywords = {Circular polarization,Discrete Fourier transforms,Fourier transform spectroscopy,Holographic optical elements,Point spread function,Polarized light},
  pages = {5453-5469}
}

@article{opencv_library,
  title = {The {{OpenCV Library}}},
  volume = {25},
  number = {11},
  journal = {Dr. Dobb's Journal of Software Tools},
  url = {http://www.drdobbs.com/open-source/the-opencv-library/184404319},
  author = {Bradski, Gary},
  year = {2000},
  keywords = {bibtex-import},
  pages = {120-125},
  citeulike-article-id = {2236121},
  posted-at = {2008-01-15 19:21:54},
  priority = {4}
}

@article{garrido-jurado_automatic_2014,
  title = {Automatic Generation and Detection of Highly Reliable Fiducial Markers under Occlusion},
  volume = {47},
  issn = {0031-3203},
  abstract = {This paper presents a fiducial marker system specially appropriated for camera pose estimation in applications such as augmented reality and robot localization. Three main contributions are presented. First, we propose an algorithm for generating configurable marker dictionaries (in size and number of bits) following a criterion to maximize the inter-marker distance and the number of bit transitions. In the process, we derive the maximum theoretical inter-marker distance that dictionaries of square binary markers can have. Second, a method for automatically detecting the markers and correcting possible errors is proposed. Third, a solution to the occlusion problem in augmented reality applications is shown. To that aim, multiple markers are combined with an occlusion mask calculated by color segmentation. The experiments conducted show that our proposal obtains dictionaries with higher inter-marker distances and lower false negative rates than state-of-the-art systems, and provides an effective solution to the occlusion problem.},
  number = {6},
  journal = {Pattern Recognition},
  doi = {10.1016/j.patcog.2014.01.005},
  url = {http://www.sciencedirect.com/science/article/pii/S0031320314000235},
  author = {{Garrido-Jurado}, S. and {Mu{\~n}oz-Salinas}, R. and {Madrid-Cuevas}, F. J. and {Mar{\'i}n-Jim{\'e}nez}, M. J.},
  month = jun,
  year = {2014},
  keywords = {Computer vision,Augmented reality,Fiducial marker},
  pages = {2280-2292}
}

@article{garrido-jurado_generation_2016,
  title = {Generation of Fiducial Marker Dictionaries Using {{Mixed Integer Linear Programming}}},
  volume = {51},
  issn = {0031-3203},
  abstract = {Square-based fiducial markers are one of the most popular approaches for camera pose estimation due to its fast detection and robustness. In order to maximize their error correction capabilities, it is required to use an inner binary codification with a large inter-marker distance. This paper proposes two Mixed Integer Linear Programming (MILP) approaches to generate configurable square-based fiducial marker dictionaries maximizing their inter-marker distance. The first approach guarantees the optimal solution, however, it can only be applied to relatively small dictionaries and number of bits since the computing times are too long for many situations. The second approach is an alternative formulation to obtain suboptimal dictionaries within restricted time, achieving results that still surpass significantly the current state of the art methods.},
  journal = {Pattern Recognition},
  doi = {10.1016/j.patcog.2015.09.023},
  url = {http://www.sciencedirect.com/science/article/pii/S0031320315003544},
  author = {{Garrido-Jurado}, S. and {Mu{\~n}oz-Salinas}, R. and {Madrid-Cuevas}, F. J. and {Medina-Carnicer}, R.},
  month = mar,
  year = {2016},
  keywords = {Computer vision,Fiducial markers,Augmented reality,MILP,Mixed Integer Linear Programming},
  pages = {481-491}
}

@article{romero-ramirez_speeded_2018,
  title = {Speeded up Detection of Squared Fiducial Markers},
  volume = {76},
  issn = {0262-8856},
  abstract = {Squared planar markers have become a popular method for pose estimation in applications such as autonomous robots, unmanned vehicles and virtual trainers. The markers allow estimating the position of a monocular camera with minimal cost, high robustness, and speed. One only needs to create markers with a regular printer, place them in the desired environment so as to cover the working area, and then registering their location from a set of images. Nevertheless, marker detection is a time-consuming process, especially as the image dimensions grows. Modern cameras are able to acquire high resolutions images, but fiducial marker systems are not adapted in terms of computing speed. This paper proposes a multi-scale strategy for speeding up marker detection in video sequences by wisely selecting the most appropriate scale for detection, identification and corner estimation. The experiments conducted show that the proposed approach outperforms the state-of-the-art methods without sacrificing accuracy or robustness. Our method is up to 40 times faster than the state-of-the-art method, achieving over 1000 fps in 4 K images without any parallelization.},
  journal = {Image and Vision Computing},
  doi = {10.1016/j.imavis.2018.05.004},
  url = {http://www.sciencedirect.com/science/article/pii/S0262885618300799},
  author = {{Romero-Ramirez}, Francisco J. and {Mu{\~n}oz-Salinas}, Rafael and {Medina-Carnicer}, Rafael},
  month = aug,
  year = {2018},
  keywords = {Fiducial markers,Marker mapping,SLAM},
  pages = {38-47}
}

@article{gendre_full_2011,
  title = {Full {{Stokes}} Polarimetric Imaging Using a Single Ferroelectric Liquid Crystal Device},
  volume = {50},
  abstract = {This paper reports the design and the implementation of a Stokes imaging polarimeter able to provide full polarimetric information at 200 fps. This portable implementation is based on a division-oftime architecture and uses a single ferroelectric liquid crystal device as the polarization modulating element. Our system is designed to work at 532 nm with natural light or with controlled illumination, without temperature control. We propose an optimized driving scheme of the modulator such that the liquid crystal device can produce four polarization states which makes it possible to retrieve the full polarimetric information. The modulator characterization is reported and experimental results are provided.},
  number = {8},
  urldate = {2019-09-29},
  journal = {Optical Engineering},
  doi = {10/fg4bfk},
  url = {https://hal.archives-ouvertes.fr/hal-00851931},
  author = {Gendre, Luc and Foulonneau, Alban and Bigue, Laurent},
  year = {2011},
  keywords = {ferroelectrics,imaging,liquid crystals,Stokes polarimetry},
  pages = {1-9}
}

@article{wolff_polarization_1995,
  title = {Polarization Camera Sensors},
  volume = {13},
  issn = {0262-8856},
  abstract = {Recently, polarization vision has been shown to simplify some important image understanding tasks that can be more difficult to perform with intensity vision alone. This, together with the more general capabilities of polarization vision for image understanding, motivates the building of camera sensors that automatically sense and process polarization information. Described in this paper are a variety of designs for polarization camera sensors that have been built to automatically sense partial linearly polarized light, and computationally process this sensed polarization information at pixel resolution to produce a visualization of reflected polarization from a scene, and/or a visualization of physical information in a scene directly related to sensed polarization. The three designs for polarization camera sensors presented utilize (i) serial acquisition of polarization components using liquid crystals, (ii) parallel acquisition of polarization components using a stereo pair of cameras and a polarizing beamsplitter, and (iii) a prototype photosensing chip with three scanlines, each scanline coated with a particular orientation of polarizing material. As the sensory input to polarization camera sensors subsumes that of standard intensity cameras, they can potentially significantly expand the application potential of computer vision. A number of images taken with polarization cameras are presented, showing potential applications to image understanding, object recognition, circuit board inspection and marine biology.},
  number = {6},
  journal = {Image and Vision Computing},
  doi = {10.1016/0262-8856(95)94383-B},
  url = {http://www.sciencedirect.com/science/article/pii/026288569594383B},
  author = {Wolff, Lawrence B and Andreou, Andreas G},
  month = aug,
  year = {1995},
  keywords = {polarization vision,image understanding,camera sensors},
  pages = {497-510}
}

@article{riou_calibration_2015,
  title = {Calibration and Disparity Maps for a Depth Camera Based on a Four-Lens Device},
  volume = {24},
  abstract = {We propose a model of depth camera based on a four-lens device. This device is used for validating
alternate approaches for calibrating multi-view cameras and also for computing disparity or depth images.
Calibration method arises from previous works, where principles of variable homography were extended for 3-D
measurement. In this paper, calibration is performed between two contiguous views obtained on the same image
sensor. This approach leads to propose a new approach for simplifying calibration, by using the properties of the
variable homography. The second part of this paper addresses new principles for obtaining disparity images without
any matching. A fast algorithm using a contour propagation algorithm is proposed without requiring structured or
random pattern projection. These principles are proposed in a framework of quality control by vision, for inspection
in natural illumination. By preserving scene photometry, some other standard controls, as for example calipers,
shape recognition, or barcode reading, can be done conjointly with 3-D measurements. Approaches presented in this
paper are evaluated. Firstly, we show that rapid calibration is relevant for devices mounted with multi lenses.
Secondly, synthetic and real experimentations validate our method for computing depth images.},
  journal = {Journal of Electronic Imaging},
  doi = {10.1117/1.JEI.24.6.061108},
  url = {https://hal.archives-ouvertes.fr/hal-01510301},
  author = {Riou, C{\'e}cile and Colicchio, Bruno and Lauffenburger, Jean-Philippe and Haeberle, Olivier and Cudel, Christophe},
  year = {2015},
  keywords = {camera calibration,Homography,Multi-view cameras,depth cameras,disparity map},
  pages = {1-11}
}

@article{stokes_composition_1851,
  title = {On the {{Composition}} and {{Resolution}} of {{Streams}} of {{Polarized Light}} from Different {{Sources}}},
  volume = {9},
  abstract = {Not Available},
  urldate = {2019-03-27},
  journal = {Transactions of the Cambridge Philosophical Society},
  url = {http://adsabs.harvard.edu/abs/1851TCaPS...9..399S},
  author = {Stokes, G. G.},
  year = {1851},
  pages = {399}
}

@article{li_method_2014,
  title = {A Method to Calculate {{Stokes}} Parameters and Angle of Polarization of Skylight from Polarized {{CIMEL}} Sun/Sky Radiometers},
  volume = {149},
  issn = {0022-4073},
  abstract = {The polarized CIMEL sun/sky radiometers have been routinely operated within the Sun/sky-radiometer Observation NETwork (SONET) in China and some sites of the AErosol RObotic NETwork (AERONET) around the world. However, the polarization measurements are not yet widely used due to in a certain degree the lack of Stokes parameters derived directly from these polarization measurements. Meanwhile, it have been shown that retrievals of several microphysical properties of aerosol particles can be significantly improved by using degree of linear polarization (DoLP) measurements of polarized CIMEL sun/sky radiometers (CE318-DP). The Stokes parameters Q and U, as well as angle of polarization (AoP) contain additional information about linear polarization and its orientation. A method to calculate Stokes parameters Q, U, and AoP from CE318-DP polarized skylight measurements is introduced in this study. A new polarized almucantar geometry based on CE318-DP is measured to illustrate abundant variation features of these parameters. The polarization parameters calculated in this study are consistent with previous results of DoLP and I, and also comparable to vector radiative transfer simulations.},
  urldate = {2019-09-29},
  journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},
  doi = {10/gf8zt7},
  url = {http://www.sciencedirect.com/science/article/pii/S0022407314003744},
  author = {Li, L. and Li, Z. and Li, K. and Blarel, L. and Wendisch, M.},
  month = dec,
  year = {2014},
  keywords = {Polarization,Aerosol particles,Polarized almucantar geometry,Stokes parameters,Sun/sky radiometer},
  pages = {334-346}
}

@inproceedings{wilburn_high_2005,
  address = {{New York, NY, USA}},
  series = {{{SIGGRAPH}} '05},
  title = {High {{Performance Imaging Using Large Camera Arrays}}},
  abstract = {The advent of inexpensive digital image sensors and the ability to create photographs that combine information from a number of sensed images are changing the way we think about photography. In this paper, we describe a unique array of 100 custom video cameras that we have built, and we summarize our experiences using this array in a range of imaging applications. Our goal was to explore the capabilities of a system that would be inexpensive to produce in the future. With this in mind, we used simple cameras, lenses, and mountings, and we assumed that processing large numbers of images would eventually be easy and cheap. The applications we have explored include approximating a conventional single center of projection video camera with high performance along one or more axes, such as resolution, dynamic range, frame rate, and/or large aperture, and using multiple cameras to approximate a video camera with a large synthetic aperture. This permits us to capture a video light field, to which we can apply spatiotemporal view interpolation algorithms in order to digitally simulate time dilation and camera motion. It also permits us to create video sequences using custom non-uniform synthetic apertures.},
  urldate = {2019-09-29},
  booktitle = {{{ACM SIGGRAPH}} 2005 {{Papers}}},
  publisher = {{ACM}},
  doi = {10/b7wkxc},
  url = {http://doi.acm.org/10.1145/1186822.1073259},
  author = {Wilburn, Bennett and Joshi, Neel and Vaish, Vaibhav and Talvala, Eino-Ville and Antunez, Emilio and Barth, Adam and Adams, Andrew and Horowitz, Mark and Levoy, Marc},
  year = {2005},
  keywords = {camera arrays,spatiotemporal sampling,synthetic aperture},
  pages = {765--776}
}

@inproceedings{gendre_interest_2018,
  title = {Interest of Polarimetric Refocused Images Calibrated in Depth for Control by Vision},
  volume = {10677},
  abstract = {This work shows the interest of combining polarimetric and light-field imaging. Polarimetric imaging is known for its capabilities to highlight and reveal contrasts or surfaces that are not visible in standard intensity images. This imaging mode requires to capture multiple images with a set of different polarimetric filters. The images can either be captured by a temporal or spatial multiplexing, depending on the polarimeter model used. On the other hand, light-field imaging, which is categorized in the field of computational imaging, is also based on a combination of images that allows to extract 3D information about the scene. In this case, images are either acquired with a camera array, or with a multi-view camera such as a plenoptic camera. One of the major interests of a light-field camera is its capability to produce different kind of images, such as sub-aperture images used to compute depth images, full focus images or images refocused at a specific distance used to detect defects for instance. In this paper, we show that refocused images of a light-field camera can also be computed in the context of polarimetric imaging. The 3D information contained in the refocused images can be combined with the linear degree of polarization and can be obtained with an unique device in one acquisition. An example illustrates how these two coupled imaging modes are promising, especially for the industrial control and inspection by vision.},
  urldate = {2019-03-05},
  booktitle = {Unconventional {{Optical Imaging}}},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10.1117/12.2306939},
  url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10677/106771W/Interest-of-polarimetric-refocused-images-calibrated-in-depth-for-control/10.1117/12.2306939.short},
  author = {Gendre, L. and Bazeille, S. and Bigu{\'e}, L. and Cudel, C.},
  month = may,
  year = {2018},
  pages = {106771W}
}

@inproceedings{hanrahan_reflection_1993,
  title = {Reflection from Layered Surfaces Due to Subsurface Scattering},
  abstract = {The reflection of light from most materials consists of two major terms: the specular and the diffuse. Specular reflection may be modeled from first principles by considering a rough surface consisting of perfect reflectors, or micro-facets. Diffuse reflection is generally considered to result from multiple scattering either from a rough surface or from within a layer near the surface. Accounting for diffuse reflection by Lambert's Cosine Law, as is universally done in computer graphics, is not a physical theory based on first principles. This paper presents a model for subsurface scattering in layered surfaces in terms of one-dimensional linear transport theory. We derive explicit formulas for backscattering and transmission that can be directly incorporated in most rendering systems, and a general Monte Carlo method that is easily added to a ray tracer. This model is particularly appropriate for common layered materials appearing in nature, such as biological tissues (e.g. skin, leaves, etc.) or inorganic materials (e.g. snow, sand, paint, varnished or dusty surfaces). As an application of the model, we simulate the appearance of a face and a cluster of leaves from experimental data describing their layer properties. CR Categories and Subject Descriptors: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism.},
  booktitle = {{{SIGGRAPH}}},
  doi = {10/b4tw3j},
  author = {Hanrahan, Pat and Kr{\"u}ger, Wolfgang},
  year = {1993},
  keywords = {Computer graphics,Diffuse reflection,Lambert's cosine law,Light transport theory,Monte Carlo method,Ray tracing (graphics),Simulation,Subsurface scattering}
}

@article{torrance_theory_1967,
  title = {Theory for {{Off}}-{{Specular Reflection From Roughened Surfaces}}*},
  volume = {57},
  copyright = {\&\#169; 1967 Optical Society of America},
  abstract = {The directional distribution of radiant flux reflected from roughened surfaces is analyzed on the basis of geometrical optics. The analytical model assumes that the surface consists of small, randomly disposed, mirror-like facets. Specular reflection from these facets plus a diffuse component due to multiple reflections and/or internal scattering are postulated as the basic mechanisms of the reflection process. The effects of shadowing and masking of facets by adjacent facets are included in the analysis. The angular distributions of reflected flux predicted by the analysis are in very good agreement with experiment for both metallic and nonmetallic surfaces. Moreover, the analysis successfully predicts the off-specular maxima in the reflection distribution which are observed experimentally and which emerge as the incidence angle increases. The model thus affords a rational explanation for the off-specular peak phenomenon in terms of mutual masking and shadowing of mirror-like, specularly reflecting surface facets.},
  language = {EN},
  number = {9},
  urldate = {2019-09-29},
  journal = {JOSA},
  doi = {10/dw6f9n},
  url = {https://www.osapublishing.org/josa/abstract.cfm?uri=josa-57-9-1105},
  author = {Torrance, K. E. and Sparrow, E. M.},
  month = sep,
  year = {1967},
  keywords = {Absorption coefficient,Bidirectional reflectance distribution function,Fresnel reflection,Geometric optics,Material properties,Refractive index},
  pages = {1105-1114}
}

@article{nayar_separation_1997,
  title = {Separation of {{Reflection Components Using Color}} and {{Polarization}}},
  volume = {21},
  issn = {1573-1405},
  abstract = {Specular reflections and interreflections produce strong highlights in brightness images. These highlights can cause vision algorithms for segmentation, shape from shading, binocular stereo, and motion estimation to produce erroneous results. A technique is developed for separating the specular and diffuse components of reflection from images. The approach is to use color and polarization information, simultaneously, to obtain constraints on the reflection components at each image point. Polarization yields local and independent estimates of the color of specular reflection. The result is a linear subspace in color space in which the local diffuse component must lie. This subspace constraint is applied to neighboring image points to determine the diffuse component. In contrast to previous separation algorithms, the proposed method can handle highlights on surfaces with substantial texture, smoothly varying diffuse reflectance, and varying material properties. The separation algorithm is applied to several complex scenes with textured objects and strong interreflections. The separation results are then used to solve three problems pertinent to visual perception; determining illumination color, estimating illumination direction, and shape recovery.},
  language = {en},
  number = {3},
  urldate = {2019-09-29},
  journal = {International Journal of Computer Vision},
  doi = {10/cp36j8},
  url = {https://doi.org/10.1023/A:1007937815113},
  author = {Nayar, Shree K. and Fang, Xi-Sheng and Boult, Terrance},
  month = feb,
  year = {1997},
  keywords = {Diffuse Component,Image Point,Motion Estimation,Shape Recovery,Specular Reflection},
  pages = {163-186}
}

@inproceedings{debevec_acquiring_2000,
  address = {{New York, NY, USA}},
  series = {{{SIGGRAPH}} '00},
  title = {Acquiring the {{Reflectance Field}} of a {{Human Face}}},
  isbn = {978-1-58113-208-3},
  abstract = {We present a method to acquire the reflectance field of a human face and use these measurements to render the face under arbitrary changes in lighting and viewpoint. We first acquire images of the face from a small set of viewpoints under a dense sampling of incident illumination directions using a light stage. We then construct a reflectance function image for each observed image pixel from its values over the space of illumination directions. From the reflectance functions, we can directly generate images of the face from the original viewpoints in any form of sampled or computed illumination. To change the viewpoint, we use a model of skin reflectance to estimate the appearance of the reflectance functions for novel viewpoints. We demonstrate the technique with synthetic renderings of a person's face under novel illumination and viewpoints.},
  urldate = {2019-09-29},
  booktitle = {Proceedings of the 27th {{Annual Conference}} on {{Computer Graphics}} and {{Interactive Techniques}}},
  publisher = {{ACM Press/Addison-Wesley Publishing Co.}},
  doi = {10/d329r5},
  url = {http://dx.doi.org/10.1145/344779.344855},
  author = {Debevec, Paul and Hawkins, Tim and Tchou, Chris and Duiker, Haarm-Pieter and Sarokin, Westley and Sagar, Mark},
  year = {2000},
  keywords = {facial animation,image-based modeling,rendering and lighting},
  pages = {145--156}
}












































@inproceedings{laput_synthetic_2017,
  title = {Synthetic {{Sensors}}: {{Towards General}}-{{Purpose Sensing}}},
  isbn = {978-1-4503-4655-9},
  shorttitle = {Synthetic {{Sensors}}},
  urldate = {2018-02-23},
  booktitle = {Proceedings of the 2017 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  publisher = {{ACM}},
  doi = {10.1145/3025453.3025773},
  url = {http://dl.acm.org/citation.cfm?id=3025453.3025773},
  author = {Laput, Gierad and Zhang, Yang and Harrison, Chris},
  month = feb,
  year = {2017},
  pages = {3986-3999}
}

@article{van_eck_comparison_2010,
  title = {A Comparison of Two Techniques for Bibliometric Mapping: {{Multidimensional}} Scaling and {{VOS}}},
  volume = {61},
  issn = {1532-2890},
  shorttitle = {A Comparison of Two Techniques for Bibliometric Mapping},
  abstract = {VOS is a new mapping technique that can serve as an alternative to the well-known technique of multidimensional scaling (MDS). We present an extensive comparison between the use of MDS and the use of VOS for constructing bibliometric maps. In our theoretical analysis, we show the mathematical relation between the two techniques. In our empirical analysis, we use the techniques for constructing maps of authors, journals, and keywords. Two commonly used approaches to bibliometric mapping, both based on MDS, turn out to produce maps that suffer from artifacts. Maps constructed using VOS turn out not to have this problem. We conclude that in general maps constructed using VOS provide a more satisfactory representation of a dataset than maps constructed using well-known MDS approaches.},
  language = {en},
  number = {12},
  urldate = {2016-11-02},
  journal = {Journal of the American Society for Information Science and Technology},
  doi = {10.1002/asi.21421},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/asi.21421/abstract},
  author = {{van Eck}, Nees Jan and Waltman, Ludo and Dekker, Rommert and {van den Berg}, Jan},
  month = dec,
  year = {2010},
  pages = {2405-2416}
}



@phdthesis{collomb_2018,
    abstract = {Ce travail a pour but de proposer : (i) un nouveau concept d'outillage {\`{a}} faible inertie thermique destin{\'{e}} {\`{a}} la transformation rapide des composites hautes performances visant {\`{a}} r{\'{e}}pondre aux enjeux industriels, (ii) une nouvelle approche du dimensionnement et de l'optimisation thermique de l'outillage bas{\'{e}}e sur l'exploitation d'un mod{\`{e}}le {\'{e}}l{\'{e}}ments finis peu co{\^{u}}teux. La d{\'{e}}marche s'appuie sur trois axes principaux : (i) mettre en place un processus permettant d'estimer le coefficient convectif et de mod{\'{e}}liser le thermor{\'{e}}gulateur avec la prise en compte des donn{\'{e}}es technologiques du syst{\`{e}}me de chauffe (fluide, puissance thermique, r{\'{e}}gulation, pompe),(ii) d{\'{e}}velopper une d{\'{e}}marche d'optimisation thermom{\'{e}}canique sous contrainte, reposant sur des m{\'{e}}thodes de m{\'{e}}tamod{\'{e}}lisation et d'optimisation par algorithme g{\'{e}}n{\'{e}}tique, (iii) qualifier la d{\'{e}}marche compl{\`{e}}te {\`{a}} l'aide de campagnes exp{\'{e}}rimentales permettant la confrontation aux r{\'{e}}sultats num{\'{e}}riques.},
    author = {Collomb, Jean},
    keywords = {Canaux conformables,Mat{\'{e}}- riaux composites,Mod{\'{e}}lisation,Optimisation thermo-m{\'{e}}canique,Outillage,Transferts de chaleur},
    pages = {214},
    school = {Communaut{\'{e}} Universit{\'{e}} Grenoble Alpes},
    title = {{Optimisation du processus de dimensionnement thermom{\'{e}}canique de Moule Autonome {\`{a}} Transfert Thermique Efficient pour la transformation rapide des mat{\'{e}}riaux composites {\`{a}} renforts continus}},
    url = {https://hal.archives-ouvertes.fr/tel-02022616},
    hal = {tel-02022616},
    year = {2018}
}

@article{bearman_what_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.02106},
  primaryClass = {cs},
  title = {What's the {{Point}}: {{Semantic Segmentation}} with {{Point Supervision}}},
  shorttitle = {What's the {{Point}}},
  abstract = {The semantic image segmentation task presents a trade-off between test time accuracy and training-time annotation cost. Detailed per-pixel annotations enable training accurate models but are very time-consuming to obtain, image-level class labels are an order of magnitude cheaper but result in less accurate models. We take a natural step from image-level annotation towards stronger supervision: we ask annotators to point to an object if one exists. We incorporate this point supervision along with a novel objectness potential in the training loss function of a CNN model. Experimental results on the PASCAL VOC 2012 benchmark reveal that the combined effect of point-level supervision and objectness potential yields an improvement of 12.9\% mIOU over image-level supervision. Further, we demonstrate that models trained with point-level supervision are more accurate than models trained with image-level, squiggle-level or full supervision given a fixed annotation budget.},
  urldate = {2019-08-30},
  journal = {arXiv:1506.02106 [cs]},
  url = {http://arxiv.org/abs/1506.02106},
  author = {Bearman, Amy and Russakovsky, Olga and Ferrari, Vittorio and {Fei-Fei}, Li},
  month = jun,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{baudet_visual_2011,
  title = {Visual Inspection of Products: A Comparison of the Methods Used to Evaluate Surface Anomalies.},
  shorttitle = {Visual Inspection of Products},
  abstract = {This paper presents and compares different approaches currently used to assess surface anomalies identified on a product. The common point between these methods is that they are based on a document presented in the form of a table, which is to help the inspector to assess the anomaly detected in a repeatable and reproducible way. We will present three types of table: criteria/level table, tree-like presentation table and an indexed table. As each of these tables presents certain limits when applied to the inspection of a product surface, we will describe the table proposed in order to help the inspector determine the intensity to attribute to the identified anomaly.},
  language = {en},
  urldate = {2019-08-30},
  url = {https://hal.archives-ouvertes.fr/hal-00712918},
  hal = {hal-00712918},
  author = {Baudet, Nathalie and Pillet, Maurice and Maire, Jean-Luc},
  month = jun,
  year = {2011}
}

@book{automotiveindustryactiongroup_measurement_2010,
  address = {{Southfield, Mich}},
  edition = {4},
  title = {Measurement {{Systems Analysis}} ({{MSA}}) {{Reference Manual}}},
  isbn = {978-1-60534-211-5},
  shorttitle = {Measurement Systems Analysis},
  abstract = {Measurement data is used in nearly every manufacturing process. As the quality of the data improves, the quality of decisions improves. This guide will help you assess the quality of your measurement systems, providing a basis for recognizing where improvements can be made. The result is knowledge that can be used to improve your measurement process, in turn improving repeatable product quality.},
  language = {eng},
  publisher = {{Automotive Industry Action Group}},
  url = {https://www.aiag.org/store/publications/details?ProductCode=MSA-4},
  author = {{Automotive Industry Action Group},  and Chrysler,  and Ford,  and General Motors, },
  year = {2010},
  note = {OCLC: 729994003}
}

@inproceedings{scholkopf_support_1999,
  address = {{Cambridge, MA, USA}},
  series = {{{NIPS}}'99},
  title = {Support {{Vector Method}} for {{Novelty Detection}}},
  abstract = {Suppose you are given some dataset drawn from an underlying probability distribution P and you want to estimate a "simple" subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified {$\nu$} between 0 and 1. We propose a method to approach this problem by trying to estimate a function f which is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. We provide a theoretical analysis of the statistical performance of our algorithm. The algorithm is a natural extension of the support vector algorithm to the case of unlabelled data.},
  urldate = {2019-09-01},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Neural Information Processing Systems}}},
  publisher = {{MIT Press}},
  url = {http://dl.acm.org/citation.cfm?id=3009657.3009740},
  author = {Sch{\"o}lkopf, Bernhard and Williamson, Robert and Smola, Alex and {Shawe-Taylor}, John and Platt, John},
  year = {1999},
  pages = {582--588},
}

@article{scholkopf_estimating_2001,
  title = {Estimating the {{Support}} of a {{High}}-{{Dimensional Distribution}}},
  volume = {13},
  issn = {0899-7667},
  abstract = {Suppose you are given some data set drawn from an underlying probability distribution P and you want to estimate a "simple" subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified value between 0 and 1. We propose a method to approach this problem by trying to estimate a function f that is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. The expansion coefficients are found by solving a quadratic programming problem, which we do by carrying out sequential optimization over pairs of input patterns. We also provide a theoretical analysis of the statistical performance of our algorithm. The algorithm is a natural extension of the support vector algorithm to the case of unlabeled data.},
  number = {7},
  urldate = {2019-09-01},
  journal = {Neural Comput.},
  doi = {10.1162/089976601750264965},
  url = {https://doi.org/10.1162/089976601750264965},
  author = {Sch{\"o}lkopf, Bernhard and Platt, John C. and {Shawe-Taylor}, John C. and Smola, Alex J. and Williamson, Robert C.},
  month = jul,
  year = {2001},
  pages = {1443--1471}
}

@article{plackett_design_1946,
    title = {The {Design} of {Optimum} {Multifactorial} {Experiments}},
    volume = {33},
    issn = {00063444},
    url = {http://www.jstor.org/stable/2332195?origin=crossref},
    doi = {10.2307/2332195},
    number = {4},
    urldate = {2016-09-26},
    journal = {Biometrika},
    author = {Plackett, R. L. and Burman, J. P.},
    month = jun,
    year = {1946},
    pages = {305}
}

@phdthesis{baudet_maitrise_2012,
  title = {{Ma{\^i}trise de la qualit{\'e} visuelle des produits - Formalisation du processus d'expertise et proposition d'une approche robuste de contr{\^o}le visuel humain}},
  abstract = {L'apparence d'un produit joue un r{\^o}le important dans la perception de sa qualit{\'e} par le client. Au-del{\`a} des fonctionnalit{\'e}s qu'il doit remplir, un produit doit d{\'e}sormais avoir un aspect irr{\'e}prochable. Il n'y a cependant pas de surface parfaite car, {\`a} un niveau de grossissement donn{\'e}, un {\'e}cart par rapport {\`a} une surface id{\'e}ale peut toujours {\^e}tre identifi{\'e}. Pour d{\'e}tecter cet {\'e}cart et {\'e}valuer son impact sur la qualit{\'e} per{\c c}ue du produit, les entreprises mettent g{\'e}n{\'e}ralement en place un contr{\^o}le visuel d'aspect de surface de leurs produits. Une premi{\`e}re th{\`e}se a {\'e}t{\'e} r{\'e}alis{\'e}e au Laboratoire SYMME afin de proposer une m{\'e}thodologie permettant de r{\'e}duire la variabilit{\'e} g{\'e}n{\'e}ralement observ{\'e}e sur les r{\'e}sultats de ce type de contr{\^o}le. Nos travaux se situent dans la continuit{\'e} de ces travaux avec l'objectif de proposer des m{\'e}thodes et des outils pour la ma{\^i}trise des trois {\'e}tapes d'un contr{\^o}le visuel d'aspect : exploration, {\'e}valuation et d{\'e}cision. Le projet de th{\`e}se r{\'e}alis{\'e} dans le cadre d'un programme europ{\'e}en de recherche INTERREG IV r{\'e}unissait diff{\'e}rentes Universit{\'e}s et Entreprises. Les pratiques des entreprises partenaires ont procur{\'e}s un terrain d'exp{\'e}rimentation des recherches propos{\'e}es. Partant de cette observation, nous avons propos{\'e} une conceptualisation du contr{\^o}le visuel humain d{\'e}bouchant sur des propositions de m{\'e}thodes et outils adapt{\'e}s aux trois {\'e}tapes. Ces propositions ont {\'e}t{\'e} test{\'e}es dans les entreprises partenaires pour v{\'e}rifier leur robustesse {\`a} la vari{\'e}t{\'e} des situations industriels. Nous avons par exemple propos{\'e} un nouveau test R2\&E2 Conformit{\'e} qui mesure la variabilit{\'e} d'un contr{\^o}le visuel et contribue {\`a} l'identification des sources possibles de cette variabilit{\'e}. Outre cette conceptualisation pour la cr{\'e}ation d'outils, nous listons un ensemble de recommandations {\`a} suivre par les entreprises pour une meilleure exploration des anomalies. Nous proposons {\'e}galement un ensemble d'attributs sensoriels permettant de caract{\'e}riser, en vue de l'{\'e}valuer, toute anomalie d'aspect. Enfin, nous montrons comment, en formalisant le processus d'expertise, un contr{\^o}leur peut {\'e}valuer une anomalie d'aspect et juger de son impact sur la qualit{\'e} per{\c c}ue du produit.},
  language = {fr},
  urldate = {2016-09-28},
  school = {Universit{\'e} de Grenoble},
  url = {https://tel.archives-ouvertes.fr/tel-00807304/document},
  HAL = {tel-00807304},
  author = {Baudet, Nathalie},
  month = dec,
  year = {2012},
  keywords = {Quality control,quality model,quality,visual inspection,Quality improvement,Quality Inspection,visual,Aspects de surface}
}

@article{desage_contraintes_2015,
  title = {{Contraintes et opportunit{\'e}s pour l'automatisation de l'inspection visuelle au regard du processus humain}},
  abstract = {Ces travaux de recherche ont pour ambition de contribuer {\`a} l'automatisation de l'inspection visuelle, dans le cadre du contr{\^o}le qualit{\'e} de pi{\`e}ces m{\'e}talliques {\`a} g{\'e}om{\'e}trie complexe. En soi, de nombreuses techniques d'optique, de num{\'e}risation, d'impl{\'e}mentation de rendu photo-r{\'e}aliste, de classification d'images ou de donn{\'e}es, et de reconnaissance de formes sont d{\'e}j{\`a} fortement d{\'e}velopp{\'e}es et appliqu{\'e}es chacune dans des domaines particuliers. Or, elles ne sont pas, ou rarement pour des cas particuliers, combin{\'e}es pour obtenir une m{\'e}thode compl{\`e}te de num{\'e}risation de l'apparence jusqu'{\`a} la reconnaissance, effective et perceptuelle, de l'objet et des anomalies esth{\'e}tiques.Ces travaux ont profit{\'e} des avancements des th{\`e}ses pr{\'e}c{\'e}dentes sur la formalisation du contr{\^o}le qualit{\'e} ainsi que sur un syst{\`e}me agile de num{\'e}risation d'aspect de surface permettant la mise en {\'e}vidence de toute la diversit{\'e} d'anomalies esth{\'e}tiques de surfaces. Ainsi, la contribution majeure r{\'e}side dans l'adaptation des m{\'e}thodes de traitement d'images {\`a} la structure formalis{\'e}e du contr{\^o}le qualit{\'e}, au format riche des donn{\'e}es d'apparence et aux m{\'e}thodes de classification pour r{\'e}aliser la reconnaissance telle que le contr{\^o}leur humain.En ce sens, la th{\`e}se propose un d{\'e}cryptage des diff{\'e}rentes m{\'e}thodologies li{\'e}es au contr{\^o}le qualit{\'e}, au comportement du contr{\^o}leur humain, aux anomalies d'aspect de surface, aux managements et traitements de l'information visuelle, jusqu'{\`a} la combinaison de toutes ces contraintes pour obtenir un syst{\`e}me de substitution partielle au contr{\^o}leur humain. L'objectif de la th{\`e}se, et du d{\'e}cryptage, est d'identifier et de r{\'e}duire les sources de variabilit{\'e} pour obtenir un meilleur contr{\^o}le qualit{\'e}, notamment par l'automatisation intelligente et structur{\'e}e de l'inspection visuelle. A partir d'un dispositif de vision par ordinateur choisi, la solution propos{\'e}e consiste {\`a} analyser la texture visuelle. Celle est consid{\'e}r{\'e}e en tant que signature globale de l'information d'apparence visuelle sup{\'e}rieure {\`a} une unique image contenant des textures images. L'analyse est effectu{\'e}e avec des m{\'e}canismes de reconnaissance de formes et d'apprentissage machine pour {\'e}tablir la d{\'e}tection et l'{\'e}valuation automatiques d'anomalies d'aspect.},
  language = {fr},
  urldate = {2019-07-03},
  url = {https://tel.archives-ouvertes.fr/tel-01254349},
  author = {D{\'e}sage, Simon-Fr{\'e}d{\'e}ric},
  month = nov,
  year = {2015}
}

@phdthesis{pitard_metrologie_2016,
  title = {{M{\'e}trologie et mod{\'e}lisation de l'aspect pour l'inspection qualit{\'e} des surfaces}},
  abstract = {Dans les secteurs industriels, la ma{\^i}trise de l'aspect des surfaces est une probl{\'e}matique majeure de la conception jusqu'{\`a} la r{\'e}alisation des produits. En entreprise, l'{\'e}valuation de la qualit{\'e} des surfaces est g{\'e}n{\'e}ralement r{\'e}alis{\'e}e par des contr{\^o}leurs humains, sauf pour certaines applications sp{\'e}cifiques pour lesquels des syst{\`e}mes ont pu {\^e}tre mis en {\oe}uvre. L'objectif est donc d'aider les fabricants {\`a} mieux {\'e}valuer l'aspect et d'avancer vers l'automatisation du processus d'inspection qualit{\'e} des surfaces.D'un point de vue m{\'e}trologique, la quantification de l'aspect passe par l'acquisition de la fonction de r{\'e}partition du coefficient de luminance (BRDF) qui fournit une cartographie de la lumi{\`e}re r{\'e}fl{\'e}chie {\`a} la surface d'un {\'e}chantillon. Le syst{\`e}me visuel humain extrait de cette mesure des facteurs {\`a} partir desquels il {\'e}labore des attributs de l'aspect : r{\'e}gularit{\'e} d'une texture, uniformit{\'e} de la couleur, qualit{\'e} du brillant, saillance d'une anomalie, etc.En cons{\'e}quence, notre approche consiste {\`a} utiliser les techniques appel{\'e}es Reflectance Transformation Imaging (RTI) originellement issues du domaine arch{\'e}ologique, pour l'industrie. Elles permettent d'obtenir simultan{\'e}ment une estimation r{\'e}duite et simplifi{\'e}e de la BRDF et une estimation des normales {\`a} la surface.Un dispositif d'acquisition RTI appel{\'e} la Sph{\`e}re MeSurA permet d'obtenir des donn{\'e}es st{\'e}r{\'e}ophotom{\'e}triques (luminances). L'approximation de forme des mesures discr{\`e}tes de luminances acquises est fournie selon le principe de la D{\'e}composition Modale Discr{\`e}te (DMD). Une analyse comparative avec les autres mod{\`e}les montre que la DMD d{\'e}crit plus fid{\`e}lement les r{\'e}flexions sp{\'e}culaires, et plus g{\'e}n{\'e}ralement les zones locales de surfaces brillantes.Nous d{\'e}veloppons une m{\'e}thode permettant de mesurer la similarit{\'e} d'aspect en d{\'e}finissant des descripteurs invariants {\`a} la rotation obtenus par un changement de param{\'e}trage de la DMD. Nous calculons ainsi des cartes de distance permettant d'extraire les anomalies les plus saillantes. Nous proposons {\'e}galement leur {\'e}valuation par des attributs pertinents sur lesquels les contr{\^o}leurs pourront se baser pour d{\'e}cider de la conformit{\'e} d'un produit.De plus, nous mettons l'accent sur des indicateurs directionnels de normales et de courbures {\`a} la surface. Ils permettent de s{\'e}parer efficacement les composantes p{\'e}riodiques et non-p{\'e}riodiques de la surface, et ainsi de caract{\'e}riser g{\'e}om{\'e}triquement les anomalies d'aspect d'une part et d'autre part la signature d'un proc{\'e}d{\'e} de fabrication.Les r{\'e}sultats de ces travaux permettent ainsi d'aider les fabricants {\`a} ma{\^i}triser la qualit{\'e} d'aspect en acc{\'e}dant {\`a} diff{\'e}rentes modalit{\'e}s de la surface inspect{\'e}e, dans un logiciel d'application appel{\'e} MsaTool\textregistered.},
  language = {fr},
  urldate = {2016-09-28},
  school = {Universit{\'e} Grenoble Alpes},
  url = {https://tel.archives-ouvertes.fr/tel-01357821/document},
  HAL = {tel-01357821},
  author = {Pitard, Gilles},
  month = may,
  year = {2016}
}

@article{albert_smart_2017,
  series = {Knowledge-{{Based}} and {{Intelligent Information}} \& {{Engineering Systems}}: {{Proceedings}} of the 21st {{International Conference}}, {{KES}}-20176-8 {{September}} 2017, {{Marseille}}, {{France}}},
  title = {A {{Smart System}} to {{Standardize}} the {{Specifications}} of {{Haptic Quality Control}}},
  volume = {112},
  issn = {1877-0509},
  abstract = {The specific attention paid to the quality perceived through the senses of costumers when touching a product has led to a rapid growth in the industrial interest for the field of haptics. Controlling the quality of products with such expectations has become a challenge for manufacturers, especially considering the current lack of a generic method to standardize control specifications and provide efficient control tools, whether a manual or automated control is considered. This study provides a new insight on the definition of control specifications regarding perceived quality control. Smart systems have proven useful and efficient in a number of other domains, but has never been applied in a generic manner to the control of the quality related to the sense of touch. Therefore, a system based on formalized knowledge on haptic perceptions and its relations with quality control is proposed. This paper presents the proposed approach for the standardization of haptic quality control specifications, along with an example of a manufacturing application. The structure of the proposed knowledge model is detailed, as well as the semantic approach that enabled the development of a formalized haptic sensation vocabulary. An experimental method was used to model the influence of exploration on perception, considering the application case.},
  urldate = {2019-07-03},
  journal = {Procedia Computer Science},
  doi = {10/gf4j4k},
  url = {http://www.sciencedirect.com/science/article/pii/S1877050917315120},
  author = {Albert, Bruno and {Zanni-Merk}, Cecilia and {de Beuvron}, Francois de Bertrand and Pillet, Maurice and Maire, Jean-Luc and Knecht, Christophe and Charrier, Julien},
  month = jan,
  year = {2017},
  keywords = {Control specifications,Haptics,Ontology,Quality control,Smart system},
  pages = {723-730}
}

@article{metz_basic_1978,
    title = {Basic principles of {ROC} analysis},
    volume = {8},
    issn = {0001-2998},
    abstract = {The limitations of diagnostic "accuracy" as a measure of decision performance require introduction of the concepts of the "sensitivity" and "specificity" of a diagnostic test. These measures and the related indices, "true positive fraction" and "false positive fraction," are more meaningful than "accuracy," yet do not provide a unique description of diagnostic performance because they depend on the arbitrary selection of a decision threshold. The receiver operating characteristic (ROC) curve is shown to be a simple yet complete empirical description of this decision threshold effect, indicating all possible combinations of the relative frequencies of the various kinds of correct and incorrect decisions. Practical experimental techniques for measuring ROC curves are described, and the issues of case selection and curve-fitting are discussed briefly. Possible generalizations of conventional ROC analysis to account for decision performance in complex diagnostic tasks are indicated. ROC analysis is shown to be related in a direct and natural way to cost/benefit analysis of diagnostic decision making. The concepts of "average diagnostic cost" and "average net benefit" are developed and used to identify the optimal compromise among various kinds of diagnostic error. Finally, the way in which ROC analysis can be employed to optimize diagnostic strategies is suggested.},
    language = {eng},
    number = {4},
    journal = {Seminars in Nuclear Medicine},
    author = {Metz, C. E.},
    month = oct,
    year = {1978},
    pmid = {112681},
    keywords = {Cost-Benefit Analysis, Decision Making, Diagnosis, Diagnosis, Differential, Diagnostic Errors, Evaluation Studies as Topic, False Positive Reactions, Humans, Methods, Models, Theoretical, Radionuclide Imaging, Statistics as Topic},
    pages = {283--298}
}

@article{japkowicz_class_2002,
    title = {The class imbalance problem: {A} systematic study},
    volume = {6},
    issn = {1088-467X},
    shorttitle = {The class imbalance problem},
    url = {https://content.iospress.com/articles/intelligent-data-analysis/ida00103},
    doi = {10.3233/IDA-2002-6504},
    abstract = {In machine learning problems, differences in prior class probabilities -- or class imbalances -- have been reported to hinder the performance of some standard classifiers, such as decision trees. This paper presents a systematic study aimed at answer},
    language = {en},
    number = {5},
    urldate = {2019-06-14},
    journal = {Intelligent Data Analysis},
    author = {Japkowicz, Nathalie and Stephen, Shaju},
    month = jan,
    year = {2002},
    pages = {429--449}
}

@inproceedings{brodersen_balanced_2010,
    title = {The {Balanced} {Accuracy} and {Its} {Posterior} {Distribution}},
    doi = {10.1109/ICPR.2010.764},
    abstract = {Evaluating the performance of a classification algorithm critically requires a measure of the degree to which unseen examples have been identified with their correct class labels. In practice, generalizability is frequently estimated by averaging the accuracies obtained on individual cross-validation folds. This procedure, however, is problematic in two ways. First, it does not allow for the derivation of meaningful confidence intervals. Second, it leads to an optimistic estimate when a biased classifier is tested on an imbalanced dataset. We show that both problems can be overcome by replacing the conventional point estimate of accuracy by an estimate of the posterior distribution of the balanced accuracy.},
    booktitle = {2010 20th {International} {Conference} on {Pattern} {Recognition}},
    author = {Brodersen, K. H. and Ong, C. S. and Stephan, K. E. and Buhmann, J. M.},
    month = aug,
    year = {2010},
    keywords = {Accuracy, Approximation algorithms, balanced accuracy, bias, class imbalance, classification algorithm, classification performance, generalisation (artificial intelligence), generalizability, Inference algorithms, Machine learning, pattern classification, performance evaluation, posterior distribution, Prediction algorithms, Probabilistic logic, statistical distributions, Training},
    pages = {3121--3124}
}

@article{mosley_balanced_2013,
    title = {A balanced approach to the multi-class imbalance problem},
    url = {https://lib.dr.iastate.edu/etd/13537},
    doi = {https://doi.org/10.31274/etd-180810-3375},
    journal = {Graduate Theses and Dissertations},
    author = {Mosley, Lawrence},
    month = jan,
    year = {2013},
}

@inproceedings{kohavi_study_1995,
  address = {{San Francisco, CA, USA}},
  series = {{{IJCAI}}'95},
  title = {A {{Study}} of {{Cross}}-Validation and {{Bootstrap}} for {{Accuracy Estimation}} and {{Model Selection}}},
  isbn = {978-1-55860-363-9},
  abstract = {We review accuracy estimation methods and compare the two most common methods crossvalidation and bootstrap. Recent experimental results on artificial data and theoretical re cults in restricted settings have shown that for selecting a good classifier from a set of classifiers (model selection), ten-fold cross-validation may be better than the more expensive leaveone-out cross-validation. We report on a largescale experiment--over half a million runs of C4.5 and a Naive-Bayes algorithm--to estimate the effects of different parameters on these algrithms on real-world datasets. For crossvalidation we vary the number of folds and whether the folds are stratified or not, for bootstrap, we vary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, The best method to use for model selection is ten fold stratified cross validation even if computation power allows using more folds.},
  urldate = {2017-02-23},
  booktitle = {Proceedings of the 14th {{International Joint Conference}} on {{Artificial Intelligence}} - {{Volume}} 2},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  url = {http://dl.acm.org/citation.cfm?id=1643031.1643047},
  author = {Kohavi, Ron},
  year = {1995},
  pages = {1137--1143}
}

@article{bellet_supervised_2012,
    title = {Supervised {Metric} {Learning} with {Generalization} {Guarantees}},
    url = {https://tel.archives-ouvertes.fr/tel-00770627},
    abstract = {In recent years, the crucial importance of metrics in machine learning algorithms has led to an increasing interest in optimizing distance and similarity functions using knowledge from training data to make them suitable for the problem at hand. This area of research is known as metric learning. Existing methods typically aim at optimizing the parameters of a given metric with respect to some local constraints over the training sample. The learned metrics are generally used in nearest-neighbor and clustering algorithms. When data consist of feature vectors, a large body of work has focused on learning a Mahalanobis distance, which is parameterized by a positive semi-definite matrix. Recent methods offer good scalability to large datasets. Less work has been devoted to metric learning from structured objects (such as strings or trees), because it often involves complex procedures. Most of the work has focused on optimizing a notion of edit distance, which measures (in terms of number of operations) the cost of turning an object into another. We identify two important limitations of current supervised metric learning approaches. First, they allow to improve the performance of local algorithms such as k-nearest neighbors, but metric learning for global algorithms (such as linear classifiers) has not really been studied so far. Second, and perhaps more importantly, the question of the generalization ability of metric learning methods has been largely ignored. In this thesis, we propose theoretical and algorithmic contributions that address these limitations. Our first contribution is the derivation of a new kernel function built from learned edit probabilities. Unlike other string kernels, it is guaranteed to be valid and parameter-free. Our second contribution is a novel framework for learning string and tree edit similarities inspired by the recent theory of (epsilon,gamma,tau)-good similarity functions and formulated as a convex optimization problem. Using uniform stability arguments, we establish theoretical guarantees for the learned similarity that give a bound on the generalization error of a linear classifier built from that similarity. In our third contribution, we extend the same ideas to metric learning from feature vectors by proposing a bilinear similarity learning method that efficiently optimizes the (epsilon,gamma,tau)-goodness. The similarity is learned based on global constraints that are more appropriate to linear classification. Generalization guarantees are derived for our approach, highlighting that our method minimizes a tighter bound on the generalization error of the classifier. Our last contribution is a framework for establishing generalization bounds for a large class of existing metric learning algorithms. It is based on a simple adaptation of the notion of algorithmic robustness and allows the derivation of bounds for various loss functions and regularizers.},
    language = {en},
    urldate = {2019-06-11},
    author = {Bellet, Aurélien},
    month = dec,
    year = {2012}
}

@inproceedings{mahalanobis_generalised_1936,
  title = {On the Generalised Distance in Statistics},
  volume = {2},
  booktitle = {Proceedings {{National Institute}} of {{Science}}, {{India}}},
  url = {http://ir.isical.ac.in/dspace/handle/1/1268},
  author = {Mahalanobis, PC},
  month = apr,
  year = {1936},
  keywords = {Statistics,distance,mahalanobis},
  pages = {49-55}
}

@techreport{fix_discriminatory_1951,
  title = {Discriminatory {{Analysis}} - {{Nonparametric Discrimination}}: {{Consistency Properties}}},
  shorttitle = {Discriminatory {{Analysis}} - {{Nonparametric Discrimination}}},
  abstract = {The discrimination problem (two population case) may be defined as follows: e random variable Z, of observed value z, is distributed over some space (say, p-dimensional) either according to distribution F, or according to distribution G. The problem is to decide, on the basis of z, which of the two distributions Z has.},
  language = {en},
  urldate = {2019-06-25},
  institution = {{CALIFORNIA UNIV BERKELEY}},
  url = {https://apps.dtic.mil/docs/citations/ADA800276},
  author = {Fix, Evelyn and Hodges, Jr},
  month = feb,
  year = {1951}
}

@article{cover_nearest_1967,
  title = {Nearest Neighbor Pattern Classification},
  volume = {13},
  issn = {0018-9448},
  abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of errorRof such a rule must be at least as great as the Bayes probability of errorR\^\textbackslash{}ast\textendash{}the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in theM-category case thatR\^\textbackslash{}ast \textbackslash{}leq R \textbackslash{}leq R\^\textbackslash{}ast(2 \textendash{}MR\^\textbackslash{}ast/(M-1)), where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
  number = {1},
  journal = {IEEE Transactions on Information Theory},
  doi = {10/dn5msp},
  author = {Cover, T. and Hart, P.},
  month = jan,
  year = {1967},
  keywords = {Pattern classification},
  pages = {21-27}
}

@article{breiman_bagging_1996,
  title = {Bagging {{Predictors}}},
  volume = {24},
  issn = {0885-6125, 1573-0565},
  abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
  language = {en},
  number = {2},
  urldate = {2017-06-21},
  journal = {Machine Learning},
  doi = {10.1023/A:1018054314350},
  url = {https://link.springer.com/article/10.1023/A:1018054314350},
  author = {Breiman, Leo},
  month = aug,
  year = {1996},
  pages = {123-140}
}

@inproceedings{ho_random_1995,
  address = {{Washington, DC, USA}},
  series = {{{ICDAR}} '95},
  title = {Random {{Decision Forests}}},
  isbn = {978-0-8186-7128-9},
  abstract = {Decision trees are attractive classifiers due to their high execution speed. But trees derived with traditional methods often cannot be grown to arbitrary complexity for possible loss of generalization accuracy on unseen data. The limitation on complexity usually means suboptimal accuracy on training data. Following the principles of stochastic modeling, we propose a method to construct tree-based classifiers whose capacity can be arbitrarily expanded for increases in accuracy for both training and unseen data. The essence of the method is to build multiple trees in randomly selected subspaces of the feature space. Trees in, different subspaces generalize their classification in complementary ways, and their combined classification can be monotonically improved. The validity of the method is demonstrated through experiments on the recognition of handwritten digits.},
  urldate = {2017-03-29},
  booktitle = {Proceedings of the {{Third International Conference}} on {{Document Analysis}} and {{Recognition}} ({{Volume}} 1) - {{Volume}} 1},
  publisher = {{IEEE Computer Society}},
  url = {http://dl.acm.org/citation.cfm?id=844379.844681},
  author = {Ho, Tin Kam},
  year = {1995},
  keywords = {complexity,optical character recognition,decision theory,decision trees,generalization accuracy,handwriting recognition,handwritten digits,random decision forests,stochastic modeling,suboptimal accuracy,tree-based classifiers},
  pages = {278--}
}

@article{ho_random_1998,
  title = {The Random Subspace Method for Constructing Decision Forests},
  volume = {20},
  issn = {0162-8828},
  abstract = {Much of previous attention on decision trees focuses on the splitting criteria and optimization of tree sizes. The dilemma between overfitting and achieving maximum accuracy is seldom resolved. A method to construct a decision tree based classifier is proposed that maintains highest accuracy on training data and improves on generalization accuracy as it grows in complexity. The classifier consists of multiple trees constructed systematically by pseudorandomly selecting subsets of components of the feature vector, that is, trees constructed in randomly chosen subspaces. The subspace method is compared to single-tree classifiers and other forest construction methods by experiments on publicly available datasets, where the method's superiority is demonstrated. We also discuss independence between trees in a forest and relate that to the combined classification accuracy.},
  number = {8},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi = {10/bn3jhv},
  author = {Ho, Tin Kam},
  month = aug,
  year = {1998},
  keywords = {Binary trees,classification accuracy,Classification tree analysis,Clustering algorithms,decision forests,decision theory,decision tree based classifier,decision trees,Decision trees,feature vector,generalization accuracy,learning (artificial intelligence),maximum accuracy,overfitting,pattern classification,random processes,random subspace method,Stochastic systems,Support vector machine classification,Support vector machines,Tin,Training data,trees (mathematics)},
  pages = {832-844}
}

@article{breiman_random_2001,
  title = {Random {{Forests}}},
  volume = {45},
  issn = {0885-6125, 1573-0565},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\textendash{}156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  language = {en},
  number = {1},
  urldate = {2017-03-29},
  journal = {Machine Learning},
  doi = {10.1023/A:1010933404324},
  url = {https://link.springer.com/article/10.1023/A:1010933404324},
  author = {Breiman, Leo},
  month = oct,
  year = {2001},
  pages = {5-32}
}


@article{kearns_thoughts_1988,
  title = {Thoughts on {{Hypothesis Boosting}}.},
  abstract = {In this paper we present initial and modest progress on the Hypothesis Boosting Problem. Informally, this problem asks whether an efficient learning algorithm (in the distribution-free model of [V84]) that outputs an hypothesis whose performance is only slightly better than random guessing implies the existence of an efficient algorithm that outputs an hypothesis of arbitrary accuracy. The resolution of this question is of theoretical interest and possibly of practical importance. From the theoretical standpoint, we are interested more generally in the question of whether there is a discrete
hierarchy of achievable accuracy in the model of [V84]; from a practical standpoint, the collapse of such a proposed hierarchy may yield an efficient algorithm for converting relatively poor hypotheses into very good hypotheses.},
  language = {English},
  journal = {Non publiée, Machine Learning class project},
  author = {Kearns, Michael},
  month = dec,
  year = {1988},
}

@article{schapire_strength_1990,
  title = {The {{Strength}} of {{Weak Learnability}}},
  volume = {5},
  issn = {0885-6125},
  abstract = {This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class is learnable (or strongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class is weakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent.A method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences, including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error {$\epsilon$}.},
  number = {2},
  urldate = {2017-06-21},
  journal = {Mach. Learn.},
  doi = {10.1023/A:1022648800760},
  url = {http://dx.doi.org/10.1023/A:1022648800760},
  author = {Schapire, Robert E.},
  month = jul,
  year = {1990},
  keywords = {Machine learning,PAC learning,learnability theory,learning from examples,polynomial-time identification},
  pages = {197--227}
}

@techreport{breiman_bias_1996,
  title = {Bias, {{Variance}}, and {{Arcing Classifiers}}},
  abstract = {Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. To study this, the concepts of bias and variance of a classifier are defined. Unstable classifiers can have universally low bias. Their problem is high variance. Combining multiple versions is a variance reducing device. One of the most effective is bagging (Breiman [1996a] ) Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting . Freund and Schapire [1995,1996] propose an algorithm the basis of which is to  adaptively resample and combine (hence the acronym--arcing) so that the weights in the resampling are increased for those cases most often missclassified and the combining is done by weighted voting. Arcing is more successful than bagging in variance reduction. We explore two arcing algorithms, compare them to each other and to baggi...},
  author = {Breiman, Leo},
  year = {1996}
}

@inproceedings{breiman_arcing_1997,
  title = {Arcing the Edge},
  abstract = {Recent work has shown that adaptively reweighting the training set, growing a classifier using the new weights, and combining the classifiers constructed to date can significantly decrease generalization error. Procedures of this type were called arcing by Breiman[1996]. The first successful arcing procedure was introduced by Freund and Schapire[1995,1996] and called Adaboost. In an effort to explain why Adaboost works, Schapire et.al. [1997] derived a bound on the generalization error of a convex combination of classifiers in terms of the margin. We introduce a function called the edge, which differs from the margin only if there are more than two classes. A framework for understanding arcing algorithms is defined. In this framework, we see that the arcing algorithms currently in the literature are optimization algorithms which minimize some function of the edge. A relation is derived between the optimal reduction in the maximum value of the edge and the PAC concept of weak learner. Two algorithms are described which achieve the optimal reduction. Tests on both synthetic and real data cast doubt on the Schapire et.al. explanation.},
  author = {Breiman, Leo},
  year = {1997},
  keywords = {AdaBoost,Approximation algorithm,Atrial Premature Complexes,Generalization (Psychology),Generalization error,Greater Than,Margin (machine learning),Mathematical optimization,Microsoft Edge,Synthetic data,Test set}
}

@article{breiman_arcing_1998,
  title = {Arcing Classifier (with Discussion and a Rejoinder by the Author)},
  volume = {26},
  issn = {0090-5364, 2168-8966},
  abstract = {Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. One of the more effective is bagging. Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire propose an algorithm the basis of which is to adaptively resample and combine (hence the acronym ``arcing'') so that the weights in the resampling are increased for those cases most often misclassified and the combining is done by weighted voting. Arcing is more successful than bagging in test set error reduction. We explore two arcing algorithms, compare them to each other and to bagging, and try to understand how arcing works. We introduce the definitions of bias and variance for a classifier as components of the test set error. Unstable classifiers can have low bias on a large range of data sets. Their problem is high variance. Combining multiple versions either through bagging or arcing reduces variance significantly.},
  language = {en},
  number = {3},
  urldate = {2019-06-29},
  journal = {The Annals of Statistics},
  doi = {10.1214/aos/1024691079},
  url = {https://projecteuclid.org/euclid.aos/1024691079},
  author = {Breiman, Leo},
  month = jun,
  year = {1998},
  keywords = {bagging,boosting,decision trees,Ensemble methods,error-correcting,Markov chain,Monte Carlo,neural networks,output coding},
  pages = {801-849}
}

@inproceedings{mason_boosting_1999,
  address = {{Cambridge, MA, USA}},
  series = {{{NIPS}}'99},
  title = {Boosting {{Algorithms As Gradient Descent}}},
  abstract = {We provide an abstract characterization of boosting algorithms as gradient decsent on cost-functionals in an inner-product function space. We prove convergence of these functional-gradient-descent algorithms under quite weak conditions. Following previous theoretical results bounding the generalization performance of convex combinations of classifiers in terms of general cost functions of the margin, we present a new algorithm (DOOM II) for performing a gradient descent optimization of such cost functions. Experiments on several data sets from the UC Irvine repository demonstrate that DOOM II generally outperforms AdaBoost, especially in high noise situations, and that the overfitting behaviour of AdaBoost is predicted by our cost functions.},
  urldate = {2019-06-29},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Neural Information Processing Systems}}},
  publisher = {{MIT Press}},
  url = {http://dl.acm.org/citation.cfm?id=3009657.3009730},
  author = {Mason, Llew and Baxter, Jonathan and Bartlett, Peter and Frean, Marcus},
  year = {1999},
  pages = {512--518}
}

@article{friedman_greedy_2001,
  title = {Greedy {{Function Approximation}}: {{A Gradient Boosting Machine}}},
  volume = {29},
  issn = {0090-5364, 2168-8966},
  shorttitle = {Greedy Function Approximation},
  abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent ``boosting'' paradigm is developed for additive expansions based on any fitting criterion.Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such ``TreeBoost'' models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
  number = {5},
  urldate = {2017-03-29},
  journal = {The Annals of Statistics},
  doi = {10.1214/aos/1013203451},
  url = {http://projecteuclid.org/euclid.aos/1013203451},
  author = {Friedman, Jerome H.},
  month = oct,
  year = {2001},
  keywords = {boosting,decision trees,Function estimation,robust nonparametric regression},
  pages = {1189-1232}
}

@article{friedman_stochastic_2002,
  series = {Nonlinear {{Methods}} and {{Data Mining}}},
  title = {Stochastic {{Gradient Boosting}}},
  volume = {38},
  issn = {0167-9473},
  abstract = {Gradient boosting constructs additive regression models by sequentially fitting a simple parameterized function (base learner) to current "pseudo'-residuals by least squares at each iteration. The pseudo-residuals are the gradient of the loss functional being minimized, with respect to the model values at each training data point evaluated at the current step. It is shown that both the approximation accuracy and execution speed of gradient boosting can be substantially improved by incorporating randomization into the procedure. Specifically, at each iteration a subsample of the training data is drawn at random (without replacement) from the full training data set. This randomly selected subsample is then used in place of the full sample to fit the base learner and compute the model update for the current iteration. This randomized approach also increases robustness against overcapacity of the base learner.},
  number = {4},
  urldate = {2017-03-29},
  journal = {Computational Statistics \& Data Analysis},
  doi = {10.1016/S0167-9473(01)00065-2},
  url = {http://dx.doi.org/10.1016/S0167-9473(01)00065-2},
  author = {Friedman, Jerome H.},
  month = feb,
  year = {2002},
  pages = {367--378}
}

@article{mcculloch_logical_1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  volume = {5},
  issn = {1522-9602},
  abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  language = {en},
  number = {4},
  urldate = {2019-06-30},
  journal = {The bulletin of mathematical biophysics},
  doi = {10/djsbj6},
  url = {https://doi.org/10.1007/BF02478259},
  author = {McCulloch, Warren S. and Pitts, Walter},
  month = dec,
  year = {1943},
  keywords = {Excitatory Synapse,Inhibitory Synapse,Nervous Activity,Spatial Summation,Temporal Summation},
  pages = {115-133},
}

@article{rosenblatt_perceptron_1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain},
  volume = {65},
  issn = {1939-1471(Electronic),0033-295X(Print)},
  shorttitle = {The Perceptron},
  abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  number = {6},
  journal = {Psychological Review},
  doi = {10/fg6wr5},
  author = {Rosenblatt, F.},
  year = {1958},
  keywords = {Brain,Cognition,Memory,Nervous System},
  pages = {386-408}
}

@article{linnainmaa_taylor_1976,
  title = {Taylor Expansion of the Accumulated Rounding Error},
  volume = {16},
  issn = {1572-9125},
  abstract = {The article describes analytic and algorithmic methods for determining the coefficients of the Taylor expansion of an accumulated rounding error with respect to the local rounding errors, and hence determining the influence of the local errors on the accumulated error. Second and higher order coefficients are also discussed, and some possible methods of reducing the extensive storage requirements are analyzed.},
  language = {en},
  number = {2},
  urldate = {2019-06-30},
  journal = {BIT Numerical Mathematics},
  doi = {10/cv4d63},
  url = {https://doi.org/10.1007/BF01931367},
  author = {Linnainmaa, Seppo},
  month = jun,
  year = {1976},
  keywords = {Computational Mathematic,Local Error,Rounding Error,Storage Requirement,Taylor Expansion},
  pages = {146-160}
}

@article{fukushima_neocognitron_1980,
  title = {Neocognitron: {{A}} Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  volume = {36},
  issn = {1432-0770},
  shorttitle = {Neocognitron},
  abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by ``learning without a teacher'', and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname ``neocognitron''. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of ``S-cells'', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of ``C-cells'' similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any ``teacher'' during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
  language = {en},
  number = {4},
  urldate = {2019-06-30},
  journal = {Biological Cybernetics},
  doi = {10/fhrjjv},
  url = {https://doi.org/10.1007/BF00344251},
  author = {Fukushima, Kunihiko},
  month = apr,
  year = {1980},
  keywords = {Complex Cell,Digital Computer,Input Layer,Neural Network Model,Pattern Recognition},
  pages = {193-202},
}

@article{vapnik_patterns_1963,
  title = {{Patterns recognition using Generalized Portraits method}},
  volume = {24},
  abstract = {An axiomatic definition of pattern is given. The concepts of "generalized portrait", "distinction", and "recognition" are introduced. Algorithms are proposed for learning recognition and distinction on the basis of finding generalized portraits of patterns.
Translated from Avtomatika i Telemekhanika, Vol. 24, No. 6.},
  language = {Russian},
  number = {6},
  urldate = {2019-06-26},
  journal = {Automation and Remote Control},
  url = {https://ci.nii.ac.jp/naid/10020952249/},
  author = {Vapnik, Vladimir Naumovitch and Lerner, Alexander Yakovlevich},
  year = {1963},
  pages = {774-780}
}

@article{aizerman_theoretical_1964,
  title = {Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning},
  volume = {25},
  abstract = {Algorithms for teaching automata to recognize classes of input situations based on the construction of so-called potential functions are proposed. The main hypothesis about the nature of the functions separating the sets corresponding to different classes of input situations is introduced into consideration. Proceeding from this hypothesis, theorems on the convergence of algorithms are proved in a finite number of steps. It is shown that the proposed algorithms are implemented by a wide class of schemes. The characteristics of the elements from which the schemes are assembled are practically arbitrary.
It is shown that Rosenblat's perceptron belongs to this class of schemes, that is, it is proved that the work of the perceptron can be understood as the implementation of the method of potential functions. In this connection, the proven theorems on the convergence of algorithms for potential functions also solve the problem of convergence of a process in a perceptron.},
  number = {6},
  journal = {Automation and Remote Control},
  url = {http://mi.mathnet.ru/at11677},
  author = {A{\u i}zerman, Mark Aronovich and Braverman, {\'E}.M. and Rozono{\'e}r, L. I.},
  month = feb,
  year = {1964},
  keywords = {Foundations,Pattern Recognition},
  pages = {821-837},
}

@inproceedings{boser_training_1992,
  address = {{New York, NY, USA}},
  series = {{{COLT}} '92},
  title = {A {{Training Algorithm}} for {{Optimal Margin Classifiers}}},
  isbn = {978-0-89791-497-0},
  abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
  urldate = {2019-06-26},
  booktitle = {Proceedings of the {{Fifth Annual Workshop}} on {{Computational Learning Theory}}},
  publisher = {{ACM}},
  doi = {10/bwgqr8},
  url = {http://doi.acm.org/10.1145/130385.130401},
  author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
  year = {1992},
  pages = {144--152},
}

@article{cortes_supportvector_1995,
  title = {Support-Vector Networks},
  volume = {20},
  issn = {1573-0565},
  abstract = {Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
  language = {en},
  number = {3},
  urldate = {2019-06-26},
  journal = {Machine Learning},
  doi = {10/cv7fn6},
  url = {https://doi.org/10.1007/BF00994018},
  author = {Cortes, Corinna and Vapnik, Vladimir},
  month = sep,
  year = {1995},
  keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
  pages = {273-297}
}

@incollection{vapnik_support_1997,
  title = {Support {{Vector Method}} for {{Function Approximation}}, {{Regression Estimation}} and {{Signal Processing}}},
  urldate = {2017-03-29},
  booktitle = {Advances in {{Neural Information Processing Systems}} 9},
  publisher = {{MIT Press}},
  url = {http://papers.nips.cc/paper/1187-support-vector-method-for-function-approximation-regression-estimation-and-signal-processing.pdf},
  author = {Vapnik, Vladimir and Golowich, Steven E. and Smola, Alex J.},
  editor = {Mozer, M. C. and Jordan, M. I. and Petsche, T.},
  year = {1997},
  pages = {281--287}
}

@article{chang_libsvm_2011,
  title = {{{LIBSVM}}: {{A Library}} for {{Support Vector Machines}}},
  volume = {2},
  issn = {2157-6904},
  shorttitle = {{{LIBSVM}}},
  abstract = {LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.},
  number = {3},
  urldate = {2017-03-29},
  journal = {ACM Trans. Intell. Syst. Technol.},
  doi = {10.1145/1961189.1961199},
  url = {http://doi.acm.org/10.1145/1961189.1961199},
  author = {Chang, Chih-Chung and Lin, Chih-Jen},
  month = may,
  year = {2011},
  keywords = {classification,Optimization,regression,SVM,LIBSVM,support,vector,machines},
  pages = {27:1--27:27}
}

@article{fan_liblinear_2008,
  title = {{{LIBLINEAR}}: {{A Library}} for {{Large Linear Classification}}},
  volume = {9},
  issn = {1532-4435},
  shorttitle = {{{LIBLINEAR}}},
  abstract = {LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.},
  urldate = {2019-06-26},
  journal = {J. Mach. Learn. Res.},
  url = {http://dl.acm.org/citation.cfm?id=1390681.1442794},
  author = {Fan, Rong-En and Chang, Kai-Wei and Hsieh, Cho-Jui and Wang, Xiang-Rui and Lin, Chih-Jen},
  month = jun,
  year = {2008},
  pages = {1871--1874}
}

@techreport{rumelhart_learning_1985,
    title = {Learning {Internal} {Representations} by {Error} {Propagation}},
    copyright = {Approved for public release; distribution is unlimited.},
    language = {en},
    author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
    month = sep,
    year = {1985},
    keywords = {Psychology, Electrical and Electronic Equipment, *ADAPTIVE SYSTEMS, *NETWORKS, *LEARNING, ERRORS, DESCENT, PERCEPTION, GRADIENTS, INTERNAL, LEARNING MACHINES, PROPAGATION, WEIGHT, WUNR667548}
}

@article{lecun_backpropagation_1989,
    title = {Backpropagation {Applied} to {Handwritten} {Zip} {Code} {Recognition}},
    volume = {1},
    issn = {0899-7667},
    url = {http://dx.doi.org/10.1162/neco.1989.1.4.541},
    doi = {10.1162/neco.1989.1.4.541},
    abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
    number = {4},
    urldate = {2019-06-11},
    journal = {Neural Comput.},
    author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
    month = dec,
    year = {1989},
    pages = {541--551}
}

@inproceedings{lecun_efficient_1998,
    address = {London, UK, UK},
    title = {Efficient {BackProp}},
    isbn = {978-3-540-65311-0},
    url = {http://dl.acm.org/citation.cfm?id=645754.668382},
    urldate = {2017-05-12},
    booktitle = {Neural {Networks}: {Tricks} of the {Trade}, {This} {Book} is an {Outgrowth} of a 1996 {NIPS} {Workshop}},
    publisher = {Springer-Verlag},
    author = {LeCun, Yann and Bottou, Léon and Orr, Genevieve B. and Müller, Klaus-Robert},
    year = {1998},
}

@techreport{sadowski_notes_2016,
    address = {Irvine, CA 92697},
    title = {Notes on backpropagation},
    url = {. https://www.ics.uci.edu/~pjsadows/notes.pdf},
    abstract = {This document derives backpropagation for some common neural networks.},
    language = {English},
    urldate = {2019-01-01},
    institution = {University of California},
    author = {Sadowski, Peter},
    year = {2016},
    pages = {1--4}
}

@incollection{vapnik_principles_1992,
  title = {Principles of {{Risk Minimization}} for {{Learning Theory}}},
  urldate = {2019-06-25},
  booktitle = {Advances in {{Neural Information Processing Systems}} 4},
  publisher = {{Morgan-Kaufmann}},
  url = {http://papers.nips.cc/paper/506-principles-of-risk-minimization-for-learning-theory.pdf},
  author = {Vapnik, V.},
  editor = {Moody, J. E. and Hanson, S. J. and Lippmann, R. P.},
  year = {1992},
  pages = {831--838}
}

@article{shannon_mathematical_1948,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  volume = {27},
  issn = {0005-8580},
  abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist1 and Hartley2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.},
  number = {3},
  journal = {The Bell System Technical Journal},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  author = {Shannon, C. E.},
  month = jul,
  year = {1948},
  pages = {379-423}
}

@article{kullback_information_1951,
  title = {On {{Information}} and {{Sufficiency}}},
  volume = {22},
  issn = {0003-4851, 2168-8990},
  abstract = {Project Euclid - mathematics and statistics online},
  language = {EN},
  number = {1},
  urldate = {2017-11-17},
  journal = {The Annals of Mathematical Statistics},
  doi = {10.1214/aoms/1177729694},
  url = {https://projecteuclid.org/euclid.aoms/1177729694},
  author = {Kullback, S. and Leibler, R. A.},
  month = mar,
  year = {1951},
  pages = {79-86}
}

@article{bengio_learning_1994,
    title = {Learning long-term dependencies with gradient descent is difficult},
    volume = {5},
    issn = {1045-9227},
    doi = {10.1109/72.279181},
    abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.{\textless}{\textgreater}},
    number = {2},
    journal = {IEEE Transactions on Neural Networks},
    author = {Bengio, Y. and Simard, P. and Frasconi, P.},
    month = mar,
    year = {1994},
    keywords = {Computer networks, Cost function, Delay effects, Discrete transforms, Displays, efficient learning, gradient descent, input/output sequence mapping, Intelligent networks, learning (artificial intelligence), long-term dependencies, Neural networks, Neurofeedback, numerical analysis, prediction problems, Production, production problems, recognition, recurrent neural nets, recurrent neural network training, Recurrent neural networks, temporal contingencies},
    pages = {157--166}
}

@book{hochreiter_gradient_2001,
    title = {Gradient {Flow} in {Recurrent} {Nets}: the {Difficulty} of {Learning} {Long}-{Term} {Dependencies}},
    shorttitle = {Gradient {Flow} in {Recurrent} {Nets}},
    abstract = {Recurrent networks (crossreference Chapter 12) can, in principle, use their feedback connections to store representations of recent input events in the form of activations. The most widely used algorithms for learning what to put in short-term memory, however, take too much time to be feasible or do not work well at all, especially when minimal time lags between inputs and corresponding teacher signals are long. Although theoretically fascinating, they do not provide clear practical advantages over, say, backprop in feedforward networks with limited time windows (see crossreference Chapters 11 and 12). With conventional "algorithms based on the computation of the complete gradient", such as "Back-Propagation Through Time" (BPTT, e.g., [22, 27, 26]) or "Real-Time Recurrent Learning" (RTRL, e.g., [21]) error signals "flowing backwards in time" tend to either (1) blow up or (2) vanish: the temporal evolution of the backpropagated error ex},
    author = {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, Jürgen},
    year = {2001}
}

@article{kingma_adam_2014,
    title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
    shorttitle = {Adam},
    url = {http://arxiv.org/abs/1412.6980},
    archivePrefix = {arXiv},
    eprinttype = {arxiv},
    eprint = {1412.6980},
    abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
    urldate = {2017-03-30},
    journal = {arXiv:1412.6980 [cs]},
    author = {Kingma, Diederik P. and Ba, Jimmy},
    month = dec,
    year = {2014}
}

@article{dumoulin_guide_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.07285},
  primaryClass = {cs, stat},
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
  urldate = {2019-08-06},
  journal = {arXiv:1603.07285 [cs, stat]},
  url = {http://arxiv.org/abs/1603.07285},
  author = {Dumoulin, Vincent and Visin, Francesco},
  month = mar,
  year = {2016},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@book{goodfellow_deep_2016,
  address = {{Cambridge, Massachusetts London, England}},
  series = {Adaptive Computation and Machine Learning},
  title = {Deep Learning},
  isbn = {978-0-262-03561-3},
  abstract = {Applied math and machine learning basics. Linear algebra -- Probability and information theory -- Numerical computation -- Machine learning basics -- Deep networks: modern practices. Deep feedforward networks -- Regularization for deep learning -- Optimization for training deep models -- Convolutional networks -- Sequence modeling: recurrent and recursive nets -- Practical methodology -- Applications -- Deep learning research. Linear factor models -- Autoencoders -- Representation learning -- Structured probabilistic models for deep learning -- Monte Carlo methods -- Confronting the partition function -- Approximate inference -- Deep generative models},
  language = {eng},
  publisher = {{The MIT Press}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016}
}

@article{srivastava_dropout_2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  volume = {15},
  shorttitle = {Dropout},
  urldate = {2019-06-27},
  journal = {Journal of Machine Learning Research},
  url = {http://jmlr.org/papers/v15/srivastava14a.html},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  pages = {1929-1958}
}

@article{ioffe_batch_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.03167},
  primaryClass = {cs},
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  urldate = {2019-06-27},
  journal = {arXiv:1502.03167 [cs]},
  url = {http://arxiv.org/abs/1502.03167},
  author = {Ioffe, Sergey and Szegedy, Christian},
  month = feb,
  year = {2015},
  keywords = {Computer Science - Machine Learning}
}

@article{yao_early_2007,
  title = {On {{Early Stopping}} in {{Gradient Descent Learning}}},
  volume = {26},
  issn = {1432-0940},
  abstract = {In this paper we study a family of gradient descent algorithms to approximate the regression function from reproducing kernel Hilbert spaces (RKHSs), the family being characterized by a polynomial decreasing rate of step sizes (or learning rate). By solving a bias-variance trade-off we obtain an early stopping rule and some probabilistic upper bounds for the convergence of the algorithms. We also discuss the implication of these results in the context of classification where some fast convergence rates can be achieved for plug-in classifiers. Some connections are addressed with Boosting, Landweber iterations, and the online learning algorithms as stochastic approximations of the gradient descent method.},
  language = {en},
  number = {2},
  urldate = {2019-08-30},
  journal = {Constructive Approximation},
  doi = {10/d3qqdj},
  url = {https://doi.org/10.1007/s00365-006-0663-2},
  author = {Yao, Yuan and Rosasco, Lorenzo and Caponnetto, Andrea},
  month = aug,
  year = {2007},
  keywords = {Convergence Rate,Gradient Descent,Gradient Descent Method,Reproduce Kernel Hilbert Space,Tikhonov Regularization},
  pages = {289-315}
}

@inproceedings{boureau_theoretical_2010,
  address = {{USA}},
  series = {{{ICML}}'10},
  title = {A {{Theoretical Analysis}} of {{Feature Pooling}} in {{Visual Recognition}}},
  isbn = {978-1-60558-907-7},
  abstract = {Many modern visual recognition algorithms incorporate a step of spatial 'pooling', where the outputs of several nearby feature detectors are combined into a local or global 'bag of features', in a way that preserves task-related information while removing irrelevant details. Pooling is used to achieve invariance to image transformations, more compact representations, and better robustness to noise and clutter. Several papers have shown that the details of the pooling operation can greatly influence the performance, but studies have so far been purely empirical. In this paper, we show that the reasons underlying the performance of various pooling methods are obscured by several confounding factors, such as the link between the sample cardinality in a spatial pool and the resolution at which low-level features have been extracted. We provide a detailed theoretical analysis of max pooling and average pooling, and give extensive empirical comparisons for object recognition tasks.},
  urldate = {2019-06-30},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  publisher = {{Omnipress}},
  url = {http://dl.acm.org/citation.cfm?id=3104322.3104338},
  author = {Boureau, Y-Lan and Ponce, Jean and LeCun, Yann},
  year = {2010},
  pages = {111--118}
}

@inproceedings{scherer_evaluation_2010,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Evaluation of {{Pooling Operations}} in {{Convolutional Architectures}} for {{Object Recognition}}},
  isbn = {978-3-642-15825-4},
  abstract = {A common practice to gain invariant features in object recognition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms subsampling operations. Despite their shift-invariant properties, overlapping pooling windows are no significant improvement over non-overlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57\% on the NORB normalized-uniform dataset and 5.6\% on the NORB jittered-cluttered dataset.},
  language = {en},
  booktitle = {Artificial {{Neural Networks}} \textendash{} {{ICANN}} 2010},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Scherer, Dominik and M{\"u}ller, Andreas and Behnke, Sven},
  editor = {Diamantaras, Konstantinos and Duch, Wlodek and Iliadis, Lazaros S.},
  year = {2010},
  keywords = {Convolutional Layer,Convolutional Neural Network,Recognition Rate,Test Error Rate,Window Function},
  pages = {92-101}
}

@article{lin_network_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.4400},
  primaryClass = {cs},
  title = {Network {{In Network}}},
  abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
  urldate = {2019-06-30},
  journal = {arXiv:1312.4400 [cs]},
  url = {http://arxiv.org/abs/1312.4400},
  author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  month = dec,
  year = {2013},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{oquab_object_2015,
  title = {Is {{Object Localization}} for {{Free}}? - {{Weakly}}-{{Supervised Learning With Convolutional Neural Networks}}},
  shorttitle = {Is {{Object Localization}} for {{Free}}?},
  urldate = {2019-06-30},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  url = {http://openaccess.thecvf.com/content_cvpr_2015/html/Oquab_Is_Object_Localization_2015_CVPR_paper.html},
  author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
  year = {2015},
  pages = {685-694}
}

@article{zhou_learning_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.04150},
  primaryClass = {cs},
  title = {Learning {{Deep Features}} for {{Discriminative Localization}}},
  abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2\% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them},
  urldate = {2019-06-30},
  journal = {arXiv:1512.04150 [cs]},
  url = {http://arxiv.org/abs/1512.04150},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  month = dec,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{krogh_simple_1991,
  address = {{San Francisco, CA, USA}},
  series = {{{NIPS}}'91},
  title = {A {{Simple Weight Decay Can Improve Generalization}}},
  isbn = {978-1-55860-222-9},
  abstract = {It has been observed in numerical simulations that a weight decay can improve generalization in a feed-forward neural network. This paper explains why. It is proven that a weight decay has two effects in a linear network. First, it suppresses any irrelevant components of the weight vector by choosing the smallest vector that solves the learning problem. Second, if the size is chosen right, a weight decay can suppress some of the effects of static noise on the targets, which improves generalization quite a lot. It is then shown how to extend these results to networks with hidden layers and non-linear units. Finally the theory is confirmed by some numerical simulations using the data from NetTalk.},
  urldate = {2019-06-30},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Neural Information Processing Systems}}},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  url = {http://dl.acm.org/citation.cfm?id=2986916.2987033},
  author = {Krogh, Anders and Hertz, John A.},
  year = {1991},
  pages = {950--957}
}

@article{tikhonov_stability_1943,
  title = {{On the stability of inverse problems}},
  volume = {39},
  language = {Russian},
  number = {5},
  urldate = {2019-08-20},
  journal = {Doklady Akademii Nauk SSSR},
  url = {https://www.scienceopen.com/document?vid=66e2e93e-7735-4f0a-ab89-a3c0dd8a5e02},
  author = {Tikhonov, Andrey Nikolayevich},
  year = {1943},
  pages = {195--198}
}

@book{tikhonov_solutions_1977,
  address = {{Washington : New York}},
  series = {Scripta Series in Mathematics},
  title = {Solutions of Ill-Posed Problems},
  isbn = {978-0-470-99124-4},
  lccn = {QA297 .T5413},
  language = {eng},
  publisher = {{Winston ; distributed solely by Halsted Press}},
  author = {Tikhonov, Andrey Nikolayevich and Arsenin, V.},
  year = {1977},
  keywords = {Improperly posed problems,Numerical analysis}
}

@article{chellapilla_high_2006,
    title = {High {Performance} {Convolutional} {Neural} {Networks} for {Document} {Processing}},
    url = {https://hal.inria.fr/inria-00112631},
    abstract = {Convolutional neural networks (CNNs) are well known for producing state-of-the-art recognizers for document processing [1]. However, they can be difficult to implement and are usually slower than traditional multi-layer perceptrons (MLPs). We present three novel approaches to speeding up CNNs: a) unrolling convolution, b) using BLAS (basic linear algebra subroutines), and c) using GPUs (graphic processing units). Unrolled convolution converts the processing in each convolutional layer (both forward-propagation and back-propagation) into a matrix-matrix product. The matrix-matrix product representation of CNNs makes their implementation as easy as MLPs. BLAS is used to efficiently compute matrix products on the CPU. We also present a pixel shader based GPU implementation of CNNs. Results on character recognition problems indicate that unrolled convolution with BLAS produces a dramatic 2.4X−3.0X speedup. The GPU implementation is even faster and produces a 3.1X−4.1X speedup.},
    language = {en},
    urldate = {2019-06-24},
    author = {Chellapilla, Kumar and Puri, Sidd and Simard, Patrice},
    month = oct,
    year = {2006}
}

@inproceedings{ciresan_flexible_2011,
    series = {{IJCAI}'11},
    title = {Flexible, {High} {Performance} {Convolutional} {Neural} {Networks} for {Image} {Classification}},
    isbn = {978-1-57735-514-4},
    url = {http://dx.doi.org/10.5591/978-1-57735-516-8/IJCAI11-210},
    doi = {10.5591/978-1-57735-516-8/IJCAI11-210},
    abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53\%, 19.51\%, 0.35\%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42\%, 0.97\% and 0.48\% after 1, 3 and 17 epochs, respectively.},
    urldate = {2019-06-24},
    booktitle = {Proceedings of the {Twenty}-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence} - {Volume} {Volume} {Two}},
    publisher = {AAAI Press},
    author = {Cireşan, Dan C. and Meier, Ueli and Masci, Jonathan and Gambardella, Luca M. and Schmidhuber, Jürgen},
    year = {2011},
    note = {event-place: Barcelona, Catalonia, Spain},
    pages = {1237--1242}
}

@inproceedings{krizhevsky_imagenet_2012,
    address = {USA},
    series = {{NIPS}'12},
    title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
    url = {http://dl.acm.org/citation.cfm?id=2999134.2999257},
    abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
    urldate = {2019-06-24},
    booktitle = {Proceedings of the 25th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 1},
    publisher = {Curran Associates Inc.},
    author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
    year = {2012},
    note = {event-place: Lake Tahoe, Nevada},
    pages = {1097--1105}
}

@article{zeiler_visualizing_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1311.2901},
  primaryClass = {cs},
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \textbackslash{}etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  urldate = {2017-03-22},
  journal = {arXiv:1311.2901 [cs]},
  url = {http://arxiv.org/abs/1311.2901},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  month = nov,
  year = {2013},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{simonyan_very_2014,
    title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
    url = {http://arxiv.org/abs/1409.1556},
    archivePrefix = {arXiv},
    eprinttype = {arxiv},
    eprint = {1409.1556},
    abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
    urldate = {2017-03-24},
    journal = {arXiv:1409.1556 [cs]},
    author = {Simonyan, Karen and Zisserman, Andrew},
    month = sep,
    year = {2014},
    keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{he_deep_2015,
    title = {Deep {Residual} {Learning} for {Image} {Recognition}},
    url = {http://arxiv.org/abs/1512.03385},
    archivePrefix = {arXiv},
    eprinttype = {arxiv},
    eprint = {1512.03385},
    abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
    urldate = {2019-06-24},
    journal = {arXiv:1512.03385 [cs]},
    author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    month = dec,
    year = {2015},
    keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{deng_imagenet_2009,
    title = {{ImageNet}: {A} large-scale hierarchical image database},
    shorttitle = {{ImageNet}},
    doi = {10.1109/CVPR.2009.5206848},
    abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
    booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
    author = {Deng, J. and Dong, W. and Socher, R. and Li, L. and Li, Kai and Fei-Fei, Li},
    month = jun,
    year = {2009},
    keywords = {Computer vision, Internet, robustness, Explosions, Image resolution, Large-scale systems, image retrieval, multimedia computing, ontologies (artificial intelligence), trees (mathematics), very large databases, visual databases, ImageNet database, large-scale hierarchical image database, multimedia data, large-scale ontology, wordNet structure, subtree, Image databases, Information retrieval, Multimedia databases, Ontologies, Spine},
    pages = {248--255},
}

@article{srivastava_highway_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.00387},
  primaryClass = {cs},
  title = {Highway {{Networks}}},
  abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
  urldate = {2019-06-30},
  journal = {arXiv:1505.00387 [cs]},
  url = {http://arxiv.org/abs/1505.00387},
  author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"u}rgen},
  month = may,
  year = {2015},
  keywords = {Computer Science - Neural and Evolutionary Computing,68T01,I.2.6,G.1.6,Computer Science - Machine Learning}
}

@article{srivastava_training_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.06228},
  primaryClass = {cs},
  title = {Training {{Very Deep Networks}}},
  abstract = {Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.},
  urldate = {2017-03-24},
  journal = {arXiv:1507.06228 [cs]},
  url = {http://arxiv.org/abs/1507.06228},
  author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"u}rgen},
  month = jul,
  year = {2015},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,68T01,I.2.6,G.1.6}
}

@misc{harisiqbal_harisiqbal88_2018,
  title = {{{HarisIqbal88}}/{{PlotNeuralNet}} v1.0.0},
  abstract = {This is the first release that is built on the idea to further extend the functionality with maximum backward compatibility. Layer names(e.g., Box,RightBandedBox) and their attributes(e.g., caption,xlabels) are generic. Moreover, the code is much better readable than the previous version. TO DO's: Add easy legend functionality Add more layer shapes like TruncatedPyramid, 2DSheet etc},
  urldate = {2019-07-02},
  howpublished = {Zenodo},
  url = {https://zenodo.org/record/2526396},
  author = {Haris Iqbal},
  month = dec,
  year = {2018},
  doi = {10.5281/zenodo.2526396},
}

@article{he_identity_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.05027},
  primaryClass = {cs},
  title = {Identity {{Mappings}} in {{Deep Residual Networks}}},
  abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
  urldate = {2019-06-30},
  journal = {arXiv:1603.05027 [cs]},
  url = {http://arxiv.org/abs/1603.05027},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month = mar,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
}

@article{xie_aggregated_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.05431},
  primaryClass = {cs},
  title = {Aggregated {{Residual Transformations}} for {{Deep Neural Networks}}},
  abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call "cardinality" (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.},
  urldate = {2019-06-30},
  journal = {arXiv:1611.05431 [cs]},
  url = {http://arxiv.org/abs/1611.05431},
  author = {Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{mahajan_exploring_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.00932},
  primaryClass = {cs},
  title = {Exploring the {{Limits}} of {{Weakly Supervised Pretraining}}},
  abstract = {State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards "small". Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4\% (97.6\% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.},
  urldate = {2019-06-24},
  journal = {arXiv:1805.00932 [cs]},
  url = {http://arxiv.org/abs/1805.00932},
  author = {Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and {van der Maaten}, Laurens},
  month = may,
  year = {2018},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{howard_mobilenets_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.04861},
  primaryClass = {cs},
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  journal = {arXiv:1704.04861 [cs]},
  url = {http://arxiv.org/abs/1704.04861},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  month = apr,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{sandler_mobilenetv2_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1801.04381},
  primaryClass = {cs},
  title = {{{MobileNetV2}}: {{Inverted Residuals}} and {{Linear Bottlenecks}}},
  shorttitle = {{{MobileNetV2}}},
  abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
  journal = {arXiv:1801.04381 [cs]},
  url = {http://arxiv.org/abs/1801.04381},
  author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  month = jan,
  year = {2018},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {eprinttype: arxiv
eprintclass: cs.CV
eprint: http://arxiv.org/abs/1801.04381}
}

@article{chollet_xception_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.02357},
  primaryClass = {cs},
  title = {Xception: {{Deep Learning}} with {{Depthwise Separable Convolutions}}},
  shorttitle = {Xception},
  abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the \textbackslash{}textit\{depthwise separable convolution\} operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameter as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
  urldate = {2017-03-24},
  journal = {arXiv:1610.02357 [cs]},
  url = {http://arxiv.org/abs/1610.02357},
  author = {Chollet, Fran{\c c}ois},
  month = oct,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{nair_rectified_2010,
  address = {{USA}},
  series = {{{ICML}}'10},
  title = {Rectified {{Linear Units Improve Restricted Boltzmann Machines}}},
  isbn = {978-1-60558-907-7},
  abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
  urldate = {2019-07-01},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{International Conference}} on {{Machine Learning}}},
  publisher = {{Omnipress}},
  url = {http://dl.acm.org/citation.cfm?id=3104322.3104425},
  author = {Nair, Vinod and Hinton, Geoffrey E.},
  year = {2010},
  pages = {807--814}
}

@article{maas_rectifier_2013,
  title = {Rectifier {{Nonlinearities Improve Neural Network Acoustic Models}}},
  journal = {Proceddings ICML},
  url = {http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf},
  author = {Maas, Andrew and Hannun, Awni and Ng, Andrew},
  month = jun,
  year = {2013},
  keywords = {activation_functions,andrew_ng,deep_learning_architectures,deep_learning_theory,networks,neural}
}

@article{he_delving_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.01852},
  primaryClass = {cs},
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human}}-{{Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
  urldate = {2017-03-24},
  journal = {arXiv:1502.01852 [cs]},
  url = {http://arxiv.org/abs/1502.01852},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month = feb,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Artificial Intelligence}
}

@article{clevert_fast_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.07289},
  primaryClass = {cs},
  title = {Fast and {{Accurate Deep Network Learning}} by {{Exponential Linear Units}} ({{ELUs}})},
  abstract = {We introduce the "exponential linear unit" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10\% classification error for a single crop, single model network.},
  urldate = {2019-07-01},
  journal = {arXiv:1511.07289 [cs]},
  url = {http://arxiv.org/abs/1511.07289},
  author = {Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
  month = nov,
  year = {2015},
  keywords = {Computer Science - Machine Learning},
}

@book{chapelle_semi-supervised_2010,
    address = {Cambridge, Mass. [u.a]},
    series = {Adaptive computation and machine learning series},
    title = {Semi-supervised learning},
    isbn = {978-0-262-03358-9},
    language = {eng},
    publisher = {MIT Press},
    editor = {Chapelle, Olivier and Schölkopf, Bernhard and Zien, Alexander},
    year = {2010},
    note = {OCLC: 845655904}
}

@phdthesis{durand_weakly_2017,
    title = {Weakly supervised learning for visual recognition},
    url = {https://tel.archives-ouvertes.fr/tel-01635374},
    abstract = {This thesis studies the problem of classification of images, where the goal is to predict if a semantic category is present in the image, based on its visual content. To analyze complex scenes, it is important to learn localized representations. To limit the cost of annotation during training, we have focused on weakly supervised learning approaches. In this thesis, we propose several models that simultaneously classify and localize objects, using only global labels during training. The weak supervision significantly reduces the cost of full annotation, but it makes learning more challenging. The key issue is how to aggregate local scores - e.g. regions - into global score - e.g. image. The main contribution of this thesis is the design of new pooling functions for weakly supervised learning. In particular, we propose a “max + min” pooling function, which unifies many pooling functions. We describe how to use this pooling in the Latent Structured SVM framework as well as in convolutional networks. To solve the optimization problems, we present several solvers, some of which allow to optimize a ranking metric such as Average Precision. We experimentally show the interest of our models with respect to state-of-the-art methods, on ten standard image classification datasets, including the large-scale dataset ImageNet.},
    language = {en},
    urldate = {2019-06-10},
    school = {Universit{\'e} Pierre et Marie Curie},
    author = {Durand, Thibaut},
    month = sep,
    year = {2017}
}

@article{pedregosa_scikit-learn_2011,
    title = {Scikit-learn: {Machine} {Learning} in {Python}},
    volume = {12},
    issn = {1532-4435},
    shorttitle = {Scikit-learn},
    url = {http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html},
    abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
    urldate = {2017-03-29},
    journal = {Journal of Machine Learning Research},
    author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
    month = nov,
    year = {2011}
}

@techreport{zhu_learning_2002,
    title = {Learning from {Labeled} and {Unlabeled} {Data} with {Label} {Propagation}},
    abstract = {We investigate the use of unlabeled data to help labeled data in classification. We propose a simple iterative algorithm, label propagation, to propagate labels through the dataset along high density areas defined by unlabeled data. We give the analysis of the algorithm, show its solution, and its connection to several other algorithms. We also show how to learn parameters by minimum spanning tree heuristic and entropy minimization, and the algorithm's ability to do feature selection. Experiment results are promising.},
    author = {Zhu, Xiaojin and Ghahramani, Zoubin},
    year = {2002}
}

@inproceedings{zhou_learning_2003,
    address = {Cambridge, MA, USA},
    series = {{NIPS}'03},
    title = {Learning with {Local} and {Global} {Consistency}},
    url = {http://dl.acm.org/citation.cfm?id=2981345.2981386},
    abstract = {We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufficiently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classification problems and demonstrates effective use of unlabeled data.},
    urldate = {2019-06-24},
    booktitle = {Proceedings of the 16th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
    publisher = {MIT Press},
    author = {Zhou, Dengyong and Bousquet, Olivier and Lal, Thomas Navin and Weston, Jason and Schölkopf, Bernhard},
    year = {2003},
    note = {event-place: Whistler, British Columbia, Canada},
    pages = {321--328}
}

@article{huang_densely_2016,
    title = {Densely {Connected} {Convolutional} {Networks}},
    url = {http://arxiv.org/abs/1608.06993},
    archivePrefix = {arXiv},
    eprinttype = {arxiv},
    eprint = {1608.06993},
    abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
    urldate = {2019-06-24},
    journal = {arXiv:1608.06993 [cs]},
    author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
    month = aug,
    year = {2016},
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning}
}

@article{baldi_neural_1993,
    title = {Neural {Networks} for {Fingerprint} {Recognition}},
    volume = {5},
    issn = {0899-7667},
    url = {https://www.mitpressjournals.org/doi/10.1162/neco.1993.5.3.402},
    doi = {10.1162/neco.1993.5.3.402},
    abstract = {After collecting a data base of fingerprint images, we design a neural network algorithm for fingerprint recognition. When presented with a pair of fingerprint images, the algorithm outputs an estimate of the probability that the two images originate from the same finger. In one experiment, the neural network is trained using a few hundred pairs of images and its performance is subsequently tested using several thousand pairs of images originated from a subset of the database corresponding to 20 individuals. The error rate currently achieved is less than 0.5\%. Additional results, extensions, and possible applications are also briefly discussed.},
    number = {3},
    urldate = {2019-06-07},
    journal = {Neural Computation},
    author = {Baldi, Pierre and Chauvin, Yves},
    month = may,
    year = {1993},
    pages = {402--418}
}

@inproceedings{bromley_signature_1993,
    address = {San Francisco, CA, USA},
    series = {{NIPS}'93},
    title = {Signature {Verification} {Using} a "{Siamese}" {Time} {Delay} {Neural} {Network}},
    url = {http://dl.acm.org/citation.cfm?id=2987189.2987282},
    abstract = {This paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a "Siamese" neural network. This network consists of two identical sub-networks joined at their outputs. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance between the two feature vectors. Verification consists of comparing an extracted feature vector with a stored feature vector for the signer. Signatures closer to this stored representation than a chosen threshold are accepted, all other signatures are rejected as forgeries.},
    urldate = {2019-06-07},
    booktitle = {Proceedings of the 6th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
    publisher = {Morgan Kaufmann Publishers Inc.},
    author = {Bromley, Jane and Guyon, Isabelle and LeCun, Yann and Säckinger, Eduard and Shah, Roopak},
    year = {1993},
    note = {event-place: Denver, Colorado},
    pages = {737--744}
}

@inproceedings{rao_deep_2016,
    title = {A {Deep} {Siamese} {Neural} {Network} {Learns} the {Human}-{Perceived} {Similarity} {Structure} of {Facial} {Expressions} {Without} {Explicit} {Categories}},
    abstract = {In previous work, we showed that a simple neurocomputational model The Model, or TM) trained on the Ekman \& Friesen Pictures of Facial Affect (POFA) dataset to categorize the images into the six basic expressions can account for wide array of data (albeit from a single study) on facial expression processing. The model demonstrated categorical perception of facial expressions, as well as the so-called facial expression circumplex, a circular configuration based on MDS results that places the categories in the order happy, surprise, fear, sadness, anger and disgust. Somewhat ironically, the circumplex in TM was generated from the similarity between the categorical outputs of the network, i.e., the six numbers representing the probability of the category given the face. Here, we extend this work by 1) using a new dataset, NimsStims, that is much larger than POFA, and is not as tightly controlled for the correct Facial Action Units; 2) using a completely different neural network architecture, a Siamese Neural Network (SNN) that maps two faces through twin networks into a 2D similarity space; and 3) training the network only implicitly, based on a teaching signal that pairs of faces are in either in the same or different categories. Our results show that in this setting, the network learns a representation that is very similar to the original circumplex. Fear and surprise overlap, which is consistent with the inherent confusability between these two facial expressions. Our results suggest that humans evolved in such a way that nearby emotions are represented by similar appearances.},
    booktitle = {{CogSci}},
    author = {Rao, Sanjeev Jagannatha and Wang, Yufei and Cottrell, Garrison W.},
    year = {2016},
    keywords = {Artificial neural network, Categorization, Image, Map, Network architecture, Sadness, Spiking neural network}
}

@inproceedings{koch_siamese_2015,
    address = {Lille, France},
    title = {Siamese {Neural} {Networks} for {One}-{Shot} {Image} {Recognition}},
    volume = {2},
    url = {http://www.cs.toronto.edu/~rsalakhu/papers/oneshot1.pdf},
    abstract = {The process of learning good features for machine learning applications can be very computationally expensive and may prove difficult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classification tasks. Humans exhibit a strong ability to acquire and recognize new patterns. In particular, we observe that when presented with stimuli, people seem to be able to understand new concepts quickly and then recognize variations on these concepts in future percepts (Lake et al., 2011). Machine learning has been successfully used to achieve state-ofthe-art performance in a variety of applications such as web search, spam detection, caption generation, and speech and image recognition. However, these algorithms often break down when forced to make predictions about data for which little supervised information is available. We desire to generalize to these unfamiliar categories without necessitating extensive retraining which may be either expensive or impossible due to limited data or in an online prediction setting, such as web retrieval.},
    language = {English},
    booktitle = {{ICML} {Deep} {Learning} {Workshop}},
    author = {Koch, Gregory and Zemel, Richard and Salakhutdinov, Ruslan},
    month = jul,
    year = {2015},
    keywords = {Algorithm, Analysis of algorithms, anatomical layer, Baseline (configuration management), cell transformation, Cerebrovascular accident, Class, Columns, Computer vision, Deep learning, Distortion, Experiment, Exploit (computer security), Hope (emotion), Machine learning, Neural Network Simulation, Neural Networks, One-shot learning, Personality Character, research study, Verification of Theories}
}

@inproceedings{hadsell_dimensionality_2006,
    address = {New York, NY, USA},
    title = {Dimensionality {Reduction} by {Learning} an {Invariant} {Mapping}},
    volume = {2},
    isbn = {978-0-7695-2597-6},
    url = {http://ieeexplore.ieee.org/document/1640964/},
    doi = {10.1109/CVPR.2006.100},
    urldate = {2019-06-07},
    booktitle = {2006 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} - {Volume} 2 ({CVPR}'06)},
    publisher = {IEEE},
    author = {Hadsell, R. and Chopra, S. and LeCun, Y.},
    year = {2006},
    pages = {1735--1742}
}


@article{wang_learning_2014,
    title = {Learning {Fine}-grained {Image} {Similarity} with {Deep} {Ranking}},
    url = {http://arxiv.org/abs/1404.4661},
    archivePrefix = {arXiv},
    eprinttype = {arxiv},
    eprint = {1404.4661},
    abstract = {Learning fine-grained image similarity is a challenging task. It needs to capture between-class and within-class image differences. This paper proposes a deep ranking model that employs deep learning techniques to learn similarity metric directly from images.It has higher learning capability than models based on hand-crafted features. A novel multiscale network structure has been developed to describe the images effectively. An efficient triplet sampling algorithm is proposed to learn the model with distributed asynchronized stochastic gradient. Extensive experiments show that the proposed algorithm outperforms models based on hand-crafted visual features and deep classification models.},
    urldate = {2019-06-10},
    journal = {arXiv:1404.4661 [cs]},
    author = {Wang, Jiang and song, Yang and Leung, Thomas and Rosenberg, Chuck and Wang, Jinbin and Philbin, James and Chen, Bo and Wu, Ying},
    month = apr,
    year = {2014},
    keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{schroff_facenet_2015,
    title = {{FaceNet}: {A} {Unified} {Embedding} for {Face} {Recognition} and {Clustering}},
    shorttitle = {{FaceNet}},
    url = {http://arxiv.org/abs/1503.03832},
    archivePrefix = {arXiv},
    eprinttype = {arxiv},
    eprint = {1503.03832},
    doi = {10.1109/CVPR.2015.7298682},
    abstract = {Despite significant recent advances in the field of face recognition, implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-art face recognition performance using only 128-bytes per face. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result by 30\% on both datasets. We also introduce the concept of harmonic embeddings, and a harmonic triplet loss, which describe different versions of face embeddings (produced by different networks) that are compatible to each other and allow for direct comparison between each other.},
    urldate = {2019-06-07},
    journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
    month = jun,
    year = {2015},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    pages = {815--823}
}

@inproceedings{weinberger_distance_2006,
    address = {Vancouver, Canada},
    title = {Distance {Metric} {Learning} for {Large} {Margin} {Nearest} {Neighbor} {Classification}},
    url = {http://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf},
    abstract = {We show how to learn a Mahanalobis distance metric for k-nearest neighbor (kNN) classification by semidefinite programming. The metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. On seven data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification—for example, achieving a test error rate of 1.3\% on the MNIST handwritten digits. As in support vector machines (SVMs), the learning problem reduces to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our framework requires no modification or extension for problems in multiway (as opposed to binary) classification.},
    language = {English},
    urldate = {2019-06-07},
    booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 18},
    publisher = {MIT Press},
    author = {Weinberger, Kilian Q and Blitzer, John and Saul, Lawrence K.},
    editor = {Weiss, Y. and Schölkopf, B. and Platt, J. C.},
    year = {2006},
    pages = {1473--1480}
}

@article{szegedy_going_2014,
    title = {Going {Deeper} with {Convolutions}},
    url = {http://arxiv.org/abs/1409.4842},
    archivePrefix = {arXiv},
    eprinttype = {arxiv},
    eprint = {1409.4842},
    abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
    urldate = {2017-03-22},
    journal = {arXiv:1409.4842 [cs]},
    author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
    month = sep,
    year = {2014},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{hermans_defense_2017,
    title = {In {Defense} of the {Triplet} {Loss} for {Person} {Re}-{Identification}},
    url = {http://arxiv.org/abs/1703.07737},
    archivePrefix = {arXiv},
    eprinttype = {arxiv},
    eprint = {1703.07737},
    abstract = {In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person re-identification subfield is no exception to this. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms most other published methods by a large margin.},
    urldate = {2019-06-11},
    journal = {arXiv:1703.07737 [cs]},
    author = {Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},
    month = mar,
    year = {2017},
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{yu_correcting_2018,
    title = {Correcting the {Triplet} {Selection} {Bias} for {Triplet} {Loss}},
    url = {http://openaccess.thecvf.com/content_ECCV_2018/html/Baosheng_Yu_Correcting_the_Triplet_ECCV_2018_paper.html},
    urldate = {2019-06-11},
    author = {Yu, Baosheng and Liu, Tongliang and Gong, Mingming and Ding, Changxing and Tao, Dacheng},
    year = {2018},
    pages = {71--87}
}

@article{shi_embedding_2016,
    title = {Embedding {Deep} {Metric} for {Person} {Re}-identication {A} {Study} {Against} {Large} {Variations}},
    url = {http://arxiv.org/abs/1611.00137},
    archivePrefix = {arXiv},
    eprinttype = {arxiv},
    eprint = {1611.00137},
    abstract = {Person re-identification is challenging due to the large variations of pose, illumination, occlusion and camera view. Owing to these variations, the pedestrian data is distributed as highly-curved manifolds in the feature space, despite the current convolutional neural networks (CNN)'s capability of feature extraction. However, the distribution is unknown, so it is difficult to use the geodesic distance when comparing two samples. In practice, the current deep embedding methods use the Euclidean distance for the training and test. On the other hand, the manifold learning methods suggest to use the Euclidean distance in the local range, combining with the graphical relationship between samples, for approximating the geodesic distance. From this point of view, selecting suitable positive i.e. intra-class) training samples within a local range is critical for training the CNN embedding, especially when the data has large intra-class variations. In this paper, we propose a novel moderate positive sample mining method to train robust CNN for person re-identification, dealing with the problem of large variation. In addition, we improve the learning by a metric weight constraint, so that the learned metric has a better generalization ability. Experiments show that these two strategies are effective in learning robust deep metrics for person re-identification, and accordingly our deep model significantly outperforms the state-of-the-art methods on several benchmarks of person re-identification. Therefore, the study presented in this paper may be useful in inspiring new designs of deep models for person re-identification.},
    urldate = {2019-06-11},
    journal = {arXiv:1611.00137 [cs]},
    author = {Shi, Hailin and Yang, Yang and Zhu, Xiangyu and Liao, Shengcai and Lei, Zhen and Zheng, Weishi and Li, Stan Z.},
    month = nov,
    year = {2016},
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning}
}

@inproceedings{sohn_improved_2016,
    address = {USA},
    series = {{NIPS}'16},
    title = {Improved {Deep} {Metric} {Learning} with {Multi}-class {N}-pair {Loss} {Objective}},
    isbn = {978-1-5108-3881-9},
    url = {http://dl.acm.org/citation.cfm?id=3157096.3157304},
    abstract = {Deep metric learning has gained much popularity in recent years, following the success of deep learning. However, existing frameworks of deep metric learning based on contrastive loss and triplet loss often suffer from slow convergence, partially because they employ only one negative example while not interacting with the other negative classes in each update. In this paper, we propose to address this problem with a new metric learning objective called multi-class N-pair loss. The proposed objective function firstly generalizes triplet loss by allowing joint comparison among more than one negative examples - more specifically, N-1 negative examples - and secondly reduces the computational burden of evaluating deep embedding vectors via an efficient batch construction strategy using only N pairs of examples, instead of (N+1) x N. We demonstrate the superiority of our proposed loss to the triplet loss as well as other competing loss functions for a variety of tasks on several visual recognition benchmark, including fine-grained object recognition and verification, image clustering and retrieval, and face verification and identification.},
    urldate = {2019-06-11},
    booktitle = {Proceedings of the 30th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
    publisher = {Curran Associates Inc.},
    author = {Sohn, Kihyuk},
    year = {2016},
    note = {event-place: Barcelona, Spain},
    pages = {1857--1865}
}

@article{song_deep_2015,
    title = {Deep {Metric} {Learning} via {Lifted} {Structured} {Feature} {Embedding}},
    url = {http://arxiv.org/abs/1511.06452},
    archivePrefix = {arXiv},
    eprinttype = {arxiv},
    eprint = {1511.06452},
    abstract = {Learning the distance metric between pairs of examples is of great importance for learning and visual recognition. With the remarkable success from the state of the art convolutional neural networks, recent works have shown promising results on discriminatively training the networks to learn semantic feature embeddings where similar examples are mapped close to each other and dissimilar examples are mapped farther apart. In this paper, we describe an algorithm for taking full advantage of the training batches in the neural network training by lifting the vector of pairwise distances within the batch to the matrix of pairwise distances. This step enables the algorithm to learn the state of the art feature embedding by optimizing a novel structured prediction objective on the lifted problem. Additionally, we collected Online Products dataset: 120k images of 23k classes of online products for metric learning. Our experiments on the CUB-200-2011, CARS196, and Online Products datasets demonstrate significant improvement over existing deep feature embedding methods on all experimented embedding sizes with the GoogLeNet network.},
    urldate = {2019-06-11},
    journal = {arXiv:1511.06452 [cs]},
    author = {Song, Hyun Oh and Xiang, Yu and Jegelka, Stefanie and Savarese, Silvio},
    month = nov,
    year = {2015},
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning}
}

@article{wang_deep_2017,
    title = {Deep {Metric} {Learning} with {Angular} {Loss}},
    url = {http://arxiv.org/abs/1708.01682},
    archivePrefix = {arXiv},
    eprinttype = {arxiv},
    eprint = {1708.01682},
    abstract = {The modern image search system requires semantic understanding of image, and a key yet under-addressed problem is to learn a good metric for measuring the similarity between images. While deep metric learning has yielded impressive performance gains by extracting high level abstractions from image data, a proper objective loss function becomes the central issue to boost the performance. In this paper, we propose a novel angular loss, which takes angle relationship into account, for learning better similarity metric. Whereas previous metric learning methods focus on optimizing the similarity (contrastive loss) or relative similarity (triplet loss) of image pairs, our proposed method aims at constraining the angle at the negative point of triplet triangles. Several favorable properties are observed when compared with conventional methods. First, scale invariance is introduced, improving the robustness of objective against feature variance. Second, a third-order geometric constraint is inherently imposed, capturing additional local structure of triplet triangles than contrastive loss or triplet loss. Third, better convergence has been demonstrated by experiments on three publicly available datasets.},
    urldate = {2019-06-11},
    journal = {arXiv:1708.01682 [cs]},
    author = {Wang, Jian and Zhou, Feng and Wen, Shilei and Liu, Xiao and Lin, Yuanqing},
    month = aug,
    year = {2017},
    keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{wu_sampling_2017,
    title = {Sampling {Matters} in {Deep} {Embedding} {Learning}},
    url = {http://arxiv.org/abs/1706.07567},
    archivePrefix = {arXiv},
    eprinttype = {arxiv},
    eprint = {1706.07567},
    abstract = {Deep embeddings answer one simple question: How similar are two images? Learning these embeddings is the bedrock of verification, zero-shot learning, and visual search. The most prominent approaches optimize a deep convolutional network with a suitable loss function, such as contrastive loss or triplet loss. While a rich line of work focuses solely on the loss functions, we show in this paper that selecting training examples plays an equally important role. We propose distance weighted sampling, which selects more informative and stable examples than traditional approaches. In addition, we show that a simple margin based loss is sufficient to outperform all other loss functions. We evaluate our approach on the Stanford Online Products, CAR196, and the CUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset for face verification. Our method achieves state-of-the-art performance on all of them.},
    urldate = {2019-06-11},
    journal = {arXiv:1706.07567 [cs]},
    author = {Wu, Chao-Yuan and Manmatha, R. and Smola, Alexander J. and Krähenbühl, Philipp},
    month = jun,
    year = {2017},
    keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{duan_deep_2018,
    title = {Deep {Adversarial} {Metric} {Learning}},
    url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Duan_Deep_Adversarial_Metric_CVPR_2018_paper.html},
    urldate = {2019-06-11},
    author = {Duan, Yueqi and Zheng, Wenzhao and Lin, Xudong and Lu, Jiwen and Zhou, Jie},
    year = {2018},
    pages = {2780--2789}
}

@article{zheng_hardness-aware_2019,
    title = {Hardness-{Aware} {Deep} {Metric} {Learning}},
    url = {http://arxiv.org/abs/1903.05503},
    archivePrefix = {arXiv},
    eprinttype = {arxiv},
    eprint = {1903.05503},
    abstract = {This paper presents a hardness-aware deep metric learning (HDML) framework. Most previous deep metric learning methods employ the hard negative mining strategy to alleviate the lack of informative samples for training. However, this mining strategy only utilizes a subset of training data, which may not be enough to characterize the global geometry of the embedding space comprehensively. To address this problem, we perform linear interpolation on embeddings to adaptively manipulate their hard levels and generate corresponding label-preserving synthetics for recycled training, so that information buried in all samples can be fully exploited and the metric is always challenged with proper difficulty. Our method achieves very competitive performance on the widely used CUB-200-2011, Cars196, and Stanford Online Products datasets.},
    urldate = {2019-06-11},
    journal = {arXiv:1903.05503 [cs]},
    author = {Zheng, Wenzhao and Chen, Zhaodong and Lu, Jiwen and Zhou, Jie},
    month = mar,
    year = {2019},
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning}
}

@book{jolliffe_principal_2002,
    address = {New York, NY},
    edition = {2. ed},
    series = {Springer series in statistics},
    title = {Principal component analysis},
    isbn = {978-0-387-95442-4},
    language = {eng},
    publisher = {Springer},
    author = {Jolliffe, Ian T.},
    year = {2002}
}

@inproceedings{macqueen_methods_1967,
  title = {Some Methods for Classification and Analysis of Multivariate Observations},
  abstract = {Project Euclid - mathematics and statistics online},
  language = {EN},
  urldate = {2019-06-27},
  booktitle = {Proceedings of the {{Fifth Berkeley Symposium}} on {{Mathematical Statistics}} and {{Probability}}, {{Volume}} 1: {{Statistics}}},
  publisher = {{The Regents of the University of California}},
  url = {https://projecteuclid.org/euclid.bsmsp/1200512992},
  author = {MacQueen, J.},
  year = {1967}
}

@article{lloyd_least_1982,
  title = {Least Squares Quantization in {{PCM}}},
  volume = {28},
  issn = {0018-9448},
  abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for2\^bquanta,b=1,2, \textbackslash{}cdots, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
  number = {2},
  journal = {IEEE Transactions on Information Theory},
  doi = {10.1109/TIT.1982.1056489},
  author = {Lloyd, S.},
  month = mar,
  year = {1982},
  keywords = {Least-squares approximation,PCM communication,Quantization (signal),Signal quantization},
  pages = {129-137}
}

@inproceedings{arthur_kmeans_2007,
  address = {{Philadelphia, PA, USA}},
  series = {{{SODA}} '07},
  title = {K-Means++: {{The Advantages}} of {{Careful Seeding}}},
  isbn = {978-0-89871-624-5},
  shorttitle = {K-Means++},
  abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is {$\Theta$}(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
  urldate = {2019-06-27},
  booktitle = {Proceedings of the {{Eighteenth Annual ACM}}-{{SIAM Symposium}} on {{Discrete Algorithms}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  url = {http://dl.acm.org/citation.cfm?id=1283383.1283494},
  author = {Arthur, David and Vassilvitskii, Sergei},
  year = {2007},
  pages = {1027--1035}
}

@inproceedings{hinton_stochastic_2003,
  address = {{Vancouver, British Columbia, Canada}},
  title = {Stochastic {{Neighbor Embedding}}},
  urldate = {2019-08-18},
  booktitle = {Advances in {{Neural Information Processing Systems}} 15},
  publisher = {{MIT Press}},
  url = {http://papers.nips.cc/paper/2276-stochastic-neighbor-embedding.pdf},
  author = {Hinton, Geoffrey E and Roweis, Sam T.},
  editor = {Becker, S. and Thrun, S. and Obermayer, K.},
  year = {2003},
  pages = {857--864}
}

@article{maaten_visualizing_2008,
  title = {Visualizing {{Data}} Using T-{{SNE}}},
  volume = {9},
  issn = {ISSN 1533-7928},
  number = {Nov},
  urldate = {2017-02-20},
  journal = {Journal of Machine Learning Research},
  url = {http://www.jmlr.org/papers/v9/vandermaaten08a.html},
  author = {{van der Maaten}, Laurens and Hinton, Geoffrey},
  year = {2008},
  pages = {2579-2605}
}

@inproceedings{ester_densitybased_1996,
  series = {{{KDD}}'96},
  title = {A {{Density}}-Based {{Algorithm}} for {{Discovering Clusters}} a {{Density}}-Based {{Algorithm}} for {{Discovering Clusters}} in {{Large Spatial Databases}} with {{Noise}}},
  abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
  urldate = {2019-08-07},
  booktitle = {Proceedings of the {{Second International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{AAAI Press}},
  url = {http://dl.acm.org/citation.cfm?id=3001460.3001507},
  author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei},
  year = {1996},
  keywords = {arbitrary shape of clusters,clustering algorithms,efficiency on large spatial databases,handling nlj4-275oise},
  pages = {226--231}
}

@article{vonluxburg_tutorial_2007,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0711.0189},
  primaryClass = {cs},
  title = {A {{Tutorial}} on {{Spectral Clustering}}},
  abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
  urldate = {2019-08-07},
  journal = {arXiv:0711.0189 [cs]},
  url = {http://arxiv.org/abs/0711.0189},
  author = {{von Luxburg}, Ulrike},
  month = nov,
  year = {2007},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning}
}

@inproceedings{campello_densitybased_2013,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Density-{{Based Clustering Based}} on {{Hierarchical Density Estimates}}},
  isbn = {978-3-642-37456-2},
  abstract = {We propose a theoretically and practically improved density-based, hierarchical clustering method, providing a clustering hierarchy from which a simplified tree of significant clusters can be constructed. For obtaining a ``flat'' partition consisting of only the most significant clusters (possibly corresponding to different density thresholds), we propose a novel cluster stability measure, formalize the problem of maximizing the overall stability of selected clusters, and formulate an algorithm that computes an optimal solution to this problem. We demonstrate that our approach outperforms the current, state-of-the-art, density-based clustering methods on a wide variety of real world data.},
  language = {en},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Campello, Ricardo J. G. B. and Moulavi, Davoud and Sander, Joerg},
  editor = {Pei, Jian and Tseng, Vincent S. and Cao, Longbing and Motoda, Hiroshi and Xu, Guandong},
  year = {2013},
  keywords = {Cluster Tree,Core Object,Density Threshold,Hierarchical Cluster Method,Minimum Span Tree},
  pages = {160-172}
}

@article{barnes_hierarchical_1986,
  title = {A Hierarchical {{O}}({{N}} Log {{N}}) Force-Calculation Algorithm},
  volume = {324},
  copyright = {1986 Nature Publishing Group},
  issn = {1476-4687},
  abstract = {Until recently the gravitational N-body problem has been modelled numerically either by direct integration, in which the computation needed increases as N2, or by an iterative potential method in which the number of operations grows as N log N. Here we describe a novel method of directly calculating the force on N bodies that grows only as N log N. The technique uses a tree-structured hierarchical subdivision of space into cubic cells, each of which is recursively divided into eight subcells whenever more than one particle is found to occupy the same cell. This tree is constructed anew at every time step, avoiding ambiguity and tangling. Advantages over potential-solving codes are: accurate local interactions; freedom from geometrical assumptions and restrictions; and applicability to a wide class of systems, including (proto-)planetary, stellar, galactic and cosmological ones. Advantages over previous hierarchical tree-codes include simplicity and the possibility of rigorous analysis of error. Although we concentrate here on stellar dynamical applications, our techniques of efficiently handling a large number of long-range interactions and concentrating computational effort where most needed have potential applications in other areas of astrophysics as well.},
  language = {en},
  number = {6096},
  urldate = {2019-08-19},
  journal = {Nature},
  doi = {10/fvcc2m},
  url = {https://www.nature.com/articles/324446a0},
  author = {Barnes, Josh and Hut, Piet},
  month = dec,
  year = {1986},
  pages = {446-449}
}

@article{maaten_accelerating_2014,
  title = {Accelerating T-{{SNE}} Using {{Tree}}-{{Based Algorithms}}},
  volume = {15},
  urldate = {2019-08-19},
  journal = {Journal of Machine Learning Research},
  url = {http://jmlr.org/papers/v15/vandermaaten14a.html},
  author = {van der Maaten, Laurens},
  year = {2014},
  pages = {3221-3245}
}

@article{mcinnes_umap_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.03426},
  primaryClass = {cs, stat},
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  urldate = {2019-08-18},
  journal = {arXiv:1802.03426 [cs, stat]},
  url = {http://arxiv.org/abs/1802.03426},
  author = {McInnes, Leland and Healy, John and Melville, James},
  month = feb,
  year = {2018},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{mcinnes_umap_2018a,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}}},
  volume = {3},
  issn = {2475-9066},
  shorttitle = {{{UMAP}}},
  number = {29},
  urldate = {2019-08-18},
  journal = {Journal of Open Source Software},
  doi = {10/gf6k3s},
  url = {http://joss.theoj.org/papers/10.21105/joss.00861},
  author = {McInnes, Leland and Healy, John and Saul, Nathaniel and Gro{\ss}berger, Lukas},
  month = sep,
  year = {2018},
  pages = {861}
}

@book{zisman_topologie_1972,
  address = {{Paris, France}},
  title = {{Topologie alg{\'e}brique {\'e}l{\'e}mentaire: Ma{\^i}trise de math{\'e}matiques}},
  shorttitle = {{Topologie alg{\'e}brique {\'e}l{\'e}mentaire}},
  language = {fran{\c c}ais},
  publisher = {{Armand Colin}},
  author = {Zisman, Michel},
  year = {1972},
  keywords = {Algebraic topology,QA612. .Z57,Topologie algébrique -- Manuels d'enseignement supérieur},
}

@inproceedings{dong_efficient_2011,
  address = {{New York, NY, USA}},
  series = {{{WWW}} '11},
  title = {Efficient {{K}}-Nearest {{Neighbor Graph Construction}} for {{Generic Similarity Measures}}},
  isbn = {978-1-4503-0632-4},
  abstract = {K-Nearest Neighbor Graph (K-NNG) construction is an important operation with many web related applications, including collaborative filtering, similarity search, and many others in data mining and machine learning. Existing methods for K-NNG construction either do not scale, or are specific to certain similarity measures. We present NN-Descent, a simple yet efficient algorithm for approximate K-NNG construction with arbitrary similarity measures. Our method is based on local search, has minimal space overhead and does not rely on any shared global index. Hence, it is especially suitable for large-scale applications where data structures need to be distributed over the network. We have shown with a variety of datasets and similarity measures that the proposed method typically converges to above 90\% recall with each point comparing only to several percent of the whole dataset on average.},
  urldate = {2019-08-20},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{World Wide Web}}},
  publisher = {{ACM}},
  doi = {10/dh93jt},
  url = {http://doi.acm.org/10.1145/1963405.1963487},
  author = {Dong, Wei and Moses, Charikar and Li, Kai},
  year = {2011},
  keywords = {arbitrary similarity measure,iterative method,k-nearest neighbor graph},
  pages = {577--586}
}

@article{pearson_lines_1901,
  title = {On Lines and Planes of Closest Fit to Systems of Points in Space},
  volume = {2},
  issn = {1941-5982},
  language = {English},
  number = {11},
  urldate = {2019-07-02},
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  doi = {10/dd63n4},
  url = {https://doi.org/10.1080/14786440109462720},
  author = {Pearson, Karl},
  month = nov,
  year = {1901},
  pages = {559-572}
}

@article{hotelling_analysis_1933,
  title = {Analysis of a Complex of Statistical Variables into Principal Components},
  volume = {24},
  issn = {1939-2176(Electronic),0022-0663(Print)},
  abstract = {The problem is stated in detail, a method of analysis is derived and its geometrical meaning shown, methods of solution are illustrated and certain derivative problems are discussed. (To be concluded in October issue.) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  number = {6},
  journal = {Journal of Educational Psychology},
  doi = {10/fb5435},
  author = {Hotelling, Harold},
  year = {1933},
  keywords = {Statistical Analysis,Statistical Variables},
  pages = {417-441}
}

@inproceedings{scholkopf_kernel_1997,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {Kernel Principal Component Analysis},
  isbn = {978-3-540-69620-9},
  abstract = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in highdimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
  language = {en},
  booktitle = {Artificial {{Neural Networks}} \textemdash{} {{ICANN}}'97},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Sch{\"o}lkopf, Bernhard and Smola, Alexander and M{\"u}ller, Klaus-Robert},
  editor = {Gerstner, Wulfram and Germond, Alain and Hasler, Martin and Nicoud, Jean-Daniel},
  year = {1997},
  keywords = {Feature Space,Independent Component Analysis,Kernel Principal Component Analysis,Standard Principal Component Analysis,Support Vector Machine},
  pages = {583-588}
}

@article{scholkopf_nonlinear_1998,
  title = {Nonlinear {{Component Analysis}} as a {{Kernel Eigenvalue Problem}}},
  volume = {10},
  issn = {0899-7667},
  abstract = {A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear map\textemdash{}for instance, the space of all possible five-pixel products in 16 \texttimes{} 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
  number = {5},
  urldate = {2019-07-03},
  journal = {Neural Computation},
  doi = {10/dpn6x6},
  url = {https://www.mitpressjournals.org/doi/10.1162/089976698300017467},
  author = {Sch{\"o}lkopf, Bernhard and Smola, Alexander and M{\"u}ller, Klaus-Robert},
  month = jul,
  year = {1998},
  pages = {1299-1319}
}

@phdthesis{lecun_modeles_1987,
  address = {{Paris}},
  type = {{PhD Thesis}},
  title = {{Mod{\`e}les connexionnistes de l'apprentissage}},
  shorttitle = {{Mod{\`e}les connexionnistes de l'apprentissage}},
  abstract = {Etude des mod{\`e}les d'apprentissage simples et application aux m{\'e}moires associatives. M{\'e}thodes d'apprentissage pour r{\'e}seaux {\`a} cellules cach{\'e}es. Algorithme de retro propagation et ses variantes. Applications diverses (associative, reconnaissance de caract{\`e}res, diagnostic m{\'e}dical) et logiciel de simulation.},
  language = {French},
  urldate = {2019-07-20},
  school = {Universit{\'e} Paris 6},
  url = {http://yann.lecun.com/exdb/publis/index.html\#lecun-87},
  author = {Le Cun, Yann and Milgram, Maurice and {Universit{\'e} Pierre et Marie Curie (Paris)}},
  year = {1987},
}

@article{gallinari_memoires_1987,
  title = {{M{\'e}moires associatives distribu{\'e}es: Une comparaison (Distributed associative memories: A comparison)}},
  shorttitle = {{M{\'e}moires associatives distribu{\'e}es}},
  language = {English (US)},
  urldate = {2019-07-20},
  journal = {Proceedings of COGNITIVA 87, Paris, La Villette, May 1987},
  url = {https://nyuscholars.nyu.edu/en/publications/memoires-associatives-distribuees-une-comparaison-distributed-ass},
  author = {Gallinari, P. and Lecun, Yann and Thiria, S. and Soulie, F. Fogelman},
  year = {1987}
}

@article{bourlard_autoassociation_1988,
  title = {Auto-Association by Multilayer Perceptrons and Singular Value Decomposition},
  volume = {59},
  issn = {1432-0770},
  abstract = {The multilayer perceptron, when working in auto-association mode, is sometimes considered as an interesting candidate to perform data compression or dimensionality reduction of the feature space in information processing applications. The present paper shows that, for auto-association, the nonlinearities of the hidden units are useless and that the optimal parameter values can be derived directly by purely linear techniques relying on singular value decomposition and low rank matrix approximation, similar in spirit to the well-known Karhunen-Lo{\`e}ve transform. This approach appears thus as an efficient alternative to the general error back-propagation algorithm commonly used for training multilayer perceptrons. Moreover, it also gives a clear interpretation of the r{\^o}le of the different parameters.},
  language = {en},
  number = {4},
  urldate = {2019-07-22},
  journal = {Biological Cybernetics},
  doi = {10/fm3xqd},
  url = {https://doi.org/10.1007/BF00332918},
  author = {Bourlard, H. and Kamp, Y.},
  month = sep,
  year = {1988},
  keywords = {Data Compression,Dimensionality Reduction,Feature Space,Multilayer Perceptrons,Processing Application},
  pages = {291-294}
}

@inproceedings{ballard_modular_1987,
  series = {{{AAAI}}'87},
  title = {Modular {{Learning}} in {{Neural Networks}}},
  isbn = {978-0-934613-42-2},
  abstract = {In the development of large-scale knowledge networks much recent progress has been inspired by connections to neurobiology. An important component of any "neural" network is an accompanying learning algorithm. Such an algorithm, to be biologically plausible, must work for very large numbers of units. Studies of large-scale systems have so far been restricted to systems Without internal units (units With no direct connections to the input or output). Internal units are crucial to such systems as they are the means by which a system can encode high-order regularities (or invariants) that are Implicit in its inputs and outputs. Computer simulations of learning using internal units have been restricted to small-scale systems. This paper describes away of coupling autoassociative learning modules Into hierarchies that should greatly improve the performance of learning algorithms in large-scale systems. The Idea has been tested experimentally with positive results.},
  urldate = {2019-07-20},
  booktitle = {Proceedings of the {{Sixth National Conference}} on {{Artificial Intelligence}} - {{Volume}} 1},
  publisher = {{AAAI Press}},
  url = {http://dl.acm.org/citation.cfm?id=1863696.1863746},
  author = {Ballard, Dana H.},
  year = {1987},
  pages = {279--284}
}

@article{kingma_autoencoding_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.6114},
  primaryClass = {cs, stat},
  title = {Auto-{{Encoding Variational Bayes}}},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  urldate = {2019-07-21},
  journal = {arXiv:1312.6114 [cs, stat]},
  url = {http://arxiv.org/abs/1312.6114},
  author = {Kingma, Diederik P. and Welling, Max},
  month = dec,
  year = {2013},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning}
}

@article{rezende_stochastic_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1401.4082},
  primaryClass = {cs, stat},
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
  urldate = {2019-07-21},
  journal = {arXiv:1401.4082 [cs, stat]},
  url = {http://arxiv.org/abs/1401.4082},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  month = jan,
  year = {2014},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Statistics - Computation,Statistics - Methodology,Computer Science - Machine Learning}
}

@article{doersch_tutorial_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.05908},
  primaryClass = {cs, stat},
  title = {Tutorial on {{Variational Autoencoders}}},
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  urldate = {2019-07-23},
  journal = {arXiv:1606.05908 [cs, stat]},
  url = {http://arxiv.org/abs/1606.05908},
  author = {Doersch, Carl},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{kingma_introduction_2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.02691},
  primaryClass = {cs, stat},
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  urldate = {2019-07-25},
  journal = {arXiv:1906.02691 [cs, stat]},
  url = {http://arxiv.org/abs/1906.02691},
  author = {Kingma, Diederik P. and Welling, Max},
  month = jun,
  year = {2019},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning}
}

@article{kingma_semisupervised_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.5298},
  primaryClass = {cs, stat},
  title = {Semi-{{Supervised Learning}} with {{Deep Generative Models}}},
  abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
  urldate = {2019-07-23},
  journal = {arXiv:1406.5298 [cs, stat]},
  url = {http://arxiv.org/abs/1406.5298},
  author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
  month = jun,
  year = {2014},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning}
}

@article{goodfellow_generative_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.2661},
  primaryClass = {cs, stat},
  title = {Generative {{Adversarial Networks}}},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  urldate = {2017-02-20},
  journal = {arXiv:1406.2661 [cs, stat]},
  url = {http://arxiv.org/abs/1406.2661},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  month = jun,
  year = {2014},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
}

@article{nash_equilibrium_1950,
  title = {Equilibrium Points in N-Person Games},
  volume = {36},
  issn = {0027-8424, 1091-6490},
  abstract = {One may define a concept of an n -person game in which each player has a finite set of pure strategies and in which a definite set of payments to the n players corresponds to each n -tuple of pure strategies, one strategy being taken for each player. For mixed strategies, which are probability distributions over the pure strategies, the pay-off functions are the expectations of the players, thus becoming polylinear forms \ldots{}},
  language = {en},
  number = {1},
  urldate = {2019-07-28},
  journal = {Proceedings of the National Academy of Sciences},
  doi = {10/b6hm7s},
  url = {https://www.pnas.org/content/36/1/48},
  author = {Nash, John F.},
  month = jan,
  year = {1950},
  pages = {48-49},
  pmid = {16588946},
}

@article{nash_noncooperative_1951,
  title = {Non-{{Cooperative Games}}},
  volume = {54},
  issn = {0003-486X},
  number = {2},
  urldate = {2019-07-28},
  journal = {Annals of Mathematics},
  doi = {10/dzj4db},
  url = {https://www.jstor.org/stable/1969529},
  author = {Nash, John},
  year = {1951},
  pages = {286-295}
}

@article{schmidhuber_unsupervised_2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.04493},
  primaryClass = {cs},
  title = {Unsupervised {{Minimax}}: {{Adversarial Curiosity}}, {{Generative Adversarial Networks}}, and {{Predictability Minimization}}},
  shorttitle = {Unsupervised {{Minimax}}},
  abstract = {Generative Adversarial Networks (GANs) learn to model data distributions through two unsupervised neural networks, each minimizing the objective function maximized by the other. We relate this game theoretic strategy to earlier neural networks playing unsupervised minimax games. (i) GANs can be formulated as a special case of Adversarial Curiosity (1990) based on a minimax duel between two networks, one generating data through its probabilistic actions, the other predicting consequences thereof. (ii) We correct a previously published claim that Predictability Minimization (PM, 1990s) is not based on a minimax game. PM models data distributions through a neural encoder that maximizes the objective function minimized by a neural predictor of the code components.},
  urldate = {2019-06-28},
  journal = {arXiv:1906.04493 [cs]},
  url = {http://arxiv.org/abs/1906.04493},
  author = {Schmidhuber, Juergen},
  month = jun,
  year = {2019},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Machine Learning}
}

@article{radford_unsupervised_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.06434},
  primaryClass = {cs},
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  journal = {arXiv:1511.06434 [cs]},
  url = {http://arxiv.org/abs/1511.06434},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  month = nov,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning}
}

@article{odena_semisupervised_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.01583},
  primaryClass = {cs, stat},
  title = {Semi-{{Supervised Learning}} with {{Generative Adversarial Networks}}},
  abstract = {We extend Generative Adversarial Networks (GANs) to the semi-supervised context by forcing the discriminator network to output class labels. We train a generative model G and a discriminator D on a dataset with inputs belonging to one of N classes. At training time, D is made to predict which of N+1 classes the input belongs to, where an extra class is added to correspond to the outputs of G. We show that this method can be used to create a more data-efficient classifier and that it allows for generating higher quality samples than a regular GAN.},
  urldate = {2019-08-06},
  journal = {arXiv:1606.01583 [cs, stat]},
  url = {http://arxiv.org/abs/1606.01583},
  author = {Odena, Augustus},
  month = jun,
  year = {2016},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
}

@article{salimans_improved_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.03498},
  primaryClass = {cs},
  title = {Improved {{Techniques}} for {{Training GANs}}},
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  urldate = {2019-08-04},
  journal = {arXiv:1606.03498 [cs]},
  url = {http://arxiv.org/abs/1606.03498},
  author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Computer Science - Machine Learning}
}

@article{arjovsky_wasserstein_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.07875},
  primaryClass = {cs, stat},
  title = {Wasserstein {{GAN}}},
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
  urldate = {2017-03-09},
  journal = {arXiv:1701.07875 [cs, stat]},
  url = {http://arxiv.org/abs/1701.07875},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  month = jan,
  year = {2017},
  keywords = {Computer Science - Learning,Statistics - Machine Learning}
}

@article{gulrajani_improved_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.00028},
  primaryClass = {cs, stat},
  title = {Improved {{Training}} of {{Wasserstein GANs}}},
  abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
  urldate = {2019-08-12},
  journal = {arXiv:1704.00028 [cs, stat]},
  url = {http://arxiv.org/abs/1704.00028},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
  month = mar,
  year = {2017},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning}
}

@article{miyato_spectral_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.05957},
  primaryClass = {cs, stat},
  title = {Spectral {{Normalization}} for {{Generative Adversarial Networks}}},
  abstract = {One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.},
  urldate = {2019-08-12},
  journal = {arXiv:1802.05957 [cs, stat]},
  url = {http://arxiv.org/abs/1802.05957},
  author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  month = feb,
  year = {2018},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning,Computer Science - Machine Learning}
}

@article{zhang_selfattention_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.08318},
  primaryClass = {cs, stat},
  title = {Self-{{Attention Generative Adversarial Networks}}},
  abstract = {In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.},
  urldate = {2019-08-01},
  journal = {arXiv:1805.08318 [cs, stat]},
  url = {http://arxiv.org/abs/1805.08318},
  author = {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus},
  month = may,
  year = {2018},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning}
}

@article{bahdanau_neural_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.0473},
  primaryClass = {cs, stat},
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  urldate = {2019-08-21},
  journal = {arXiv:1409.0473 [cs, stat]},
  url = {http://arxiv.org/abs/1409.0473},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  month = sep,
  year = {2014},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{xu_show_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.03044},
  primaryClass = {cs.LG},
  title = {Show, {{Attend}} and {{Tell}}: {{Neural Image Caption Generation}} with {{Visual Attention}}},
  shorttitle = {Show, {{Attend}} and {{Tell}}},
  abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
  urldate = {2019-08-21},
  journal = {arXiv:1502.03044 [cs.LG]},
  url = {http://arxiv.org/abs/1502.03044},
  author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
  month = feb,
  year = {2015},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{vaswani_attention_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.03762},
  primaryClass = {cs},
  title = {Attention {{Is All You Need}}},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  urldate = {2017-06-13},
  journal = {arXiv:1706.03762 [cs]},
  url = {http://arxiv.org/abs/1706.03762},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Learning,Computer Science - Computation and Language}
}

@article{brock_large_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1809.11096},
  primaryClass = {cs, stat},
  title = {Large {{Scale GAN Training}} for {{High Fidelity Natural Image Synthesis}}},
  abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick," allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.},
  urldate = {2019-08-04},
  journal = {arXiv:1809.11096 [cs, stat]},
  url = {http://arxiv.org/abs/1809.11096},
  author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  month = sep,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{greff_sacred_2017,
  title = {The {{Sacred Infrastructure}} for {{Computational Research}}},
  urldate = {2018-03-23},
  booktitle = {Proceedings of the 15th {{Python}} in {{Science Conference}}},
  url = {http://conference.scipy.org/proceedings/scipy2017/klaus_greff.html},
  author = {Greff, Klaus and Klein, Aaron and Chovanec, Martin and Hutter, Frank and Schmidhuber, J{\"u}rgen},
  year = {2017},
  pages = {49-56}
}

@book{fisher_design_1974,
  address = {{New York}},
  title = {The {{Design}} of {{Experiments}}},
  isbn = {978-0-02-844690-5},
  language = {English},
  author = {Fisher, Ronald Aylmer},
  year = {1974},
  note = {OCLC: 924773451}
}

@book{myers_response_1971,
  address = {{Boston}},
  title = {Response Surface Methodology},
  language = {English},
  urldate = {2019-08-26},
  publisher = {{Allyn and Bacon}},
  url = {http://books.google.com/books?id=48E-AAAAIAAJ},
  author = {Myers, Raymond H},
  year = {1971},
  note = {OCLC: 655239528}
}

@article{mckay_comparison_1979,
  title = {A {{Comparison}} of {{Three Methods}} for {{Selecting Values}} of {{Input Variables}} in the {{Analysis}} of {{Output}} from a {{Computer Code}}},
  volume = {21},
  issn = {0040-1706},
  abstract = {Two types of sampling plans are examined as alternatives to simple random sampling in Monte Carlo studies. These plans are shown to be improvements over simple random sampling with respect to variance for a class of estimators which includes the sample mean and the empirical distribution function.},
  number = {2},
  urldate = {2019-08-26},
  journal = {Technometrics},
  doi = {10/bp9p63},
  url = {https://www.jstor.org/stable/1268522},
  author = {McKay, M. D. and Beckman, R. J. and Conover, W. J.},
  year = {1979},
  pages = {239-245}
}

@article{sobol_distribution_1967,
  title = {On the Distribution of Points in a Cube and the Approximate Evaluation of Integrals},
  volume = {7},
  issn = {00415553},
  language = {en},
  number = {4},
  urldate = {2019-08-26},
  journal = {USSR Computational Mathematics and Mathematical Physics},
  doi = {10/crdj6j},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0041555367901449},
  author = {Sobol’, Ilya Meyerovich},
  month = jan,
  year = {1967},
  pages = {86-112}
}

@article{bergstra_random_2012,
  title = {Random {{Search}} for {{Hyper}}-{{Parameter Optimization}}},
  volume = {13},
  issn = {ISSN 1533-7928},
  number = {Feb},
  urldate = {2019-03-29},
  journal = {Journal of Machine Learning Research},
  url = {http://www.jmlr.org/papers/v13/bergstra12a.html},
  author = {Bergstra, James and Bengio, Yoshua},
  year = {2012},
  pages = {281-305}
}

@inproceedings{bergstra_algorithms_2011,
  address = {{USA}},
  series = {{{NIPS}}'11},
  title = {Algorithms for {{Hyper}}-{{Parameter Optimization}}},
  isbn = {978-1-61839-599-3},
  abstract = {Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [Larochelle et al., 2007] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y|x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
  urldate = {2019-03-29},
  booktitle = {Advances in {{Neural Information Processing Systems}} 24},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf},
  author = {Bergstra, James S. and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  editor = {{Shawe-Taylor}, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  year = {2011},
  pages = {2546--2554}
}

@article{jones_taxonomy_2001,
  title = {A {{Taxonomy}} of {{Global Optimization Methods Based}} on {{Response Surfaces}}},
  volume = {21},
  issn = {1573-2916},
  abstract = {This paper presents a taxonomy of existing approaches for using response surfaces for global optimization. Each method is illustrated with a simple numerical example that brings out its advantages and disadvantages. The central theme is that methods that seem quite reasonable often have non-obvious failure modes. Understanding these failure modes is essential for the development of practical algorithms that fulfill the intuitive promise of the response surface approach.},
  language = {en},
  number = {4},
  urldate = {2019-08-23},
  journal = {Journal of Global Optimization},
  doi = {10/c229dg},
  url = {https://doi.org/10.1023/A:1012771025575},
  author = {Jones, Donald R.},
  month = dec,
  year = {2001},
  keywords = {global optimization,kriging,response surface,splines},
  pages = {345-383}
}

@inproceedings{hutter_sequential_2011,
  address = {{Berlin, Heidelberg}},
  series = {{{LION}}'05},
  title = {Sequential {{Model}}-Based {{Optimization}} for {{General Algorithm Configuration}}},
  isbn = {978-3-642-25565-6},
  abstract = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach.},
  urldate = {2019-08-23},
  booktitle = {Proceedings of the 5th {{International Conference}} on {{Learning}} and {{Intelligent Optimization}}},
  publisher = {{Springer-Verlag}},
  doi = {10/fb9tjc},
  url = {http://dx.doi.org/10.1007/978-3-642-25566-3_40},
  author = {Hutter, Frank and Hoos, Holger H. and {Leyton-Brown}, Kevin},
  year = {2011},
  pages = {507--523}
}

@inproceedings{bergstra_making_2013,
  title = {Making a {{Science}} of {{Model Search}}: {{Hyperparameter Optimization}} in {{Hundreds}} of {{Dimensions}} for {{Vision Architectures}}},
  shorttitle = {Making a {{Science}} of {{Model Search}}},
  abstract = {Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is of...},
  language = {en},
  urldate = {2019-08-23},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  url = {http://proceedings.mlr.press/v28/bergstra13.html},
  author = {Bergstra, James and Yamins, Daniel and Cox, David},
  month = feb,
  year = {2013},
  pages = {115-123}
}

@phdthesis{krige_statistical_1951,
  address = {{Johannesburg, South Africa}},
  type = {Master {{Thesis}}},
  title = {A Statistical Approach to Some Mine Valuation and Allied Problems on the {{Witwatersrand}}},
  abstract = {Thesis (M.Sc.(Engineering))--University of the Witwatersrand, Faculty of Engineering, 1951.},
  language = {en},
  urldate = {2019-08-28},
  school = {University of the Witwatersrand},
  url = {http://wiredspace.wits.ac.za/handle/10539/17975},
  author = {Krige, D. G.},
  year = {1951}
}

@article{matheron_principles_1963,
  title = {Principles of Geostatistics},
  volume = {58},
  issn = {0361-0128},
  language = {en},
  number = {8},
  urldate = {2019-08-28},
  journal = {Economic Geology},
  doi = {10/fdsdjx},
  url = {https://pubs.geoscienceworld.org/segweb/economicgeology/article-abstract/58/8/1246/17275/principles-of-geostatistics},
  author = {Matheron, Georges},
  month = dec,
  year = {1963},
  pages = {1246-1266},
}

@article{jones_efficient_1998,
  title = {Efficient {{Global Optimization}} of {{Expensive Black}}-{{Box Functions}}},
  volume = {13},
  issn = {1573-2916},
  abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
  language = {en},
  number = {4},
  urldate = {2019-08-28},
  journal = {Journal of Global Optimization},
  doi = {10/fg68nc},
  url = {https://doi.org/10.1023/A:1008306431147},
  author = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
  month = dec,
  year = {1998},
  keywords = {Bayesian global optimization,Kriging,Random function,Response surface,Stochastic process,Visualization},
  pages = {455-492}
}

@inproceedings{feurer_initializing_2015,
  title = {Initializing {{Bayesian Hyperparameter Optimization}} via {{Meta}}-{{Learning}}},
  copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author's personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer's own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  abstract = {Model selection and hyperparameter optimization is crucial in applying machine learning to a novel dataset. Recently, a subcommunity of machine learning has focused on solving this problem with Sequential Model-based Bayesian Optimization (SMBO), demonstrating substantial successes in many applications. However, for computationally expensive algorithms the overhead of hyperparameter optimization can still be prohibitive. In this paper we mimic a strategy human domain experts use: speed up optimization by starting from promising configurations that performed well on similar datasets. The resulting initialization technique integrates naturally into the generic SMBO framework and can be trivially applied to any SMBO method. To validate our approach, we perform extensive experiments with two established SMBO frameworks (Spearmint and SMAC) with complementary strengths; optimizing two machine learning frameworks on 57 datasets. Our initialization procedure yields mild improvements for low-dimensional hyperparameter optimization and substantially improves the state of the art for the more complex combined algorithm selection and hyperparameter optimization problem.},
  language = {en},
  urldate = {2019-08-29},
  booktitle = {Twenty-{{Ninth AAAI Conference}} on {{Artificial Intelligence}}},
  url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/10029},
  author = {Feurer, Matthias and Springenberg, Jost Tobias and Hutter, Frank},
  month = feb,
  year = {2015}
}

@inproceedings{feurer_efficient_2015,
  title = {Efficient and {{Robust Automated Machine Learning}}},
  urldate = {2019-08-29},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  publisher = {{Curran Associates, Inc.}},
  url = {http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf},
  author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  pages = {2962--2970}
}

@article{jamieson_nonstochastic_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.07943},
  primaryClass = {cs, stat},
  title = {Non-Stochastic {{Best Arm Identification}} and {{Hyperparameter Optimization}}},
  abstract = {Motivated by the task of hyperparameter optimization, we introduce the non-stochastic best-arm identification problem. Within the multi-armed bandit literature, the cumulative regret objective enjoys algorithms and analyses for both the non-stochastic and stochastic settings while to the best of our knowledge, the best-arm identification framework has only been considered in the stochastic setting. We introduce the non-stochastic setting under this framework, identify a known algorithm that is well-suited for this setting, and analyze its behavior. Next, by leveraging the iterative nature of standard machine learning algorithms, we cast hyperparameter optimization as an instance of non-stochastic best-arm identification, and empirically evaluate our proposed algorithm on this task. Our empirical results show that, by allocating more resources to promising hyperparameter settings, we typically achieve comparable test accuracies an order of magnitude faster than baseline methods.},
  urldate = {2019-08-29},
  journal = {arXiv:1502.07943 [cs, stat]},
  url = {http://arxiv.org/abs/1502.07943},
  author = {Jamieson, Kevin and Talwalkar, Ameet},
  month = feb,
  year = {2015},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{domhan_speeding_2015,
  series = {{{IJCAI}}'15},
  title = {Speeding {{Up Automatic Hyperparameter Optimization}} of {{Deep Neural Networks}} by {{Extrapolation}} of {{Learning Curves}}},
  isbn = {978-1-57735-738-4},
  abstract = {Deep neural networks (DNNs) show very strong performance on many machine learning problems, but they are very sensitive to the setting of their hyperparameters. Automated hyperparameter optimization methods have recently been shown to yield settings competitive with those found by human experts, but their widespread adoption is hampered by the fact that they require more computational resources than human experts. Humans have one advantage: when they evaluate a poor hyperparameter setting they can quickly detect (after a few steps of stochastic gradient descent) that the resulting network performs poorly and terminate the corresponding evaluation to save time. In this paper, we mimic the early termination of bad runs using a probabilistic model that extrapolates the performance from the first part of a learning curve. Experiments with a broad range of neural network architectures on various prominent object recognition benchmarks show that our resulting approach speeds up state-of-the-art hyperparameter optimization methods for DNNs roughly twofold, enabling them to find DNN settings that yield better performance than those chosen by human experts.},
  urldate = {2019-08-30},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Artificial Intelligence}}},
  publisher = {{AAAI Press}},
  url = {http://dl.acm.org/citation.cfm?id=2832581.2832731},
  author = {Domhan, Tobias and Springenberg, Jost Tobias and Hutter, Frank},
  year = {2015},
  pages = {3460--3468}
}

@article{klein_fast_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.07079},
  primaryClass = {cs, stat},
  title = {Fast {{Bayesian Optimization}} of {{Machine Learning Hyperparameters}} on {{Large Datasets}}},
  abstract = {Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. Despite its success, for large datasets, training and validating a single configuration often takes hours, days, or even weeks, which limits the achievable performance. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary configurations on small subsets, by extrapolating to the full dataset. We construct a Bayesian optimization procedure, dubbed Fabolas, which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that Fabolas often finds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods or the recently proposed bandit strategy Hyperband.},
  urldate = {2019-08-29},
  journal = {arXiv:1605.07079 [cs, stat]},
  url = {http://arxiv.org/abs/1605.07079},
  author = {Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
  month = may,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
}

@article{li_hyperband_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.06560},
  primaryClass = {cs, stat},
  title = {Hyperband: {{A Novel Bandit}}-{{Based Approach}} to {{Hyperparameter Optimization}}},
  shorttitle = {Hyperband},
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
  urldate = {2019-08-29},
  journal = {arXiv:1603.06560 [cs, stat]},
  url = {http://arxiv.org/abs/1603.06560},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  month = mar,
  year = {2016},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
}

@inproceedings{falkner_combining_2017,
  address = {{Long Beach, USA}},
  title = {Combining {{Hyperband}} and {{Bayesian Optimization}}},
  abstract = {Proper hyperparameter optimization is computationally very costly for expensive machine learning methods, such as deep neural networks; the same holds true for neural architecture search. Recently, the bandit-based strategy Hyperband has shown superior performance to vanilla Bayesian optimization methods that are limited to the traditional problem formulation of expensive blackbox optimization. However, while Hyperband has strong anytime performance for finding configurations with acceptable results, it relies on random search and therefore does not find the best configurations quickly. We propose to combine Hyperband with Bayesian optimization by maintaining a probabilistic model that captures the density of good configurations in the input space and samples from this model instead of sampling uniformly at random. We empirically show that our new method combines Hyperband's strong anytime performance with the strong eventual performance of Bayesian optimization.},
  booktitle = {{{NIPS}} 2017 {{Bayesian Optimization Workshop}} ({{Dec}} 2017)},
  author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
  month = dec,
  year = {2017},
  keywords = {Anytime algorithm,Artificial neural network,Bayesian optimization,Deep learning,Machine learning,Mathematical optimization,Neural Network Simulation,Program optimization,Random search,Sampling (signal processing),Statistical model}
}

@article{falkner_bohb_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.01774},
  primaryClass = {cs, stat},
  title = {{{BOHB}}: {{Robust}} and {{Efficient Hyperparameter Optimization}} at {{Scale}}},
  shorttitle = {{{BOHB}}},
  abstract = {Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.},
  urldate = {2019-08-30},
  journal = {arXiv:1807.01774 [cs, stat]},
  url = {http://arxiv.org/abs/1807.01774},
  author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
  month = jul,
  year = {2018},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning}
}

@inproceedings{hutter_efficient_2014,
  series = {{{ICML}}'14},
  title = {An {{Efficient Approach}} for {{Assessing Hyperparameter Importance}}},
  abstract = {The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization. We first introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional ANOVA framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that--even in very highdimensional cases--most performance variation is attributable to just a few hyperparameters.},
  urldate = {2019-08-30},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 32},
  publisher = {{JMLR.org}},
  url = {http://dl.acm.org/citation.cfm?id=3044805.3044891},
  author = {Hutter, Frank and Hoos, Holger and {Leyton-Brown}, Kevin},
  year = {2014},
  pages = {I-754--I-762}
}

@inproceedings{olson_evaluation_2016,
  address = {{New York, NY, USA}},
  series = {{{GECCO}} '16},
  title = {Evaluation of a {{Tree}}-Based {{Pipeline Optimization Tool}} for {{Automating Data Science}}},
  isbn = {978-1-4503-4206-3},
  abstract = {As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning--pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.},
  urldate = {2019-03-28},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} 2016},
  publisher = {{ACM}},
  doi = {10.1145/2908812.2908918},
  url = {http://doi.acm.org/10.1145/2908812.2908918},
  author = {Olson, Randal S. and Bartley, Nathan and Urbanowicz, Ryan J. and Moore, Jason H.},
  month = jul,
  year = {2016},
  keywords = {data science,genetic programming,hyperparameter optimization,machine learning,Pareto optimization,pipeline optimization,python}
}

@article{olson_automating_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1601.07925},
  primaryClass = {cs},
  title = {Automating Biomedical Data Science through Tree-Based Pipeline Optimization},
  abstract = {Over the past decade, data science and machine learning has grown from a mysterious art form to a staple tool across a variety of fields in academia, business, and government. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning---pipeline design. We implement a Tree-based Pipeline Optimization Tool (TPOT) and demonstrate its effectiveness on a series of simulated and real-world genetic data sets. In particular, we show that TPOT can build machine learning pipelines that achieve competitive classification accuracy and discover novel pipeline operators---such as synthetic feature constructors---that significantly improve classification accuracy on these data sets. We also highlight the current challenges to pipeline optimization, such as the tendency to produce pipelines that overfit the data, and suggest future research paths to overcome these challenges. As such, this work represents an early step toward fully automating machine learning pipeline design.},
  urldate = {2019-03-28},
  journal = {arXiv:1601.07925 [cs]},
  url = {http://arxiv.org/abs/1601.07925},
  author = {Olson, Randal S. and Urbanowicz, Ryan J. and Andrews, Peter C. and Lavender, Nicole A. and Kidd, La Creis and Moore, Jason H.},
  month = jan,
  year = {2016},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Machine Learning},
  note = {eprinttype: arxiv
eprintclass: cs.LG, cs.NE
eprint: http://arxiv.org/abs/1601.07925}
}

@article{zoph_neural_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.01578},
  primaryClass = {cs},
  title = {Neural {{Architecture Search}} with {{Reinforcement Learning}}},
  abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
  journal = {arXiv:1611.01578 [cs]},
  url = {http://arxiv.org/abs/1611.01578},
  author = {Zoph, Barret and Le, Quoc V.},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{zoph_learning_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.07012},
  primaryClass = {cs, stat},
  title = {Learning {{Transferable Architectures}} for {{Scalable Image Recognition}}},
  abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4\% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on ImageNet. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74\% top-1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0\% achieving 43.1\% mAP on the COCO dataset.},
  urldate = {2019-03-28},
  journal = {arXiv:1707.07012 [cs, stat]},
  url = {http://arxiv.org/abs/1707.07012},
  author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
  month = jul,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {eprinttype: arxiv
eprintclass: cs.CV, cs.LG, stat.ML
eprint: http://arxiv.org/abs/1707.07012}
}

@article{jin_autokeras_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.10282},
  primaryClass = {cs, stat},
  title = {Auto-{{Keras}}: {{An Efficient Neural Architecture Search System}}},
  shorttitle = {Auto-{{Keras}}},
  abstract = {Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Intensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.},
  urldate = {2019-03-28},
  journal = {arXiv:1806.10282 [cs, stat]},
  url = {http://arxiv.org/abs/1806.10282},
  author = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
  month = jun,
  year = {2018},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {eprinttype: arxiv
eprintclass: cs.LG, cs.AI, stat.ML
eprint: http://arxiv.org/abs/1806.10282}
}

@article{elsken_neural_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.05377},
  primaryClass = {cs, stat},
  title = {Neural {{Architecture Search}}: {{A Survey}}},
  shorttitle = {Neural {{Architecture Search}}},
  abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
  urldate = {2019-08-30},
  journal = {arXiv:1808.05377 [cs, stat]},
  url = {http://arxiv.org/abs/1808.05377},
  author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  month = aug,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{gijsbers_open_2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1907.00909},
  primaryClass = {cs, stat},
  title = {An {{Open Source AutoML Benchmark}}},
  abstract = {In recent years, an active field of research has developed around automated machine learning (AutoML). Unfortunately, comparing different AutoML systems is hard and often done incorrectly. We introduce an open, ongoing, and extensible benchmark framework which follows best practices and avoids common mistakes. The framework is open-source, uses public datasets and has a website with up-to-date results. We use the framework to conduct a thorough comparison of 4 AutoML systems across 39 datasets and analyze the results.},
  urldate = {2019-08-30},
  journal = {arXiv:1907.00909 [cs, stat]},
  url = {http://arxiv.org/abs/1907.00909},
  author = {Gijsbers, Pieter and LeDell, Erin and Thomas, Janek and Poirier, S{\'e}bastien and Bischl, Bernd and Vanschoren, Joaquin},
  month = jul,
  year = {2019},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning}
}

@techreport{krizhevsky2009learning,
  type = {Technical},
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it diccult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time.
A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly improved by pre-training a layer of features on a large set of unlabeled tiny images.

This tech report (Chapter 3) describes the dataset and the methodology followed when collecting it in much greater detail. Please cite it if you intend to use this dataset.},
  language = {English},
  institution = {{University of Toronto}},
  url = {https://www.cs.toronto.edu/~kriz/cifar.html},
  author = {Krizhevsky, Alex},
  year = {2009}
}


@inproceedings{goto_anomaly_2019,
  title = {Anomaly Detection of Solder Joint on Print Circuit Board by Using {{Adversarial Autoencoder}}},
  volume = {11172},
  abstract = {We propose a defect detection method of solders on a printed circuit board using X-ray CT inspection system and Adversarial Autoencoder (AAE)\textsuperscript{[1]} . We obtain sliced images of the solder using X-ray CT and extract their features that follow the standard normal distribution by using AAE. Then, the solder defects are detected by Hotelling's T square\textsuperscript{[2].} As a result of experiments, we show that we can classify normal and anomalous data samples completely on the condition of training with large normal samples and small anomalous samples.},
  urldate = {2019-09-27},
  booktitle = {Fourteenth {{International Conference}} on {{Quality Control}} by {{Artificial Vision}}},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10/gf8v9j},
  url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11172/111720T/Anomaly-detection-of-solder-joint-on-print-circuit-board-by/10.1117/12.2521762.short},
  author = {Goto, Keisuke and Kato, Kunihito and Nakatsuka, Shunsuke and Saito, Takaho and Aizawa, Hiroaki},
  month = jul,
  year = {2019},
  pages = {111720T}
}

@inproceedings{fantinel_visual_2019,
  title = {Visual Inspection for Metallic Surfaces: {{CNN}} Driven by Features},
  volume = {11172},
  shorttitle = {Visual Inspection for Metallic Surfaces},
  abstract = {In this paper, an effective and novel automatic learning solution for the quality control of metallic objects surfaces is proposed, which can be seamlessly integrated into the industrial process. Such a system requires a coaxial illuminator to capture the object view with a single camera while lighting it with structured light: in this way, the object surface can be viewed in time as a dynamic scene under different illumination conditions. By relying on a linear model to describe the expected evolution of the light over the object of interest, the Residuals of Linear Evolution of Light (RLEL) algorithm is derived with the specific aim of identifying and characterizing anomalies and defects through the residuals of a least square approach. Then, a novel learning strategy is developed that exploits the model-based RLEL descriptor and thus promotes itself as an alternative strategy to the black box approach of Convolutional Neural Networks (CNNs). By combining both the data-driven and the model-based learning approaches to perform the inspection task, an Hybrid Learning (HL) procedure is defined: in a first phase, the HL exploits an Encoder-Decoder network to incorporate the model-based description while, in a second phase, it uses only the pre-trained encoder to drive the learning process of a 3D-CNN. In doing so, the proposed procedure reaches interesting results that exceed also the performance of state-of-the-art 3D-Inception and 3D-Residual networks.},
  urldate = {2019-09-23},
  booktitle = {Fourteenth {{International Conference}} on {{Quality Control}} by {{Artificial Vision}}},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10/gf8qgt},
  url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11172/111720H/Visual-inspection-for-metallic-surfaces-CNN-driven-by-features/10.1117/12.2521455.short},
  author = {Fantinel, Riccardo and Cenedese, Angelo},
  month = jul,
  year = {2019},
  pages = {111720H}
}

@article{meguenani_deflectometry_2019,
  title = {Deflectometry Based Inline Inspection of Linearly Moving Plastic Parts},
  abstract = {Widely used for surface slopes measurements and for three-dimensional shape reconstruction, deflectometry is a particularly powerful technique that can also be applied for defects detection on specular surfaces. In the visible domain, deflectometry is usually based on the projection of complex encoded light patterns and necessitates heavy processing that makes it not suitable for inline inspection. In this paper, A new deflectometry based approach that is more adapted for inline inspection of linearly moving parts (parts on conveyors) is proposed. Based on a more affordable and a simpler hardware setup, the new approach allows at the same time for a proper localization and a precise geometrical quantification of any defects on the scanned specular surfaces. The proposed approach uses a fast and simple processing algorithm that lends itself very well to real-time inspection. The new method is tested and validated in laboratory for the inspection of defects on specular surfaces of plastic parts.},
  author = {Meguenani, Anis and Chambard, Jean-Pierre and Maillot, Yvan and Cudel, Christophe},
  month = jul,
  year = {2019}
}







